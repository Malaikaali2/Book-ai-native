"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[7051],{152(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-1-ros/sensor-fusion","title":"Sensor Fusion Implementation Guide","description":"Overview","source":"@site/docs/module-1-ros/sensor-fusion.md","sourceDirName":"module-1-ros","slug":"/module-1-ros/sensor-fusion","permalink":"/Book-ai-native/docs/module-1-ros/sensor-fusion","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/module-1-ros/sensor-fusion.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Navigation Stack Integration","permalink":"/Book-ai-native/docs/module-1-ros/navigation-integration"},"next":{"title":"Verification and Debugging in ROS 2","permalink":"/Book-ai-native/docs/module-1-ros/debugging"}}');var s=r(4848),o=r(8453);const a={sidebar_position:6},t="Sensor Fusion Implementation Guide",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Types of Sensor Fusion",id:"types-of-sensor-fusion",level:3},{value:"Common Sensor Types in Robotics",id:"common-sensor-types-in-robotics",level:3},{value:"Mathematical Foundations",id:"mathematical-foundations",level:2},{value:"Kalman Filter",id:"kalman-filter",level:3},{value:"Extended Kalman Filter (EKF)",id:"extended-kalman-filter-ekf",level:3},{value:"Particle Filter",id:"particle-filter",level:3},{value:"ROS 2 Sensor Fusion Packages",id:"ros-2-sensor-fusion-packages",level:2},{value:"Robot Localization Package",id:"robot-localization-package",level:3},{value:"Configuration Example",id:"configuration-example",level:4},{value:"Implementation Example: IMU and Odometry Fusion",id:"implementation-example-imu-and-odometry-fusion",level:2},{value:"Creating a Sensor Fusion Node",id:"creating-a-sensor-fusion-node",level:3},{value:"Advanced Fusion Techniques",id:"advanced-fusion-techniques",level:2},{value:"Multi-Sensor Fusion with LIDAR and Camera",id:"multi-sensor-fusion-with-lidar-and-camera",level:3},{value:"Sensor Fusion Best Practices",id:"sensor-fusion-best-practices",level:2},{value:"1. Data Synchronization",id:"1-data-synchronization",level:3},{value:"2. Calibration",id:"2-calibration",level:3},{value:"3. Covariance Management",id:"3-covariance-management",level:3},{value:"4. Fault Detection",id:"4-fault-detection",level:3},{value:"5. Computational Efficiency",id:"5-computational-efficiency",level:3},{value:"Common Fusion Algorithms",id:"common-fusion-algorithms",level:2},{value:"1. Kalman Filter Family",id:"1-kalman-filter-family",level:3},{value:"2. Complementary Filter",id:"2-complementary-filter",level:3},{value:"3. Covariance Intersection",id:"3-covariance-intersection",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"Metrics for Sensor Fusion",id:"metrics-for-sensor-fusion",level:3},{value:"Testing Approaches",id:"testing-approaches",level:3},{value:"References",id:"references",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"sensor-fusion-implementation-guide",children:"Sensor Fusion Implementation Guide"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Sensor fusion is the process of combining data from multiple sensors to achieve improved accuracy and reliability compared to using a single sensor alone. In robotics, sensor fusion is critical for navigation, localization, mapping, and environmental perception."}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"types-of-sensor-fusion",children:"Types of Sensor Fusion"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data-Level Fusion"}),": Combining raw sensor measurements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature-Level Fusion"}),": Combining extracted features from sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decision-Level Fusion"}),": Combining decisions from individual sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hybrid Fusion"}),": Combining multiple fusion levels"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"common-sensor-types-in-robotics",children:"Common Sensor Types in Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU (Inertial Measurement Unit)"}),": Provides acceleration, angular velocity, and orientation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPS"}),": Provides global position (outdoor environments)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LIDAR"}),": Provides precise distance measurements for mapping and obstacle detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cameras"}),": Provide visual information for object recognition and scene understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Encoders"}),": Provide wheel rotation data for odometry"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ultrasonic sensors"}),": Provide short-range distance measurements"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"mathematical-foundations",children:"Mathematical Foundations"}),"\n",(0,s.jsx)(n.h3,{id:"kalman-filter",children:"Kalman Filter"}),"\n",(0,s.jsx)(n.p,{children:"The Kalman filter is a mathematical method for estimating the state of a system from noisy measurements. It's optimal for linear systems with Gaussian noise."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Prediction Step:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Predict state: ",(0,s.jsx)(n.code,{children:"x_pred = F * x_prev + B * u"})]}),"\n",(0,s.jsxs)(n.li,{children:["Predict covariance: ",(0,s.jsx)(n.code,{children:"P_pred = F * P_prev * F^T + Q"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Update Step:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Compute Kalman gain: ",(0,s.jsx)(n.code,{children:"K = P_pred * H^T * (H * P_pred * H^T + R)^-1"})]}),"\n",(0,s.jsxs)(n.li,{children:["Update state: ",(0,s.jsx)(n.code,{children:"x_new = x_pred + K * (z - H * x_pred)"})]}),"\n",(0,s.jsxs)(n.li,{children:["Update covariance: ",(0,s.jsx)(n.code,{children:"P_new = (I - K * H) * P_pred"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"extended-kalman-filter-ekf",children:"Extended Kalman Filter (EKF)"}),"\n",(0,s.jsx)(n.p,{children:"For non-linear systems, the Extended Kalman Filter linearizes the system around the current estimate."}),"\n",(0,s.jsx)(n.h3,{id:"particle-filter",children:"Particle Filter"}),"\n",(0,s.jsx)(n.p,{children:"A particle filter represents the probability distribution with a set of particles that evolve over time based on system dynamics and sensor measurements."}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-sensor-fusion-packages",children:"ROS 2 Sensor Fusion Packages"}),"\n",(0,s.jsx)(n.h3,{id:"robot-localization-package",children:"Robot Localization Package"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"robot_localization"})," package provides sensor fusion for robot state estimation using an EKF or UKF (Unscented Kalman Filter)."]}),"\n",(0,s.jsx)(n.h4,{id:"configuration-example",children:"Configuration Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# ekf.yaml\r\nekf_filter_node:\r\n  ros__parameters:\r\n    # The frequency, in Hz, at which the filter will output a position estimate\r\n    frequency: 30.0\r\n\r\n    # The period, in seconds, at which the filter will output a position estimate\r\n    sensor_timeout: 0.1\r\n\r\n    # Whether to two-dimensional or not\r\n    two_d_mode: true\r\n\r\n    # Whether to broadcast the transform between the input and output frames\r\n    transform_time_offset: 0.0\r\n\r\n    # Whether to publish the acceleration state\r\n    publish_acceleration: false\r\n\r\n    # Whether to broadcast the transform between the input and output frames\r\n    publish_tf: true\r\n\r\n    # Map frame\r\n    map_frame: map\r\n\r\n    # Odometry frame (world-fixed)\r\n    odom_frame: odom\r\n\r\n    # Base frame (robot-fixed)\r\n    base_link_frame: base_link\r\n\r\n    # World frame for the transform between the input and output frames\r\n    world_frame: odom\r\n\r\n    # Sensor configuration\r\n    odom0: /wheel/odometry\r\n    odom0_config: [true,  true,  false,\r\n                   false, false, false,\r\n                   false, false, false,\r\n                   false, false, true,\r\n                   false, false, false]\r\n    odom0_differential: false\r\n    odom0_relative: false\r\n\r\n    imu0: /imu/data\r\n    imu0_config: [false, false, false,\r\n                  true,  true,  true,\r\n                  false, false, false,\r\n                  true,  true,  true,\r\n                  false, false, false]\r\n    imu0_differential: false\r\n    imu0_relative: true\r\n    imu0_queue_size: 10\r\n    imu0_pose_rejection_threshold: 0.8\r\n    imu0_twist_rejection_threshold: 0.8\r\n    imu0_linear_acceleration_rejection_threshold: 0.8\n"})}),"\n",(0,s.jsx)(n.h2,{id:"implementation-example-imu-and-odometry-fusion",children:"Implementation Example: IMU and Odometry Fusion"}),"\n",(0,s.jsx)(n.h3,{id:"creating-a-sensor-fusion-node",children:"Creating a Sensor Fusion Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Imu\r\nfrom nav_msgs.msg import Odometry\r\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\r\nimport numpy as np\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass SensorFusionNode(Node):\r\n\r\n    def __init__(self):\r\n        super().__init__(\'sensor_fusion_node\')\r\n\r\n        # Subscribe to IMU data\r\n        self.imu_sub = self.create_subscription(\r\n            Imu,\r\n            \'imu/data\',\r\n            self.imu_callback,\r\n            10)\r\n\r\n        # Subscribe to odometry data\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            \'odom\',\r\n            self.odom_callback,\r\n            10)\r\n\r\n        # Publisher for fused pose\r\n        self.pose_pub = self.create_publisher(\r\n            PoseWithCovarianceStamped,\r\n            \'fused_pose\',\r\n            10)\r\n\r\n        # Initialize state variables\r\n        self.imu_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # Quaternion\r\n        self.odom_position = np.array([0.0, 0.0, 0.0])\r\n        self.odom_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # Quaternion\r\n        self.fused_pose = PoseWithCovarianceStamped()\r\n\r\n        # Covariance matrices (simplified)\r\n        self.imu_cov = np.eye(4) * 0.01  # IMU covariance\r\n        self.odom_cov = np.eye(4) * 0.1   # Odometry covariance\r\n\r\n        self.get_logger().info(\'Sensor fusion node initialized\')\r\n\r\n    def imu_callback(self, msg):\r\n        """Process IMU data"""\r\n        # Extract orientation from IMU\r\n        self.imu_orientation = np.array([\r\n            msg.orientation.x,\r\n            msg.orientation.y,\r\n            msg.orientation.z,\r\n            msg.orientation.w\r\n        ])\r\n\r\n        # Update covariance from IMU\r\n        self.imu_cov = np.array(msg.orientation_covariance).reshape(3, 3)\r\n\r\n    def odom_callback(self, msg):\r\n        """Process odometry data and perform fusion"""\r\n        # Extract position and orientation from odometry\r\n        self.odom_position = np.array([\r\n            msg.pose.pose.position.x,\r\n            msg.pose.pose.position.y,\r\n            msg.pose.pose.position.z\r\n        ])\r\n\r\n        self.odom_orientation = np.array([\r\n            msg.pose.pose.orientation.x,\r\n            msg.pose.pose.orientation.y,\r\n            msg.pose.pose.orientation.z,\r\n            msg.pose.pose.orientation.w\r\n        ])\r\n\r\n        # Perform sensor fusion (simplified example using weighted average)\r\n        fused_orientation = self.fuse_orientations(\r\n            self.imu_orientation, self.odom_orientation\r\n        )\r\n\r\n        # Create and publish fused pose\r\n        self.fused_pose.header.stamp = self.get_clock().now().to_msg()\r\n        self.fused_pose.header.frame_id = \'map\'\r\n\r\n        # Set position from odometry (usually more reliable for position)\r\n        self.fused_pose.pose.pose.position.x = msg.pose.pose.position.x\r\n        self.fused_pose.pose.pose.position.y = msg.pose.pose.position.y\r\n        self.fused_pose.pose.pose.position.z = msg.pose.pose.position.z\r\n\r\n        # Set orientation from fusion\r\n        self.fused_pose.pose.pose.orientation.x = fused_orientation[0]\r\n        self.fused_pose.pose.pose.orientation.y = fused_orientation[1]\r\n        self.fused_pose.pose.pose.orientation.z = fused_orientation[2]\r\n        self.fused_pose.pose.pose.orientation.w = fused_orientation[3]\r\n\r\n        # Set covariance (simplified)\r\n        self.fused_pose.pose.covariance = self.calculate_fused_covariance()\r\n\r\n        self.pose_pub.publish(self.fused_pose)\r\n\r\n    def fuse_orientations(self, imu_quat, odom_quat, imu_weight=0.7, odom_weight=0.3):\r\n        """\r\n        Fuse orientations using weighted average of quaternions\r\n        """\r\n        # Normalize quaternions\r\n        imu_quat = imu_quat / np.linalg.norm(imu_quat)\r\n        odom_quat = odom_quat / np.linalg.norm(odom_quat)\r\n\r\n        # Weighted average of quaternions\r\n        fused_quat = imu_weight * imu_quat + odom_weight * odom_quat\r\n        fused_quat = fused_quat / np.linalg.norm(fused_quat)\r\n\r\n        return fused_quat\r\n\r\n    def calculate_fused_covariance(self):\r\n        """\r\n        Calculate fused covariance matrix\r\n        """\r\n        # Simplified covariance fusion (in practice, use proper fusion methods)\r\n        fused_cov = [0.0] * 36\r\n        # Fill diagonal with some reasonable values\r\n        fused_cov[0] = 0.01  # x position variance\r\n        fused_cov[7] = 0.01  # y position variance\r\n        fused_cov[14] = 0.01  # z position variance\r\n        fused_cov[21] = 0.005  # x orientation variance\r\n        fused_cov[28] = 0.005  # y orientation variance\r\n        fused_cov[35] = 0.005  # z orientation variance\r\n\r\n        return fused_cov\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    sensor_fusion_node = SensorFusionNode()\r\n\r\n    try:\r\n        rclpy.spin(sensor_fusion_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        sensor_fusion_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-fusion-techniques",children:"Advanced Fusion Techniques"}),"\n",(0,s.jsx)(n.h3,{id:"multi-sensor-fusion-with-lidar-and-camera",children:"Multi-Sensor Fusion with LIDAR and Camera"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Image, PointCloud2\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass MultiSensorFusionNode(Node):\r\n\r\n    def __init__(self):\r\n        super().__init__(\'multi_sensor_fusion_node\')\r\n\r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Subscribe to different sensor types\r\n        self.lidar_sub = self.create_subscription(\r\n            LaserScan,\r\n            \'scan\',\r\n            self.lidar_callback,\r\n            10)\r\n\r\n        self.camera_sub = self.create_subscription(\r\n            Image,\r\n            \'camera/image_raw\',\r\n            self.camera_callback,\r\n            10)\r\n\r\n        # Publisher for fused data\r\n        self.fused_pub = self.create_publisher(\r\n            PointCloud2,  # Could be custom message type\r\n            \'fused_sensor_data\',\r\n            10)\r\n\r\n        # Storage for sensor data\r\n        self.lidar_data = None\r\n        self.camera_image = None\r\n        self.camera_timestamp = None\r\n        self.lidar_timestamp = None\r\n\r\n    def lidar_callback(self, msg):\r\n        """Process LIDAR data"""\r\n        self.lidar_data = msg.ranges\r\n        self.lidar_timestamp = msg.header.stamp\r\n\r\n        # If we have synchronized data, perform fusion\r\n        if self.camera_image is not None:\r\n            self.perform_lidar_camera_fusion()\r\n\r\n    def camera_callback(self, msg):\r\n        """Process camera data"""\r\n        self.camera_image = self.cv_bridge.imgmsg_to_cv2(msg, \'bgr8\')\r\n        self.camera_timestamp = msg.header.stamp\r\n\r\n        # If we have synchronized data, perform fusion\r\n        if self.lidar_data is not None:\r\n            self.perform_lidar_camera_fusion()\r\n\r\n    def perform_lidar_camera_fusion(self):\r\n        """Perform fusion between LIDAR and camera data"""\r\n        # This is a simplified example\r\n        # In practice, you\'d need calibration data and projection matrices\r\n\r\n        # Project LIDAR points to camera frame\r\n        # This requires extrinsic calibration between sensors\r\n        projected_points = self.project_lidar_to_camera(\r\n            self.lidar_data\r\n        )\r\n\r\n        # Combine with camera image features\r\n        combined_data = self.combine_lidar_camera(\r\n            projected_points,\r\n            self.camera_image\r\n        )\r\n\r\n        # Publish fused result\r\n        # In a real implementation, you\'d create a proper message\r\n        self.get_logger().info(f\'Fused {len(projected_points)} LIDAR points with camera image\')\r\n\r\n    def project_lidar_to_camera(self, lidar_ranges):\r\n        """Project LIDAR ranges to camera coordinate system"""\r\n        # This would require:\r\n        # 1. LIDAR to camera extrinsic calibration\r\n        # 2. Camera intrinsic parameters\r\n        # 3. Mathematical transformation\r\n\r\n        # Simplified example - in reality, this would be complex\r\n        points_3d = []\r\n        for i, range_val in enumerate(lidar_ranges):\r\n            if not np.isnan(range_val) and range_val > 0:\r\n                # Convert polar to Cartesian coordinates\r\n                angle = i * 0.01  # Assuming 0.01 radian increment\r\n                x = range_val * np.cos(angle)\r\n                y = range_val * np.sin(angle)\r\n                z = 0  # Assuming 2D LIDAR\r\n                points_3d.append([x, y, z])\r\n\r\n        return points_3d\r\n\r\n    def combine_lidar_camera(self, lidar_points, camera_image):\r\n        """Combine LIDAR points with camera image"""\r\n        # In practice, this might involve:\r\n        # - Colorizing LIDAR points based on camera image\r\n        # - Object detection using both modalities\r\n        # - Creating enhanced point clouds\r\n\r\n        # For this example, we\'ll just return the data\r\n        return {\r\n            \'lidar_points\': lidar_points,\r\n            \'image_shape\': camera_image.shape\r\n        }\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    fusion_node = MultiSensorFusionNode()\r\n\r\n    try:\r\n        rclpy.spin(fusion_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        fusion_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-fusion-best-practices",children:"Sensor Fusion Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"1-data-synchronization",children:"1. Data Synchronization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use ROS 2 message filters for time synchronization"}),"\n",(0,s.jsx)(n.li,{children:"Implement proper timestamp handling"}),"\n",(0,s.jsx)(n.li,{children:"Consider sensor delays and compensate accordingly"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-calibration",children:"2. Calibration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Perform extrinsic calibration between sensors"}),"\n",(0,s.jsx)(n.li,{children:"Regularly validate calibration parameters"}),"\n",(0,s.jsx)(n.li,{children:"Account for sensor mounting offsets"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-covariance-management",children:"3. Covariance Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Properly set covariance matrices"}),"\n",(0,s.jsx)(n.li,{children:"Update covariances based on sensor conditions"}),"\n",(0,s.jsx)(n.li,{children:"Consider sensor-specific noise characteristics"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-fault-detection",children:"4. Fault Detection"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement sensor health monitoring"}),"\n",(0,s.jsx)(n.li,{children:"Detect and handle sensor failures gracefully"}),"\n",(0,s.jsx)(n.li,{children:"Provide fallback mechanisms"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"5-computational-efficiency",children:"5. Computational Efficiency"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimize algorithms for real-time performance"}),"\n",(0,s.jsx)(n.li,{children:"Consider sensor data rates"}),"\n",(0,s.jsx)(n.li,{children:"Implement data decimation when appropriate"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"common-fusion-algorithms",children:"Common Fusion Algorithms"}),"\n",(0,s.jsx)(n.h3,{id:"1-kalman-filter-family",children:"1. Kalman Filter Family"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"EKF"}),": Good for non-linear systems with moderate non-linearity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"UKF"}),": Better for highly non-linear systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Particle Filter"}),": Good for multi-modal distributions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-complementary-filter",children:"2. Complementary Filter"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Simple fusion of sensors with complementary characteristics"}),"\n",(0,s.jsx)(n.li,{children:"Good for IMU and other sensors with different frequency responses"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-covariance-intersection",children:"3. Covariance Intersection"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Handles correlated uncertainties between sensors"}),"\n",(0,s.jsx)(n.li,{children:"Useful when cross-covariances are unknown"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,s.jsx)(n.h3,{id:"metrics-for-sensor-fusion",children:"Metrics for Sensor Fusion"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accuracy"}),": How close the fused estimate is to the true value"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Precision"}),": Consistency of the fused estimate"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": Performance under sensor failures or degraded conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency"}),": Time delay introduced by fusion process"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"testing-approaches",children:"Testing Approaches"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use ground truth data when available"}),"\n",(0,s.jsx)(n.li,{children:"Cross-validation with different sensor subsets"}),"\n",(0,s.jsx)(n.li,{children:"Stress testing under various environmental conditions"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsx)(n.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,r){r.d(n,{R:()=>a,x:()=>t});var i=r(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);