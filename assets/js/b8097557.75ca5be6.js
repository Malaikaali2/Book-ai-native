"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[5364],{4675(n,e,r){r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-ai-brain/gpu-optimization","title":"GPU Optimization Techniques","description":"Learning Objectives","source":"@site/docs/module-3-ai-brain/gpu-optimization.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/gpu-optimization","permalink":"/Book-ai-native/docs/module-3-ai-brain/gpu-optimization","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/module-3-ai-brain/gpu-optimization.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation Control Systems","permalink":"/Book-ai-native/docs/module-3-ai-brain/manipulation-control"},"next":{"title":"Lab: Isaac Perception System","permalink":"/Book-ai-native/docs/module-3-ai-brain/lab-perception-system"}}');var t=r(4848),o=r(8453);const a={sidebar_position:7},s="GPU Optimization Techniques",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to GPU Optimization for Robotics",id:"introduction-to-gpu-optimization-for-robotics",level:2},{value:"GPU Architecture Fundamentals",id:"gpu-architecture-fundamentals",level:2},{value:"CUDA Architecture Overview",id:"cuda-architecture-overview",level:3},{value:"Memory Hierarchy",id:"memory-hierarchy",level:3},{value:"CUDA Programming for Robotics",id:"cuda-programming-for-robotics",level:2},{value:"Basic CUDA Concepts",id:"basic-cuda-concepts",level:3},{value:"Optimized Point Cloud Processing",id:"optimized-point-cloud-processing",level:3},{value:"Memory Optimization Techniques",id:"memory-optimization-techniques",level:2},{value:"Memory Coalescing",id:"memory-coalescing",level:3},{value:"Memory Pool Management",id:"memory-pool-management",level:3},{value:"GPU-Accelerated Robotics Algorithms",id:"gpu-accelerated-robotics-algorithms",level:2},{value:"GPU-Accelerated Path Planning",id:"gpu-accelerated-path-planning",level:3},{value:"GPU-Accelerated SLAM",id:"gpu-accelerated-slam",level:3},{value:"Isaac ROS GPU Optimization",id:"isaac-ros-gpu-optimization",level:2},{value:"Isaac ROS GPU Packages",id:"isaac-ros-gpu-packages",level:3},{value:"Performance Profiling and Optimization",id:"performance-profiling-and-optimization",level:2},{value:"GPU Profiling Tools",id:"gpu-profiling-tools",level:3},{value:"Optimization Strategies",id:"optimization-strategies",level:3},{value:"Balancing CPU and GPU Workloads",id:"balancing-cpu-and-gpu-workloads",level:2},{value:"Heterogeneous Computing Strategies",id:"heterogeneous-computing-strategies",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"gpu-optimization-techniques",children:"GPU Optimization Techniques"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Optimize GPU utilization for robotics applications"}),"\n",(0,t.jsx)(e.li,{children:"Implement CUDA kernels for custom robotics algorithms"}),"\n",(0,t.jsx)(e.li,{children:"Optimize memory management for GPU-accelerated robotics"}),"\n",(0,t.jsx)(e.li,{children:"Profile and debug GPU performance bottlenecks"}),"\n",(0,t.jsx)(e.li,{children:"Balance computational load between CPU and GPU"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"introduction-to-gpu-optimization-for-robotics",children:"Introduction to GPU Optimization for Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Graphics Processing Units (GPUs) have become essential for robotics applications due to their ability to perform parallel computations efficiently. Unlike CPUs that excel at sequential processing, GPUs can handle thousands of lightweight threads simultaneously, making them ideal for robotics algorithms that involve:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Image and sensor processing"}),"\n",(0,t.jsx)(e.li,{children:"Neural network inference"}),"\n",(0,t.jsx)(e.li,{children:"Path planning and motion planning"}),"\n",(0,t.jsx)(e.li,{children:"Physics simulation"}),"\n",(0,t.jsx)(e.li,{children:"Point cloud processing"}),"\n",(0,t.jsx)(e.li,{children:"SLAM algorithms"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"The key to effective GPU optimization in robotics is understanding how to leverage parallelism while managing the unique constraints of real-time robotic systems, including latency requirements, power consumption, and thermal management."}),"\n",(0,t.jsx)(e.h2,{id:"gpu-architecture-fundamentals",children:"GPU Architecture Fundamentals"}),"\n",(0,t.jsx)(e.h3,{id:"cuda-architecture-overview",children:"CUDA Architecture Overview"}),"\n",(0,t.jsx)(e.p,{children:"NVIDIA's CUDA architecture forms the foundation for GPU computing in robotics:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Streaming Multiprocessors (SMs)"}),": Processing units that execute threads in groups called warps"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"CUDA Cores"}),": Arithmetic units within each SM that perform computations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Memory Hierarchy"}),": Different types of memory with varying speeds and accessibility"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Warp Execution"}),": Threads execute in groups of 32, requiring careful consideration of divergence"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"memory-hierarchy",children:"Memory Hierarchy"}),"\n",(0,t.jsx)(e.p,{children:"Understanding GPU memory types is crucial for optimization:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"// GPU Memory Types and Their Characteristics\r\n/*\r\n * Global Memory: Large capacity, high latency, accessible by all threads\r\n * Shared Memory: Small capacity, low latency, shared within thread blocks\r\n * Constant Memory: Cached read-only memory for constants\r\n * Texture Memory: Cached memory optimized for spatial locality\r\n * Registers: Fastest memory, private to each thread\r\n */\n"})}),"\n",(0,t.jsx)(e.h2,{id:"cuda-programming-for-robotics",children:"CUDA Programming for Robotics"}),"\n",(0,t.jsx)(e.h3,{id:"basic-cuda-concepts",children:"Basic CUDA Concepts"}),"\n",(0,t.jsx)(e.p,{children:"CUDA kernels are functions executed in parallel by many GPU threads:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"#include <cuda_runtime.h>\r\n#include <iostream>\r\n#include <vector>\r\n\r\n// Example: CUDA kernel for image processing\r\n__global__ void grayscale_kernel(\r\n    unsigned char* input,\r\n    unsigned char* output,\r\n    int width,\r\n    int height\r\n) {\r\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\r\n    int idy = blockIdx.y * blockDim.y + threadIdx.y;\r\n\r\n    if (idx < width && idy < height) {\r\n        int pixel_idx = idy * width + idx;\r\n        int rgb_idx = pixel_idx * 3;  // RGB format\r\n\r\n        // Convert RGB to grayscale\r\n        unsigned char r = input[rgb_idx];\r\n        unsigned char g = input[rgb_idx + 1];\r\n        unsigned char b = input[rgb_idx + 2];\r\n\r\n        output[pixel_idx] = 0.299f * r + 0.587f * g + 0.114f * b;\r\n    }\r\n}\r\n\r\n// Host function to launch the kernel\r\nvoid process_image_cuda(\r\n    const std::vector<unsigned char>& input,\r\n    std::vector<unsigned char>& output,\r\n    int width,\r\n    int height\r\n) {\r\n    // Allocate GPU memory\r\n    unsigned char *d_input, *d_output;\r\n    size_t image_size = width * height * 3;  // RGB\r\n    size_t gray_size = width * height;       // Grayscale\r\n\r\n    cudaMalloc(&d_input, image_size);\r\n    cudaMalloc(&d_output, gray_size);\r\n\r\n    // Copy data to GPU\r\n    cudaMemcpy(d_input, input.data(), image_size, cudaMemcpyHostToDevice);\r\n\r\n    // Define block and grid dimensions\r\n    dim3 blockSize(16, 16);  // 16x16 threads per block\r\n    dim3 gridSize(\r\n        (width + blockSize.x - 1) / blockSize.x,\r\n        (height + blockSize.y - 1) / blockSize.y\r\n    );\r\n\r\n    // Launch kernel\r\n    grayscale_kernel<<<gridSize, blockSize>>>(d_input, d_output, width, height);\r\n\r\n    // Wait for kernel to complete\r\n    cudaDeviceSynchronize();\r\n\r\n    // Copy result back to host\r\n    cudaMemcpy(output.data(), d_output, gray_size, cudaMemcpyDeviceToHost);\r\n\r\n    // Free GPU memory\r\n    cudaFree(d_input);\r\n    cudaFree(d_output);\r\n}\n"})}),"\n",(0,t.jsx)(e.h3,{id:"optimized-point-cloud-processing",children:"Optimized Point Cloud Processing"}),"\n",(0,t.jsx)(e.p,{children:"Point cloud processing is common in robotics and benefits significantly from GPU acceleration:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"#include <cuda_runtime.h>\r\n#include <vector_types.h>\r\n\r\n// Structure for 3D point\r\nstruct Point3D {\r\n    float x, y, z;\r\n    unsigned char r, g, b;  // Color information\r\n};\r\n\r\n// CUDA kernel for point cloud filtering\r\n__global__ void filter_points_kernel(\r\n    Point3D* input_points,\r\n    Point3D* output_points,\r\n    bool* valid_flags,\r\n    int num_points,\r\n    float min_x, float max_x,\r\n    float min_y, float max_y,\r\n    float min_z, float max_z\r\n) {\r\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\r\n\r\n    if (idx < num_points) {\r\n        Point3D point = input_points[idx];\r\n\r\n        // Check if point is within bounds\r\n        bool valid = (point.x >= min_x && point.x <= max_x) &&\r\n                     (point.y >= min_y && point.y <= max_y) &&\r\n                     (point.z >= min_z && point.z <= max_z);\r\n\r\n        valid_flags[idx] = valid;\r\n\r\n        if (valid) {\r\n            output_points[idx] = point;\r\n        }\r\n    }\r\n}\r\n\r\n// Optimized version using shared memory for bounds\r\n__global__ void filter_points_optimized_kernel(\r\n    Point3D* input_points,\r\n    Point3D* output_points,\r\n    int* output_count,\r\n    int num_points,\r\n    float4 bounds  // x=min_x, y=max_x, z=min_y, w=max_y\r\n) {\r\n    extern __shared__ float shared_bounds[];\r\n\r\n    int tid = threadIdx.x;\r\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\r\n\r\n    // Load bounds into shared memory\r\n    if (tid == 0) {\r\n        shared_bounds[0] = bounds.x;  // min_x\r\n        shared_bounds[1] = bounds.y;  // max_x\r\n        shared_bounds[2] = bounds.z;  // min_y\r\n        shared_bounds[3] = bounds.w;  // max_y\r\n        shared_bounds[4] = bounds.z;  // min_z (using z component)\r\n        shared_bounds[5] = bounds.w;  // max_z (using w component)\r\n    }\r\n    __syncthreads();\r\n\r\n    if (idx < num_points) {\r\n        Point3D point = input_points[idx];\r\n\r\n        // Use shared memory bounds for faster access\r\n        bool valid = (point.x >= shared_bounds[0] && point.x <= shared_bounds[1]) &&\r\n                     (point.y >= shared_bounds[2] && point.y <= shared_bounds[3]) &&\r\n                     (point.z >= shared_bounds[4] && point.z <= shared_bounds[5]);\r\n\r\n        if (valid) {\r\n            // Atomic increment to get output position\r\n            int output_idx = atomicAdd(output_count, 1);\r\n            output_points[output_idx] = point;\r\n        }\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(e.h2,{id:"memory-optimization-techniques",children:"Memory Optimization Techniques"}),"\n",(0,t.jsx)(e.h3,{id:"memory-coalescing",children:"Memory Coalescing"}),"\n",(0,t.jsx)(e.p,{children:"Memory coalescing is crucial for achieving optimal memory bandwidth:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"// GOOD: Coalesced memory access\r\n__global__ void coalesced_access(float* input, float* output, int n) {\r\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\r\n\r\n    if (idx < n) {\r\n        // Consecutive threads access consecutive memory locations\r\n        output[idx] = input[idx] * 2.0f;\r\n    }\r\n}\r\n\r\n// BAD: Strided memory access (poor coalescing)\r\n__global__ void strided_access(float* input, float* output, int n, int stride) {\r\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\r\n\r\n    if (idx < n) {\r\n        // Threads access memory with stride, causing poor coalescing\r\n        output[idx] = input[idx * stride] * 2.0f;\r\n    }\r\n}\r\n\r\n// Optimized matrix transpose to demonstrate coalescing\r\n__global__ void transpose_coalesced(\r\n    float* input,\r\n    float* output,\r\n    int width,\r\n    int height\r\n) {\r\n    // Use shared memory to improve coalescing\r\n    __shared__ float tile[16][17];  // 17 to avoid bank conflicts\r\n\r\n    int x = blockIdx.x * 16 + threadIdx.x;\r\n    int y = blockIdx.y * 16 + threadIdx.y;\r\n\r\n    // Read input in coalesced manner\r\n    for (int i = 0; i < 16; i += blockDim.y) {\r\n        if (y + i < height && x < width) {\r\n            tile[threadIdx.y + i][threadIdx.x] = input[(y + i) * width + x];\r\n        }\r\n    }\r\n\r\n    __syncthreads();\r\n\r\n    // Write output in coalesced manner\r\n    x = blockIdx.y * 16 + threadIdx.x;\r\n    y = blockIdx.x * 16 + threadIdx.y;\r\n\r\n    for (int i = 0; i < 16; i += blockDim.x) {\r\n        if (y + i < width && x < height) {\r\n            output[(y + i) * height + x] = tile[threadIdx.x][threadIdx.y + i];\r\n        }\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(e.h3,{id:"memory-pool-management",children:"Memory Pool Management"}),"\n",(0,t.jsx)(e.p,{children:"Efficient GPU memory management for robotics applications:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"class GPUMemoryPool {\r\nprivate:\r\n    void* pool_start;\r\n    size_t pool_size;\r\n    std::vector<std::pair<size_t, size_t>> free_blocks; // (offset, size)\r\n\r\npublic:\r\n    GPUMemoryPool(size_t size) : pool_size(size) {\r\n        cudaMalloc(&pool_start, size);\r\n        free_blocks.push_back({0, size});\r\n    }\r\n\r\n    ~GPUMemoryPool() {\r\n        cudaFree(pool_start);\r\n    }\r\n\r\n    void* allocate(size_t size) {\r\n        // Find suitable free block\r\n        for (auto it = free_blocks.begin(); it != free_blocks.end(); ++it) {\r\n            if (it->second >= size) {\r\n                size_t offset = it->first;\r\n                size_t remaining = it->second - size;\r\n\r\n                if (remaining > 0) {\r\n                    // Split the block\r\n                    it->first += size;\r\n                    it->second = remaining;\r\n                } else {\r\n                    // Use entire block\r\n                    free_blocks.erase(it);\r\n                }\r\n\r\n                return static_cast<char*>(pool_start) + offset;\r\n            }\r\n        }\r\n\r\n        return nullptr; // Allocation failed\r\n    }\r\n\r\n    void deallocate(void* ptr, size_t size) {\r\n        size_t offset = static_cast<char*>(ptr) - static_cast<char*>(pool_start);\r\n\r\n        // Add to free blocks (simplified - should merge adjacent blocks)\r\n        free_blocks.push_back({offset, size});\r\n    }\r\n};\r\n\r\n// Usage in robotics pipeline\r\nclass GPURoboticsPipeline {\r\nprivate:\r\n    GPUMemoryPool memory_pool;\r\n    cudaStream_t processing_stream;\r\n\r\npublic:\r\n    GPURoboticsPipeline() : memory_pool(1024 * 1024 * 100) { // 100MB pool\r\n        cudaStreamCreate(&processing_stream);\r\n    }\r\n\r\n    void process_sensor_data(const std::vector<float>& input_data) {\r\n        // Allocate GPU memory from pool\r\n        float* d_input = static_cast<float*>(\r\n            memory_pool.allocate(input_data.size() * sizeof(float))\r\n        );\r\n\r\n        float* d_output = static_cast<float*>(\r\n            memory_pool.allocate(input_data.size() * sizeof(float))\r\n        );\r\n\r\n        // Copy data to GPU\r\n        cudaMemcpyAsync(\r\n            d_input,\r\n            input_data.data(),\r\n            input_data.size() * sizeof(float),\r\n            cudaMemcpyHostToDevice,\r\n            processing_stream\r\n        );\r\n\r\n        // Launch processing kernel\r\n        dim3 blockSize(256);\r\n        dim3 gridSize((input_data.size() + blockSize.x - 1) / blockSize.x);\r\n\r\n        process_kernel<<<gridSize, blockSize, 0, processing_stream>>>(\r\n            d_input, d_output, input_data.size()\r\n        );\r\n\r\n        // Copy results back\r\n        std::vector<float> output_data(input_data.size());\r\n        cudaMemcpyAsync(\r\n            output_data.data(),\r\n            d_output,\r\n            output_data.size() * sizeof(float),\r\n            cudaMemcpyDeviceToHost,\r\n            processing_stream\r\n        );\r\n\r\n        cudaStreamSynchronize(processing_stream);\r\n\r\n        // Return memory to pool\r\n        memory_pool.deallocate(d_input, input_data.size() * sizeof(float));\r\n        memory_pool.deallocate(d_output, input_data.size() * sizeof(float));\r\n    }\r\n};\n"})}),"\n",(0,t.jsx)(e.h2,{id:"gpu-accelerated-robotics-algorithms",children:"GPU-Accelerated Robotics Algorithms"}),"\n",(0,t.jsx)(e.h3,{id:"gpu-accelerated-path-planning",children:"GPU-Accelerated Path Planning"}),"\n",(0,t.jsx)(e.p,{children:"A* path planning can benefit from GPU parallelization for certain operations:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"#include <cuda_runtime.h>\r\n#include <thrust/device_vector.h>\r\n#include <thrust/sort.h>\r\n#include <thrust/scan.h>\r\n\r\n// CUDA kernel for parallel distance calculation in path planning\r\n__global__ void calculate_distances_kernel(\r\n    float* distances,\r\n    float* points_x, float* points_y,\r\n    float goal_x, float goal_y,\r\n    int num_points\r\n) {\r\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\r\n\r\n    if (idx < num_points) {\r\n        float dx = points_x[idx] - goal_x;\r\n        float dy = points_y[idx] - goal_y;\r\n        distances[idx] = sqrtf(dx * dx + dy * dy);\r\n    }\r\n}\r\n\r\n// GPU implementation of grid-based path planning\r\nclass GPUPathPlanner {\r\nprivate:\r\n    float* d_grid;           // Occupancy grid on GPU\r\n    float* d_costs;          // Cost grid on GPU\r\n    int* d_parent_indices;   // Parent tracking on GPU\r\n    bool* d_open_set;        // Open set flags on GPU\r\n    bool* d_closed_set;      // Closed set flags on GPU\r\n\r\n    int grid_width, grid_height;\r\n\r\npublic:\r\n    GPUPathPlanner(int width, int height)\r\n        : grid_width(width), grid_height(height) {\r\n\r\n        size_t grid_size = width * height * sizeof(float);\r\n        size_t bool_size = width * height * sizeof(bool);\r\n\r\n        cudaMalloc(&d_grid, grid_size);\r\n        cudaMalloc(&d_costs, grid_size);\r\n        cudaMalloc(&d_parent_indices, width * height * sizeof(int));\r\n        cudaMalloc(&d_open_set, bool_size);\r\n        cudaMalloc(&d_closed_set, bool_size);\r\n    }\r\n\r\n    ~GPUPathPlanner() {\r\n        cudaFree(d_grid);\r\n        cudaFree(d_costs);\r\n        cudaFree(d_parent_indices);\r\n        cudaFree(d_open_set);\r\n        cudaFree(d_closed_set);\r\n    }\r\n\r\n    std::vector<int> plan_path_gpu(\r\n        const std::vector<float>& host_grid,\r\n        int start_x, int start_y,\r\n        int goal_x, int goal_y\r\n    ) {\r\n        // Copy grid to GPU\r\n        cudaMemcpy(d_grid, host_grid.data(),\r\n                  grid_width * grid_height * sizeof(float),\r\n                  cudaMemcpyHostToDevice);\r\n\r\n        // Initialize costs and sets\r\n        cudaMemset(d_costs, 0xFF, grid_width * grid_height * sizeof(float)); // INF\r\n        cudaMemset(d_open_set, 0, grid_width * grid_height * sizeof(bool));\r\n        cudaMemset(d_closed_set, 0, grid_width * grid_height * sizeof(bool));\r\n\r\n        // Set start cost to 0\r\n        float zero = 0.0f;\r\n        cudaMemcpy(&d_costs[start_y * grid_width + start_x], &zero,\r\n                  sizeof(float), cudaMemcpyHostToDevice);\r\n\r\n        // Set start in open set\r\n        bool true_val = true;\r\n        cudaMemcpy(&d_open_set[start_y * grid_width + start_x], &true_val,\r\n                  sizeof(bool), cudaMemcpyHostToDevice);\r\n\r\n        // A* algorithm implementation on GPU (simplified)\r\n        for (int iteration = 0; iteration < 1000; iteration++) {\r\n            // Find minimum cost in open set (this is complex to parallelize)\r\n            // In practice, you'd use a more sophisticated approach\r\n            if (!expand_open_set()) break;\r\n        }\r\n\r\n        // Extract path\r\n        return extract_path(goal_x, goal_y);\r\n    }\r\n\r\nprivate:\r\n    bool expand_open_set() {\r\n        // Implementation would expand the open set in parallel\r\n        // This is a simplified placeholder\r\n        return true;\r\n    }\r\n\r\n    std::vector<int> extract_path(int goal_x, int goal_y) {\r\n        // Extract path from parent indices\r\n        std::vector<int> path;\r\n        // Implementation would trace back from goal using parent indices\r\n        return path;\r\n    }\r\n};\n"})}),"\n",(0,t.jsx)(e.h3,{id:"gpu-accelerated-slam",children:"GPU-Accelerated SLAM"}),"\n",(0,t.jsx)(e.p,{children:"Simultaneous Localization and Mapping (SLAM) benefits significantly from GPU acceleration:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"// GPU-accelerated point cloud registration for SLAM\r\n__global__ void icp_kernel(\r\n    float3* source_points,\r\n    float3* target_points,\r\n    float* correspondence_distances,\r\n    int* correspondence_indices,\r\n    int num_points,\r\n    float3 transform_translation,\r\n    float4 transform_rotation\r\n) {\r\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\r\n\r\n    if (idx < num_points) {\r\n        // Transform source point\r\n        float3 transformed_point = transform_point(\r\n            source_points[idx], transform_translation, transform_rotation\r\n        );\r\n\r\n        // Find nearest neighbor in target (simplified - would use KD-tree in practice)\r\n        float min_distance = FLT_MAX;\r\n        int best_match = -1;\r\n\r\n        for (int i = 0; i < num_points; i++) {\r\n            float3 diff = make_float3(\r\n                target_points[i].x - transformed_point.x,\r\n                target_points[i].y - transformed_point.y,\r\n                target_points[i].z - transformed_point.z\r\n            );\r\n            float distance = length(diff);\r\n\r\n            if (distance < min_distance) {\r\n                min_distance = distance;\r\n                best_match = i;\r\n            }\r\n        }\r\n\r\n        correspondence_distances[idx] = min_distance;\r\n        correspondence_indices[idx] = best_match;\r\n    }\r\n}\r\n\r\n// GPU-based occupancy grid mapping\r\n__global__ void update_occupancy_grid_kernel(\r\n    float* grid,\r\n    float3* lidar_points,\r\n    int3 grid_dims,\r\n    float3 grid_origin,\r\n    float resolution,\r\n    int num_points,\r\n    float* sensor_pose  // 4x4 transformation matrix\r\n) {\r\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\r\n\r\n    if (idx < num_points) {\r\n        // Transform point to grid coordinates\r\n        float3 world_point = lidar_points[idx];\r\n\r\n        // Apply sensor pose transformation\r\n        float3 transformed_point = transform_point_by_matrix(\r\n            world_point, sensor_pose\r\n        );\r\n\r\n        // Convert to grid coordinates\r\n        int3 grid_coords;\r\n        grid_coords.x = (int)((transformed_point.x - grid_origin.x) / resolution);\r\n        grid_coords.y = (int)((transformed_point.y - grid_origin.y) / resolution);\r\n        grid_coords.z = (int)((transformed_point.z - grid_origin.z) / resolution);\r\n\r\n        // Update occupancy probability using ray casting\r\n        if (grid_coords.x >= 0 && grid_coords.x < grid_dims.x &&\r\n            grid_coords.y >= 0 && grid_coords.y < grid_dims.y &&\r\n            grid_coords.z >= 0 && grid_coords.z < grid_dims.z) {\r\n\r\n            int grid_idx = grid_coords.z * grid_dims.x * grid_dims.y +\r\n                          grid_coords.y * grid_dims.x + grid_coords.x;\r\n\r\n            // Update occupancy probability (simplified)\r\n            atomicAdd(&grid[grid_idx], 0.1f); // Increment probability\r\n        }\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(e.h2,{id:"isaac-ros-gpu-optimization",children:"Isaac ROS GPU Optimization"}),"\n",(0,t.jsx)(e.h3,{id:"isaac-ros-gpu-packages",children:"Isaac ROS GPU Packages"}),"\n",(0,t.jsx)(e.p,{children:"Isaac provides optimized GPU packages for robotics applications:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, PointCloud2\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom std_msgs.msg import Float32\r\nimport numpy as np\r\nimport cupy as cp  # Use CuPy for GPU-accelerated NumPy operations\r\n\r\nclass IsaacGPUOptimizerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_gpu_optimizer\')\r\n\r\n        # Publishers for GPU-processed data\r\n        self.gpu_processed_image_pub = self.create_publisher(\r\n            Image, \'gpu_processed_image\', 10\r\n        )\r\n\r\n        self.gpu_processed_cloud_pub = self.create_publisher(\r\n            PointCloud2, \'gpu_processed_pointcloud\', 10\r\n        )\r\n\r\n        self.performance_pub = self.create_publisher(\r\n            Float32, \'gpu_performance_metric\', 10\r\n        )\r\n\r\n        # Subscribers for sensor data\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'camera/image_raw\', self.image_callback, 10\r\n        )\r\n\r\n        self.pointcloud_sub = self.create_subscription(\r\n            PointCloud2, \'lidar/points\', self.pointcloud_callback, 10\r\n        )\r\n\r\n        # GPU memory management\r\n        self.gpu_memory_pool = cp.cuda.MemoryPool()\r\n        cp.cuda.set_allocator(self.gpu_memory_pool.malloc)\r\n\r\n        # Performance tracking\r\n        self.processing_times = []\r\n\r\n    def image_callback(self, msg):\r\n        """Process image using GPU acceleration"""\r\n        start_time = self.get_clock().now()\r\n\r\n        try:\r\n            # Convert ROS image to CuPy array (GPU memory)\r\n            cpu_image = self.ros_image_to_numpy(msg)\r\n            gpu_image = cp.asarray(cpu_image)\r\n\r\n            # Perform GPU-accelerated image processing\r\n            processed_gpu_image = self.gpu_image_processing(gpu_image)\r\n\r\n            # Convert back to CPU and publish\r\n            processed_cpu_image = cp.asnumpy(processed_gpu_image)\r\n            self.publish_processed_image(processed_cpu_image, msg.header)\r\n\r\n            # Track performance\r\n            end_time = self.get_clock().now()\r\n            processing_time = (end_time - start_time).nanoseconds / 1e9\r\n            self.processing_times.append(processing_time)\r\n\r\n            if len(self.processing_times) > 100:\r\n                self.processing_times.pop(0)\r\n\r\n            avg_time = np.mean(self.processing_times)\r\n            self.performance_pub.publish(Float32(data=1.0/avg_time))  # FPS\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'GPU image processing failed: {e}\')\r\n\r\n    def gpu_image_processing(self, gpu_image):\r\n        """Perform GPU-accelerated image processing operations"""\r\n        # Example: GPU-accelerated image filtering\r\n        if gpu_image.ndim == 3:  # Color image\r\n            # Separate RGB channels\r\n            r_channel = gpu_image[:, :, 0]\r\n            g_channel = gpu_image[:, :, 1]\r\n            b_channel = gpu_image[:, :, 2]\r\n\r\n            # Apply GPU-accelerated filters to each channel\r\n            r_filtered = self.gpu_gaussian_filter(r_channel)\r\n            g_filtered = self.gpu_gaussian_filter(g_channel)\r\n            b_filtered = self.gpu_gaussian_filter(b_channel)\r\n\r\n            # Combine channels back\r\n            result = cp.stack([r_filtered, g_filtered, b_filtered], axis=2)\r\n        else:  # Grayscale\r\n            result = self.gpu_gaussian_filter(gpu_image)\r\n\r\n        return result\r\n\r\n    def gpu_gaussian_filter(self, image):\r\n        """Apply Gaussian filter using GPU"""\r\n        # This is a simplified example\r\n        # In practice, you\'d use CuPy\'s filtering functions or implement custom kernels\r\n        kernel = cp.array([\r\n            [1, 2, 1],\r\n            [2, 4, 2],\r\n            [1, 2, 1]\r\n        ]) / 16.0\r\n\r\n        # Use CuPy\'s convolution functions for actual implementation\r\n        from cupyx.scipy.ndimage import convolve\r\n        return convolve(image.astype(cp.float32), kernel)\r\n\r\n    def pointcloud_callback(self, msg):\r\n        """Process point cloud using GPU acceleration"""\r\n        start_time = self.get_clock().now()\r\n\r\n        try:\r\n            # Convert ROS point cloud to GPU array\r\n            cpu_points = self.ros_pointcloud_to_numpy(msg)\r\n            gpu_points = cp.asarray(cpu_points)\r\n\r\n            # Perform GPU-accelerated point cloud processing\r\n            processed_gpu_points = self.gpu_pointcloud_processing(gpu_points)\r\n\r\n            # Convert back and publish\r\n            processed_cpu_points = cp.asnumpy(processed_gpu_points)\r\n            self.publish_processed_pointcloud(processed_cpu_points, msg.header)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'GPU point cloud processing failed: {e}\')\r\n\r\n    def gpu_pointcloud_processing(self, gpu_points):\r\n        """Perform GPU-accelerated point cloud operations"""\r\n        # Example: GPU-accelerated point cloud filtering\r\n        # Filter points within a certain range\r\n        x_coords = gpu_points[:, 0]\r\n        y_coords = gpu_points[:, 1]\r\n        z_coords = gpu_points[:, 2]\r\n\r\n        # Calculate distances from origin\r\n        distances = cp.sqrt(x_coords**2 + y_coords**2 + z_coords**2)\r\n\r\n        # Filter points within range [1m, 10m]\r\n        valid_mask = (distances >= 1.0) & (distances <= 10.0)\r\n\r\n        # Return filtered points\r\n        return gpu_points[valid_mask]\r\n\r\n    def ros_image_to_numpy(self, ros_image):\r\n        """Convert ROS image message to NumPy array"""\r\n        # Implementation depends on image encoding\r\n        # This is a simplified example\r\n        import cv2\r\n        from cv_bridge import CvBridge\r\n        bridge = CvBridge()\r\n        return bridge.imgmsg_to_cv2(ros_image, desired_encoding=\'passthrough\')\r\n\r\n    def publish_processed_image(self, image_array, header):\r\n        """Publish processed image back to ROS"""\r\n        from cv_bridge import CvBridge\r\n        bridge = CvBridge()\r\n        ros_image = bridge.cv2_to_imgmsg(image_array, encoding=\'passthrough\')\r\n        ros_image.header = header\r\n        self.gpu_processed_image_pub.publish(ros_image)\r\n\r\n    def ros_pointcloud_to_numpy(self, ros_pointcloud):\r\n        """Convert ROS point cloud message to NumPy array"""\r\n        # Implementation to convert PointCloud2 to numpy array\r\n        # This is a simplified example\r\n        import sensor_msgs.point_cloud2 as pc2\r\n        points = pc2.read_points(ros_pointcloud, field_names=("x", "y", "z"), skip_nans=True)\r\n        return np.array(list(points))\n'})}),"\n",(0,t.jsx)(e.h2,{id:"performance-profiling-and-optimization",children:"Performance Profiling and Optimization"}),"\n",(0,t.jsx)(e.h3,{id:"gpu-profiling-tools",children:"GPU Profiling Tools"}),"\n",(0,t.jsx)(e.p,{children:"NVIDIA provides several tools for profiling GPU performance in robotics applications:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import time\r\nimport pynvml\r\nfrom typing import Dict, List\r\n\r\nclass GPUProfiler:\r\n    def __init__(self):\r\n        pynvml.nvmlInit()\r\n        self.device_count = pynvml.nvmlDeviceGetCount()\r\n        self.devices = []\r\n\r\n        for i in range(self.device_count):\r\n            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\r\n            self.devices.append(handle)\r\n\r\n    def get_gpu_stats(self) -> List[Dict]:\r\n        \"\"\"Get current GPU statistics\"\"\"\r\n        stats = []\r\n\r\n        for i, device in enumerate(self.devices):\r\n            # Get GPU utilization\r\n            utilization = pynvml.nvmlDeviceGetUtilizationRates(device)\r\n\r\n            # Get memory info\r\n            memory_info = pynvml.nvmlDeviceGetMemoryInfo(device)\r\n\r\n            # Get temperature\r\n            temperature = pynvml.nvmlDeviceGetTemperature(\r\n                device, pynvml.NVML_TEMPERATURE_GPU\r\n            )\r\n\r\n            # Get power usage\r\n            power = pynvml.nvmlDeviceGetPowerUsage(device)\r\n\r\n            stats.append({\r\n                'device_id': i,\r\n                'gpu_utilization': utilization.gpu,\r\n                'memory_utilization': utilization.memory,\r\n                'memory_used': memory_info.used / (1024**3),  # GB\r\n                'memory_total': memory_info.total / (1024**3),  # GB\r\n                'temperature': temperature,\r\n                'power_usage': power / 1000.0  # Convert to watts\r\n            })\r\n\r\n        return stats\r\n\r\n    def profile_gpu_function(self, func, *args, **kwargs):\r\n        \"\"\"Profile a GPU function for performance analysis\"\"\"\r\n        # Get initial stats\r\n        initial_stats = self.get_gpu_stats()\r\n        start_time = time.time()\r\n\r\n        # Execute function\r\n        result = func(*args, **kwargs)\r\n\r\n        # Get final stats\r\n        end_time = time.time()\r\n        final_stats = self.get_gpu_stats()\r\n\r\n        # Calculate metrics\r\n        execution_time = end_time - start_time\r\n        memory_delta = [\r\n            final['memory_used'] - initial['memory_used']\r\n            for initial, final in zip(initial_stats, final_stats)\r\n        ]\r\n\r\n        return {\r\n            'execution_time': execution_time,\r\n            'gpu_stats_initial': initial_stats,\r\n            'gpu_stats_final': final_stats,\r\n            'memory_delta_gb': memory_delta,\r\n            'result': result\r\n        }\r\n\r\n# Usage example\r\ndef example_gpu_function(data_size):\r\n    \"\"\"Example GPU function to profile\"\"\"\r\n    import cupy as cp\r\n\r\n    # Allocate GPU memory\r\n    a = cp.random.random((data_size, data_size))\r\n    b = cp.random.random((data_size, data_size))\r\n\r\n    # Perform GPU computation\r\n    c = cp.dot(a, b)\r\n\r\n    # Synchronize to ensure completion\r\n    cp.cuda.Stream.null.synchronize()\r\n\r\n    return cp.asnumpy(c)\r\n\r\n# Profile the function\r\nprofiler = GPUProfiler()\r\nprofile_result = profiler.profile_gpu_function(example_gpu_function, 1000)\r\nprint(f\"Execution time: {profile_result['execution_time']:.4f}s\")\r\nprint(f\"GPU utilization: {profile_result['gpu_stats_final'][0]['gpu_utilization']}%\")\n"})}),"\n",(0,t.jsx)(e.h3,{id:"optimization-strategies",children:"Optimization Strategies"}),"\n",(0,t.jsx)(e.p,{children:"Several strategies can improve GPU performance in robotics applications:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class GPURoboticsOptimizer:\r\n    def __init__(self):\r\n        self.optimization_strategies = {\r\n            \'memory_coalescing\': True,\r\n            \'shared_memory_usage\': True,\r\n            \'warp_divergence_reduction\': True,\r\n            \'occupancy_optimization\': True\r\n        }\r\n\r\n    def optimize_kernel_launch(self, kernel_func, data_size, preferred_block_size=256):\r\n        """Optimize kernel launch parameters"""\r\n        # Calculate optimal grid size based on data size and block size\r\n        grid_size = (data_size + preferred_block_size - 1) // preferred_block_size\r\n\r\n        # Ensure we don\'t exceed maximum grid dimensions\r\n        max_grid_size = 65535  # Common limit for older GPUs\r\n        if grid_size > max_grid_size:\r\n            grid_size = max_grid_size\r\n\r\n        return grid_size, preferred_block_size\r\n\r\n    def optimize_memory_transfers(self, host_data, device_data, stream=None):\r\n        """Optimize memory transfers using streams and pinned memory"""\r\n        import cupy as cp\r\n\r\n        # Use pinned memory for faster transfers\r\n        if stream is None:\r\n            stream = cp.cuda.Stream()\r\n\r\n        with stream:\r\n            # Copy data asynchronously\r\n            cp.cuda.pinned_memory.copyto(\r\n                cp.asarray(device_data),\r\n                cp.asarray(host_data)\r\n            )\r\n\r\n        return stream\r\n\r\n    def adaptive_optimization(self, performance_metrics):\r\n        """Adjust optimization parameters based on performance feedback"""\r\n        avg_gpu_utilization = performance_metrics.get(\'avg_gpu_utilization\', 0)\r\n        avg_memory_utilization = performance_metrics.get(\'avg_memory_utilization\', 0)\r\n        avg_temperature = performance_metrics.get(\'avg_temperature\', 0)\r\n\r\n        recommendations = []\r\n\r\n        if avg_gpu_utilization < 30:\r\n            # GPU underutilized - consider larger batch sizes or more complex kernels\r\n            recommendations.append("Increase batch size to improve GPU utilization")\r\n\r\n        if avg_memory_utilization > 90:\r\n            # Memory pressure - consider memory pooling or data compression\r\n            recommendations.append("Implement memory pooling to reduce allocation overhead")\r\n\r\n        if avg_temperature > 80:\r\n            # Thermal issues - reduce computational intensity or improve cooling\r\n            recommendations.append("Reduce computational load or improve cooling")\r\n\r\n        return recommendations\r\n\r\n    def kernel_fusion_optimization(self):\r\n        """Combine multiple kernels to reduce kernel launch overhead"""\r\n        # Example: Instead of separate kernels for A, B, and C operations,\r\n        # create a fused kernel that performs A->B->C in one kernel\r\n        pass\r\n\r\n    def dynamic_parallelism_optimization(self):\r\n        """Use dynamic parallelism for variable workloads"""\r\n        # CUDA kernels can launch child kernels for adaptive workloads\r\n        # This is useful for robotics where workloads can vary significantly\r\n        pass\n'})}),"\n",(0,t.jsx)(e.h2,{id:"balancing-cpu-and-gpu-workloads",children:"Balancing CPU and GPU Workloads"}),"\n",(0,t.jsx)(e.h3,{id:"heterogeneous-computing-strategies",children:"Heterogeneous Computing Strategies"}),"\n",(0,t.jsx)(e.p,{children:"Effective robotics systems balance computation between CPU and GPU:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import threading\r\nimport queue\r\nimport time\r\n\r\nclass HeterogeneousPipeline:\r\n    def __init__(self):\r\n        self.cpu_queue = queue.Queue()\r\n        self.gpu_queue = queue.Queue()\r\n        self.result_queue = queue.Queue()\r\n\r\n        # Initialize GPU context\r\n        import cupy as cp\r\n        self.gpu_available = True\r\n\r\n    def cpu_task_processor(self):\r\n        \"\"\"Process tasks that are better suited for CPU\"\"\"\r\n        while True:\r\n            try:\r\n                task = self.cpu_queue.get(timeout=1.0)\r\n                if task is None:  # Shutdown signal\r\n                    break\r\n\r\n                # Process CPU-suitable task (e.g., planning, decision making)\r\n                result = self.process_cpu_task(task)\r\n                self.result_queue.put(result)\r\n\r\n            except queue.Empty:\r\n                continue\r\n\r\n    def gpu_task_processor(self):\r\n        \"\"\"Process tasks that are better suited for GPU\"\"\"\r\n        import cupy as cp\r\n\r\n        while True:\r\n            try:\r\n                task = self.gpu_queue.get(timeout=1.0)\r\n                if task is None:  # Shutdown signal\r\n                    break\r\n\r\n                # Process GPU-suitable task (e.g., image processing, neural networks)\r\n                result = self.process_gpu_task(task)\r\n                self.result_queue.put(result)\r\n\r\n            except queue.Empty:\r\n                continue\r\n\r\n    def process_cpu_task(self, task):\r\n        \"\"\"Process task on CPU\"\"\"\r\n        # Example: Path planning, state machine logic, etc.\r\n        task_type = task.get('type', 'unknown')\r\n\r\n        if task_type == 'path_planning':\r\n            return self.cpu_path_planning(task)\r\n        elif task_type == 'state_machine':\r\n            return self.cpu_state_machine(task)\r\n        else:\r\n            return {'error': f'Unknown CPU task type: {task_type}'}\r\n\r\n    def process_gpu_task(self, task):\r\n        \"\"\"Process task on GPU\"\"\"\r\n        # Example: Image processing, neural network inference, etc.\r\n        task_type = task.get('type', 'unknown')\r\n\r\n        if task_type == 'image_processing':\r\n            return self.gpu_image_processing(task)\r\n        elif task_type == 'neural_network':\r\n            return self.gpu_neural_network(task)\r\n        else:\r\n            return {'error': f'Unknown GPU task type: {task_type}'}\r\n\r\n    def cpu_path_planning(self, task):\r\n        \"\"\"CPU-intensive path planning algorithm\"\"\"\r\n        # Implementation of CPU-based planning (e.g., sampling-based methods)\r\n        import numpy as np\r\n\r\n        start = task['start']\r\n        goal = task['goal']\r\n        obstacles = task['obstacles']\r\n\r\n        # Simplified path planning\r\n        path = [start, goal]  # In reality, this would be a complex algorithm\r\n        return {'path': path, 'success': True}\r\n\r\n    def gpu_image_processing(self, task):\r\n        \"\"\"GPU-accelerated image processing\"\"\"\r\n        import cupy as cp\r\n\r\n        image_data = cp.asarray(task['image_data'])\r\n\r\n        # Perform GPU-accelerated operations\r\n        processed_image = cp.flip(image_data, axis=0)  # Example operation\r\n        result = cp.asnumpy(processed_image)\r\n\r\n        return {'processed_image': result, 'success': True}\r\n\r\n    def dispatch_task(self, task):\r\n        \"\"\"Intelligently dispatch task to CPU or GPU based on characteristics\"\"\"\r\n        task_type = task.get('type', 'unknown')\r\n\r\n        # Heuristic-based dispatch\r\n        if task_type in ['image_processing', 'neural_network', 'point_cloud']:\r\n            self.gpu_queue.put(task)\r\n        elif task_type in ['path_planning', 'state_machine', 'decision_making']:\r\n            self.cpu_queue.put(task)\r\n        else:\r\n            # Default to CPU for unknown task types\r\n            self.cpu_queue.put(task)\r\n\r\n    def start_processing(self):\r\n        \"\"\"Start the processing threads\"\"\"\r\n        cpu_thread = threading.Thread(target=self.cpu_task_processor)\r\n        gpu_thread = threading.Thread(target=self.gpu_task_processor)\r\n\r\n        cpu_thread.start()\r\n        gpu_thread.start()\r\n\r\n        return cpu_thread, gpu_thread\n"})}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"GPU optimization techniques are essential for achieving real-time performance in robotics applications. By understanding GPU architecture, implementing efficient memory management, and strategically balancing workloads between CPU and GPU, robotics systems can achieve the computational performance required for complex tasks like perception, planning, and control."}),"\n",(0,t.jsx)(e.p,{children:"The next section will provide a hands-on lab exercise for implementing a complete Isaac perception system."}),"\n",(0,t.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,t.jsx)(e.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,r){r.d(e,{R:()=>a,x:()=>s});var i=r(6540);const t={},o=i.createContext(t);function a(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);