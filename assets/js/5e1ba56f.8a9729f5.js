"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9717],{7613(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-3-ai-brain/inference-optimization","title":"Neural Network Inference Optimization","description":"Learning Objectives","source":"@site/docs/module-3-ai-brain/inference-optimization.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/inference-optimization","permalink":"/Book-ai-native/docs/module-3-ai-brain/inference-optimization","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/module-3-ai-brain/inference-optimization.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Perception Pipeline Development","permalink":"/Book-ai-native/docs/module-3-ai-brain/perception-pipeline"},"next":{"title":"Path Planning Algorithm Implementation","permalink":"/Book-ai-native/docs/module-3-ai-brain/path-planning"}}');var t=r(4848),a=r(8453);const o={sidebar_position:4},s="Neural Network Inference Optimization",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Inference Optimization",id:"introduction-to-inference-optimization",level:2},{value:"TensorRT: NVIDIA&#39;s Inference Optimization Toolkit",id:"tensorrt-nvidias-inference-optimization-toolkit",level:2},{value:"Overview",id:"overview",level:3},{value:"Key Optimization Techniques",id:"key-optimization-techniques",level:3},{value:"TensorRT Workflow",id:"tensorrt-workflow",level:3},{value:"Quantization Techniques",id:"quantization-techniques",level:2},{value:"FP16 (Half Precision) Quantization",id:"fp16-half-precision-quantization",level:3},{value:"INT8 (8-bit Integer) Quantization",id:"int8-8-bit-integer-quantization",level:3},{value:"Isaac ROS Inference Optimization",id:"isaac-ros-inference-optimization",level:2},{value:"Isaac ROS DNN Inference Package",id:"isaac-ros-dnn-inference-package",level:3},{value:"Performance Profiling and Analysis",id:"performance-profiling-and-analysis",level:2},{value:"Profiling Tools",id:"profiling-tools",level:3},{value:"Performance Measurement Code",id:"performance-measurement-code",level:3},{value:"Memory Management for Inference",id:"memory-management-for-inference",level:2},{value:"GPU Memory Optimization",id:"gpu-memory-optimization",level:3},{value:"Model Architecture Considerations",id:"model-architecture-considerations",level:2},{value:"Efficient Architectures for Edge Deployment",id:"efficient-architectures-for-edge-deployment",level:3},{value:"Model Pruning",id:"model-pruning",level:3},{value:"Isaac-Specific Optimization Patterns",id:"isaac-specific-optimization-patterns",level:2},{value:"Isaac ROS Image Format Converter",id:"isaac-ros-image-format-converter",level:3},{value:"Accuracy vs Speed Trade-offs",id:"accuracy-vs-speed-trade-offs",level:2},{value:"Quantization Impact Analysis",id:"quantization-impact-analysis",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"neural-network-inference-optimization",children:"Neural Network Inference Optimization"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Optimize neural networks for real-time inference on edge hardware"}),"\n",(0,t.jsx)(n.li,{children:"Apply TensorRT optimization techniques for maximum performance"}),"\n",(0,t.jsx)(n.li,{children:"Implement model quantization to reduce computational requirements"}),"\n",(0,t.jsx)(n.li,{children:"Profile and analyze inference performance bottlenecks"}),"\n",(0,t.jsx)(n.li,{children:"Balance accuracy versus speed trade-offs in deployed models"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-inference-optimization",children:"Introduction to Inference Optimization"}),"\n",(0,t.jsx)(n.p,{children:"Neural network inference optimization is critical for deploying AI models on robotic platforms where computational resources are limited and real-time performance is essential. Unlike training where accuracy is the primary concern, inference optimization focuses on achieving the best possible performance within hardware constraints while maintaining acceptable accuracy."}),"\n",(0,t.jsx)(n.p,{children:"Robotic applications have unique requirements for inference optimization:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time constraints"}),": Perception systems must process sensor data within strict time limits"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Power efficiency"}),": Edge devices have limited power budgets that affect computational choices"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Models must maintain performance under varying environmental conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency requirements"}),": Control systems depend on low-latency perception outputs"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"tensorrt-nvidias-inference-optimization-toolkit",children:"TensorRT: NVIDIA's Inference Optimization Toolkit"}),"\n",(0,t.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"TensorRT is NVIDIA's high-performance inference optimizer and runtime that delivers low latency, high-throughput inference for deep learning applications. It's specifically designed for deployment scenarios where performance and efficiency are critical."}),"\n",(0,t.jsx)(n.h3,{id:"key-optimization-techniques",children:"Key Optimization Techniques"}),"\n",(0,t.jsx)(n.p,{children:"TensorRT applies several optimization techniques:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Layer Fusion"}),": Combining multiple operations into single kernels"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Precision Calibration"}),": Converting from FP32 to FP16 or INT8 for speed/efficiency"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kernel Auto-Tuning"}),": Selecting the best algorithms for target hardware"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory Optimization"}),": Reducing memory usage and bandwidth requirements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic Tensor Memory"}),": Efficiently managing temporary memory resources"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"tensorrt-workflow",children:"TensorRT Workflow"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import tensorrt as trt\r\nimport pycuda.driver as cuda\r\nimport pycuda.autoinit\r\nimport numpy as np\r\n\r\nclass TensorRTInference:\r\n    def __init__(self, engine_path):\r\n        self.engine = self.load_engine(engine_path)\r\n        self.context = self.engine.create_execution_context()\r\n        self.stream = cuda.Stream()\r\n\r\n    def load_engine(self, engine_path):\r\n        """Load a pre-built TensorRT engine"""\r\n        with open(engine_path, \'rb\') as f:\r\n            runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\r\n            return runtime.deserialize_cuda_engine(f.read())\r\n\r\n    def infer(self, input_data):\r\n        """Perform inference using TensorRT engine"""\r\n        # Allocate I/O buffers\r\n        inputs, outputs, bindings = self.allocate_buffers(input_data.shape)\r\n\r\n        # Copy input to GPU\r\n        cuda.memcpy_htod_async(inputs[0].device_input, input_data, self.stream)\r\n\r\n        # Run inference\r\n        self.context.execute_async_v2(bindings=bindings, stream_handle=self.stream.handle)\r\n\r\n        # Copy output from GPU\r\n        cuda.memcpy_dtoh_async(outputs[0].host_output, outputs[0].device_output, self.stream)\r\n        self.stream.synchronize()\r\n\r\n        return outputs[0].host_output\r\n\r\n    def allocate_buffers(self, input_shape):\r\n        """Allocate input and output buffers for inference"""\r\n        inputs = []\r\n        outputs = []\r\n        bindings = []\r\n\r\n        for binding in self.engine:\r\n            size = trt.volume(self.engine.get_binding_shape(binding)) * self.engine.max_batch_size\r\n            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\r\n            host_mem = cuda.pagelocked_empty(size, dtype)\r\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\r\n\r\n            bindings.append(int(device_mem))\r\n            if self.engine.binding_is_input(binding):\r\n                inputs.append(HostDeviceMem(host_mem, device_mem))\r\n            else:\r\n                outputs.append(HostDeviceMem(host_mem, device_mem))\r\n\r\n        return inputs, outputs, bindings\r\n\r\nclass HostDeviceMem:\r\n    def __init__(self, host_mem, device_mem):\r\n        self.host = host_mem\r\n        self.device = device_mem\n'})}),"\n",(0,t.jsx)(n.h2,{id:"quantization-techniques",children:"Quantization Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"fp16-half-precision-quantization",children:"FP16 (Half Precision) Quantization"}),"\n",(0,t.jsx)(n.p,{children:"FP16 quantization reduces model size and increases throughput with minimal accuracy loss:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def create_fp16_engine(onnx_model_path):\r\n    """Create TensorRT engine with FP16 precision"""\r\n    builder = trt.Builder(trt.Logger(trt.Logger.WARNING))\r\n    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\r\n    parser = trt.OnnxParser(network, trt.Logger())\r\n\r\n    with open(onnx_model_path, \'rb\') as model:\r\n        parser.parse(model.read())\r\n\r\n    config = builder.create_builder_config()\r\n    config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision\r\n    config.max_workspace_size = 1 << 30  # 1GB workspace\r\n\r\n    return builder.build_engine(network, config)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"int8-8-bit-integer-quantization",children:"INT8 (8-bit Integer) Quantization"}),"\n",(0,t.jsx)(n.p,{children:"INT8 quantization provides significant speedups but requires careful calibration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def create_int8_engine(onnx_model_path, calibration_dataset):\r\n    """Create TensorRT engine with INT8 precision"""\r\n    builder = trt.Builder(trt.Logger(trt.Logger.WARNING))\r\n    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\r\n    parser = trt.OnnxParser(network, trt.Logger())\r\n\r\n    with open(onnx_model_path, \'rb\') as model:\r\n        parser.parse(model.read())\r\n\r\n    config = builder.create_builder_config()\r\n    config.set_flag(trt.BuilderFlag.INT8)  # Enable INT8 precision\r\n    config.max_workspace_size = 1 << 30  # 1GB workspace\r\n\r\n    # Set up INT8 calibration\r\n    config.int8_calibrator = MyCalibrator(calibration_dataset, cache_file="int8_calibration.cache")\r\n\r\n    return builder.build_engine(network, config)\r\n\r\nclass MyCalibrator(trt.IInt8EntropyCalibrator2):\r\n    def __init__(self, calibration_dataset, cache_file):\r\n        super().__init__()\r\n        self.calibration_dataset = calibration_dataset\r\n        self.cache_file = cache_file\r\n        self.current_index = 0\r\n\r\n        # Allocate GPU memory for calibration\r\n        self.device_input = cuda.mem_alloc(self.get_batch_size() * trt.volume(self.get_algorithm_io_size(0)) * 4)\r\n\r\n    def get_batch_size(self):\r\n        return 32\r\n\r\n    def get_algorithm_io_size(self, binding_index):\r\n        # Return the size of the input tensor\r\n        pass\r\n\r\n    def read_calibration_cache(self):\r\n        # Read calibration cache if it exists\r\n        try:\r\n            with open(self.cache_file, "rb") as f:\r\n                return f.read()\r\n        except:\r\n            return None\r\n\r\n    def write_calibration_cache(self, cache):\r\n        # Write calibration cache to file\r\n        with open(self.cache_file, "wb") as f:\r\n            f.write(cache)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-inference-optimization",children:"Isaac ROS Inference Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-dnn-inference-package",children:"Isaac ROS DNN Inference Package"}),"\n",(0,t.jsxs)(n.p,{children:["Isaac ROS provides optimized inference capabilities through the ",(0,t.jsx)(n.code,{children:"isaac_ros_dnn_inference"})," package:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom isaac_ros_dnn_inference_interfaces.msg import InferenceArray\r\nfrom cv_bridge import CvBridge\r\n\r\nclass OptimizedInferenceNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'optimized_inference_node\')\r\n\r\n        # Subscribe to image input\r\n        self.subscription = self.create_subscription(\r\n            Image,\r\n            \'input_image\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publish inference results\r\n        self.publisher = self.create_publisher(\r\n            InferenceArray,\r\n            \'inference_results\',\r\n            10\r\n        )\r\n\r\n        self.bridge = CvBridge()\r\n        self.tensorrt_inference = TensorRTInference(\'model.plan\')\r\n\r\n    def image_callback(self, msg):\r\n        """Process image with optimized inference"""\r\n        # Convert ROS image to numpy array\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\r\n\r\n        # Preprocess image for inference\r\n        input_tensor = self.preprocess_image(cv_image)\r\n\r\n        # Run optimized inference\r\n        results = self.tensorrt_inference.infer(input_tensor)\r\n\r\n        # Convert results to ROS message\r\n        inference_msg = self.create_inference_message(results, msg.header)\r\n\r\n        # Publish results\r\n        self.publisher.publish(inference_msg)\r\n\r\n    def preprocess_image(self, image):\r\n        """Preprocess image for neural network input"""\r\n        # Resize, normalize, and format image for network\r\n        processed = cv2.resize(image, (224, 224))\r\n        processed = processed.astype(np.float32)\r\n        processed = processed / 255.0  # Normalize to [0,1]\r\n        processed = np.transpose(processed, (2, 0, 1))  # CHW format\r\n        processed = np.expand_dims(processed, axis=0)  # Add batch dimension\r\n        return processed\r\n\r\n    def create_inference_message(self, results, header):\r\n        """Create ROS message from inference results"""\r\n        inference_msg = InferenceArray()\r\n        inference_msg.header = header\r\n\r\n        # Process and format inference results\r\n        for i, result in enumerate(results):\r\n            inference = Inference()\r\n            inference.label = str(i)\r\n            inference.confidence = float(result)\r\n            inference_msg.inferences.append(inference)\r\n\r\n        return inference_msg\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-profiling-and-analysis",children:"Performance Profiling and Analysis"}),"\n",(0,t.jsx)(n.h3,{id:"profiling-tools",children:"Profiling Tools"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA provides several tools for profiling inference performance:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nsight Systems"}),": System-wide performance analysis"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nsight Compute"}),": CUDA kernel performance analysis"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TensorRT Profiler"}),": TensorRT-specific performance analysis"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"performance-measurement-code",children:"Performance Measurement Code"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import time\r\nimport numpy as np\r\n\r\nclass InferenceProfiler:\r\n    def __init__(self, inference_engine):\r\n        self.engine = inference_engine\r\n        self.latency_samples = []\r\n        self.throughput_samples = []\r\n\r\n    def profile_inference(self, input_data, num_runs=100):\r\n        """Profile inference performance"""\r\n        # Warm up the engine\r\n        for _ in range(10):\r\n            _ = self.engine.infer(input_data)\r\n\r\n        # Measure performance\r\n        start_time = time.time()\r\n        for i in range(num_runs):\r\n            inference_start = time.time()\r\n            result = self.engine.infer(input_data)\r\n            inference_end = time.time()\r\n\r\n            latency = inference_end - inference_start\r\n            self.latency_samples.append(latency)\r\n\r\n        total_time = time.time() - start_time\r\n        throughput = num_runs / total_time\r\n\r\n        return {\r\n            \'avg_latency\': np.mean(self.latency_samples),\r\n            \'std_latency\': np.std(self.latency_samples),\r\n            \'min_latency\': np.min(self.latency_samples),\r\n            \'max_latency\': np.max(self.latency_samples),\r\n            \'throughput\': throughput,\r\n            \'total_time\': total_time\r\n        }\r\n\r\n    def print_performance_report(self, profile_results):\r\n        """Print detailed performance report"""\r\n        print("=== Inference Performance Report ===")\r\n        print(f"Average Latency: {profile_results[\'avg_latency\']:.4f} seconds")\r\n        print(f"Min Latency: {profile_results[\'min_latency\']:.4f} seconds")\r\n        print(f"Max Latency: {profile_results[\'max_latency\']:.4f} seconds")\r\n        print(f"Throughput: {profile_results[\'throughput\']:.2f} inferences/second")\r\n        print(f"Total Processing Time: {profile_results[\'total_time\']:.2f} seconds")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"memory-management-for-inference",children:"Memory Management for Inference"}),"\n",(0,t.jsx)(n.h3,{id:"gpu-memory-optimization",children:"GPU Memory Optimization"}),"\n",(0,t.jsx)(n.p,{children:"Efficient GPU memory management is crucial for inference performance:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MemoryOptimizedInference:\r\n    def __init__(self, model_path):\r\n        self.engine = self.load_engine(model_path)\r\n        self.context = self.engine.create_execution_context()\r\n\r\n        # Pre-allocate memory pools to avoid runtime allocation\r\n        self.input_buffer = self.allocate_persistent_buffer(self.get_input_shape())\r\n        self.output_buffer = self.allocate_persistent_buffer(self.get_output_shape())\r\n\r\n    def allocate_persistent_buffer(self, shape):\r\n        """Allocate persistent GPU memory buffer"""\r\n        dtype = np.float32\r\n        size = np.prod(shape) * dtype().itemsize\r\n        return cuda.mem_alloc(size)\r\n\r\n    def infer_with_persistent_memory(self, input_data):\r\n        """Perform inference using persistent memory allocation"""\r\n        # Copy input data to pre-allocated buffer\r\n        cuda.memcpy_htod(self.input_buffer, input_data.astype(np.float32))\r\n\r\n        # Set binding for input\r\n        self.context.set_binding_address(0, int(self.input_buffer))\r\n\r\n        # Set binding for output\r\n        self.context.set_binding_address(1, int(self.output_buffer))\r\n\r\n        # Run inference\r\n        self.context.execute_v2([])\r\n\r\n        # Copy result from persistent output buffer\r\n        output_data = np.empty(self.get_output_shape(), dtype=np.float32)\r\n        cuda.memcpy_dtoh(output_data, self.output_buffer)\r\n\r\n        return output_data\n'})}),"\n",(0,t.jsx)(n.h2,{id:"model-architecture-considerations",children:"Model Architecture Considerations"}),"\n",(0,t.jsx)(n.h3,{id:"efficient-architectures-for-edge-deployment",children:"Efficient Architectures for Edge Deployment"}),"\n",(0,t.jsx)(n.p,{children:"Certain neural network architectures are better suited for edge deployment:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MobileNet"}),": Efficient for image classification on edge devices"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ShuffleNet"}),": Optimized for mobile and edge scenarios"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"EfficientNet"}),": Balances accuracy and efficiency"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"YOLO variants"}),": Efficient for real-time object detection"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"model-pruning",children:"Model Pruning"}),"\n",(0,t.jsx)(n.p,{children:"Model pruning removes unnecessary weights to reduce computational requirements:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def prune_model(model, sparsity_ratio=0.5):\r\n    """Prune model to reduce computational requirements"""\r\n    import torch\r\n    import torch.nn.utils.prune as prune\r\n\r\n    # Apply unstructured pruning to all convolutional layers\r\n    for module in model.modules():\r\n        if isinstance(module, torch.nn.Conv2d):\r\n            prune.l1_unstructured(module, name=\'weight\', amount=sparsity_ratio)\r\n\r\n    # Remove pruning reparametrization to make model static\r\n    for module in model.modules():\r\n        if isinstance(module, torch.nn.Conv2d):\r\n            prune.remove(module, \'weight\')\r\n\r\n    return model\n'})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-specific-optimization-patterns",children:"Isaac-Specific Optimization Patterns"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-image-format-converter",children:"Isaac ROS Image Format Converter"}),"\n",(0,t.jsx)(n.p,{children:"Using Isaac's optimized image format conversion:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Launch file for optimized image processing\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import ComposableNodeContainer\r\nfrom launch_ros.descriptions import ComposableNode\r\n\r\ndef generate_launch_description():\r\n    \"\"\"Launch optimized image processing pipeline\"\"\"\r\n    container = ComposableNodeContainer(\r\n        name='inference_optimized_container',\r\n        namespace='',\r\n        package='rclcpp_components',\r\n        executable='component_container_mt',\r\n        composable_node_descriptions=[\r\n            ComposableNode(\r\n                package='isaac_ros_image_proc',\r\n                plugin='isaac_ros::ImageFormatConverterNode',\r\n                name='image_format_converter',\r\n                parameters=[{\r\n                    'encoding_desired': 'rgb8',\r\n                    'image_width': 640,\r\n                    'image_height': 480\r\n                }]\r\n            ),\r\n            ComposableNode(\r\n                package='isaac_ros_dnn_inference',\r\n                plugin='nvidia::isaac_ros::dnn_inference::ImageEncoderNode',\r\n                name='tensor_rt_encoder',\r\n                parameters=[{\r\n                    'model_path': '/path/to/optimized/model.plan',\r\n                    'input_tensor_names': ['input'],\r\n                    'output_tensor_names': ['output'],\r\n                    'tensor_formats': ['nitros_tensor_list'],\r\n                    'model_input_width': 224,\r\n                    'model_input_height': 224,\r\n                    'model_input_channel': 3,\r\n                    'model_tensorRT_engine_file': '/path/to/tensorrt/engine.plan'\r\n                }]\r\n            )\r\n        ]\r\n    )\r\n\r\n    return LaunchDescription([container])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"accuracy-vs-speed-trade-offs",children:"Accuracy vs Speed Trade-offs"}),"\n",(0,t.jsx)(n.h3,{id:"quantization-impact-analysis",children:"Quantization Impact Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def analyze_quantization_impact(original_model, quantized_model, test_dataset):\r\n    """Analyze the impact of quantization on model accuracy"""\r\n    original_accuracies = []\r\n    quantized_accuracies = []\r\n\r\n    for batch in test_dataset:\r\n        # Get predictions from original model\r\n        orig_pred = original_model(batch)\r\n        orig_acc = calculate_accuracy(orig_pred, batch.labels)\r\n        original_accuracies.append(orig_acc)\r\n\r\n        # Get predictions from quantized model\r\n        quant_pred = quantized_model(batch)\r\n        quant_acc = calculate_accuracy(quant_pred, batch.labels)\r\n        quantized_accuracies.append(quant_acc)\r\n\r\n    original_mean = np.mean(original_accuracies)\r\n    quantized_mean = np.mean(quantized_accuracies)\r\n    accuracy_drop = original_mean - quantized_mean\r\n\r\n    print(f"Original Model Accuracy: {original_mean:.4f}")\r\n    print(f"Quantized Model Accuracy: {quantized_mean:.4f}")\r\n    print(f"Accuracy Drop: {accuracy_drop:.4f}")\r\n    print(f"Performance Improvement: {get_speedup_ratio(original_model, quantized_model):.2f}x")\r\n\r\ndef get_speedup_ratio(original_model, quantized_model):\r\n    """Calculate performance speedup ratio"""\r\n    # Profile both models and return speedup ratio\r\n    pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Neural network inference optimization is essential for deploying AI models on robotic platforms. Through TensorRT optimization, quantization techniques, and efficient memory management, we can achieve real-time performance while maintaining acceptable accuracy for robotic applications."}),"\n",(0,t.jsx)(n.p,{children:"The next section will explore path planning algorithms, which use the perception outputs to enable autonomous navigation and movement."}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsx)(n.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>o,x:()=>s});var i=r(6540);const t={},a=i.createContext(t);function o(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);