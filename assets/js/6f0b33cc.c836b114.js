"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[3321],{406(n,r,e){e.r(r),e.d(r,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"appendices/code-samples","title":"Code Samples Reference","description":"Overview","source":"@site/docs/appendices/code-samples.md","sourceDirName":"appendices","slug":"/appendices/code-samples","permalink":"/Book-ai-native/docs/appendices/code-samples","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/appendices/code-samples.md","tags":[],"version":"current","sidebarPosition":33,"frontMatter":{"sidebar_position":33},"sidebar":"tutorialSidebar","previous":{"title":"Troubleshooting Guide","permalink":"/Book-ai-native/docs/appendices/troubleshooting"},"next":{"title":"Simulation Assets Guide","permalink":"/Book-ai-native/docs/appendices/simulation-assets"}}');var a=e(4848),o=e(8453);const i={sidebar_position:33},s="Code Samples Reference",l={},c=[{value:"Overview",id:"overview",level:2},{value:"ROS/ROS2 Fundamentals",id:"rosros2-fundamentals",level:2},{value:"Basic Publisher/Subscriber Pattern",id:"basic-publishersubscriber-pattern",level:3},{value:"C++ Publisher Example",id:"c-publisher-example",level:4},{value:"Python Publisher Example",id:"python-publisher-example",level:4},{value:"C++ Subscriber Example",id:"c-subscriber-example",level:4},{value:"Perception Systems",id:"perception-systems",level:2},{value:"Camera Data Processing",id:"camera-data-processing",level:3},{value:"RGB-D Camera Processing",id:"rgb-d-camera-processing",level:4},{value:"LiDAR Processing",id:"lidar-processing",level:3},{value:"Point Cloud Processing",id:"point-cloud-processing",level:4},{value:"Navigation and Path Planning",id:"navigation-and-path-planning",level:2},{value:"A* Path Planning Algorithm",id:"a-path-planning-algorithm",level:3},{value:"DWA (Dynamic Window Approach) Local Planner",id:"dwa-dynamic-window-approach-local-planner",level:3},{value:"Manipulation and Control",id:"manipulation-and-control",level:2},{value:"Inverse Kinematics",id:"inverse-kinematics",level:3},{value:"AI and Machine Learning Integration",id:"ai-and-machine-learning-integration",level:2},{value:"Vision-Language-Action (VLA) System",id:"vision-language-action-vla-system",level:3},{value:"Simulation Integration",id:"simulation-integration",level:2},{value:"Gazebo ROS2 Interface",id:"gazebo-ros2-interface",level:3},{value:"Hardware Integration",id:"hardware-integration",level:2},{value:"Jetson AI Inference",id:"jetson-ai-inference",level:3},{value:"Safety and Monitoring",id:"safety-and-monitoring",level:2},{value:"Robot Safety Monitor",id:"robot-safety-monitor",level:3},{value:"Utilities and Helper Functions",id:"utilities-and-helper-functions",level:2},{value:"Common Robotics Utilities",id:"common-robotics-utilities",level:3}];function p(n){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(r.header,{children:(0,a.jsx)(r.h1,{id:"code-samples-reference",children:"Code Samples Reference"})}),"\n",(0,a.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(r.p,{children:"This comprehensive code samples reference provides practical implementations and examples for the concepts discussed throughout the Physical AI and Humanoid Robotics book. Each code sample is designed to be educational, practical, and directly applicable to real-world robotics development. The samples cover various aspects of robotics development including perception, planning, control, simulation, and deployment."}),"\n",(0,a.jsx)(r.p,{children:"All code samples are provided in multiple languages where applicable (C++ and Python) and follow best practices for robotics development. Each sample includes detailed comments, error handling, and is designed to be easily integrated into larger robotics systems."}),"\n",(0,a.jsx)(r.h2,{id:"rosros2-fundamentals",children:"ROS/ROS2 Fundamentals"}),"\n",(0,a.jsx)(r.h3,{id:"basic-publishersubscriber-pattern",children:"Basic Publisher/Subscriber Pattern"}),"\n",(0,a.jsx)(r.h4,{id:"c-publisher-example",children:"C++ Publisher Example"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\r\n#include <std_msgs/msg/string.hpp>\r\n\r\nclass MinimalPublisher : public rclcpp::Node\r\n{\r\npublic:\r\n    MinimalPublisher() : Node("minimal_publisher"), count_(0)\r\n    {\r\n        publisher_ = this->create_publisher<std_msgs::msg::String>("topic", 10);\r\n        timer_ = this->create_wall_timer(\r\n            std::chrono::milliseconds(500),\r\n            std::bind(&MinimalPublisher::timer_callback, this));\r\n    }\r\n\r\nprivate:\r\n    void timer_callback()\r\n    {\r\n        auto message = std_msgs::msg::String();\r\n        message.data = "Hello World: " + std::to_string(count_++);\r\n        RCLCPP_INFO(this->get_logger(), "Publishing: \'%s\'", message.data.c_str());\r\n        publisher_->publish(message);\r\n    }\r\n\r\n    rclcpp::TimerBase::SharedPtr timer_;\r\n    rclcpp::Publisher<std_msgs::msg::String>::SharedPtr publisher_;\r\n    size_t count_;\r\n};\r\n\r\nint main(int argc, char * argv[])\r\n{\r\n    rclcpp::init(argc, argv);\r\n    rclcpp::spin(std::make_shared<MinimalPublisher>());\r\n    rclcpp::shutdown();\r\n    return 0;\r\n}\n'})}),"\n",(0,a.jsx)(r.h4,{id:"python-publisher-example",children:"Python Publisher Example"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\n\r\nclass MinimalPublisher(Node):\r\n\r\n    def __init__(self):\r\n        super().__init__('minimal_publisher')\r\n        self.publisher_ = self.create_publisher(String, 'topic', 10)\r\n        timer_period = 0.5  # seconds\r\n        self.timer = self.create_timer(timer_period, self.timer_callback)\r\n        self.i = 0\r\n\r\n    def timer_callback(self):\r\n        msg = String()\r\n        msg.data = 'Hello World: %d' % self.i\r\n        self.publisher_.publish(msg)\r\n        self.get_logger().info('Publishing: \"%s\"' % msg.data)\r\n        self.i += 1\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    minimal_publisher = MinimalPublisher()\r\n    rclpy.spin(minimal_publisher)\r\n    minimal_publisher.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(r.h4,{id:"c-subscriber-example",children:"C++ Subscriber Example"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\r\n#include <std_msgs/msg/string.hpp>\r\n\r\nclass MinimalSubscriber : public rclcpp::Node\r\n{\r\npublic:\r\n    MinimalSubscriber() : Node("minimal_subscriber")\r\n    {\r\n        subscription_ = this->create_subscription<std_msgs::msg::String>(\r\n            "topic", 10,\r\n            std::bind(&MinimalSubscriber::topic_callback, this, std::placeholders::_1));\r\n    }\r\n\r\nprivate:\r\n    void topic_callback(const std_msgs::msg::String::SharedPtr msg) const\r\n    {\r\n        RCLCPP_INFO(this->get_logger(), "I heard: \'%s\'", msg->data.c_str());\r\n    }\r\n\r\n    rclcpp::Subscription<std_msgs::msg::String>::SharedPtr subscription_;\r\n};\r\n\r\nint main(int argc, char * argv[])\r\n{\r\n    rclcpp::init(argc, argv);\r\n    rclcpp::spin(std::make_shared<MinimalSubscriber>());\r\n    rclcpp::shutdown();\r\n    return 0;\r\n}\n'})}),"\n",(0,a.jsx)(r.h2,{id:"perception-systems",children:"Perception Systems"}),"\n",(0,a.jsx)(r.h3,{id:"camera-data-processing",children:"Camera Data Processing"}),"\n",(0,a.jsx)(r.h4,{id:"rgb-d-camera-processing",children:"RGB-D Camera Processing"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom cv_bridge import CvBridge\r\nfrom geometry_msgs.msg import PointStamped\r\n\r\nclass RGBDProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'rgbd_processor\')\r\n\r\n        # Initialize OpenCV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Create subscribers\r\n        self.rgb_sub = self.create_subscription(\r\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10)\r\n        self.depth_sub = self.create_subscription(\r\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10)\r\n        self.info_sub = self.create_subscription(\r\n            CameraInfo, \'/camera/rgb/camera_info\', self.info_callback, 10)\r\n\r\n        # Store camera parameters\r\n        self.camera_matrix = None\r\n        self.distortion_coeffs = None\r\n\r\n        # Processed data storage\r\n        self.rgb_image = None\r\n        self.depth_image = None\r\n\r\n    def info_callback(self, msg):\r\n        """Store camera intrinsic parameters"""\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.distortion_coeffs = np.array(msg.d)\r\n\r\n    def rgb_callback(self, msg):\r\n        """Process RGB image"""\r\n        try:\r\n            self.rgb_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error converting RGB image: {e}\')\r\n\r\n    def depth_callback(self, msg):\r\n        """Process depth image"""\r\n        try:\r\n            # Convert depth image to meters\r\n            depth_image = self.bridge.imgmsg_to_cv2(msg, msg.encoding)\r\n\r\n            # Convert to float32 and scale to meters\r\n            if depth_image.dtype == np.uint16:\r\n                depth_image = depth_image.astype(np.float32) / 1000.0  # mm to meters\r\n            elif depth_image.dtype == np.uint8:\r\n                depth_image = depth_image.astype(np.float32)\r\n\r\n            self.depth_image = depth_image\r\n\r\n            # Process the combined RGB-D data\r\n            if self.rgb_image is not None and self.depth_image is not None:\r\n                self.process_rgbd_data()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error converting depth image: {e}\')\r\n\r\n    def process_rgbd_data(self):\r\n        """Process combined RGB-D data for object detection"""\r\n        # Example: Find objects in the scene\r\n        height, width = self.rgb_image.shape[:2]\r\n\r\n        # Convert to HSV for color-based segmentation\r\n        hsv = cv2.cvtColor(self.rgb_image, cv2.COLOR_BGR2HSV)\r\n\r\n        # Define color range for object detection (example: red objects)\r\n        lower_red = np.array([0, 120, 70])\r\n        upper_red = np.array([10, 255, 255])\r\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\r\n\r\n        lower_red = np.array([170, 120, 70])\r\n        upper_red = np.array([180, 255, 255])\r\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\r\n\r\n        mask = mask1 + mask2\r\n\r\n        # Apply depth mask to filter objects at certain distances\r\n        if self.depth_image is not None:\r\n            # Filter objects within 1-2 meters\r\n            depth_mask = (self.depth_image >= 1.0) & (self.depth_image <= 2.0)\r\n            mask = mask & depth_mask\r\n\r\n        # Find contours\r\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n        # Process detected objects\r\n        for contour in contours:\r\n            if cv2.contourArea(contour) > 500:  # Filter small contours\r\n                # Get bounding box\r\n                x, y, w, h = cv2.boundingRect(contour)\r\n\r\n                # Calculate center in 3D space\r\n                center_x = x + w // 2\r\n                center_y = y + h // 2\r\n\r\n                if center_x < self.depth_image.shape[1] and center_y < self.depth_image.shape[0]:\r\n                    depth = self.depth_image[center_y, center_x]\r\n\r\n                    if depth > 0:  # Valid depth\r\n                        # Convert pixel coordinates to 3D world coordinates\r\n                        world_x, world_y, world_z = self.pixel_to_world(\r\n                            center_x, center_y, depth)\r\n\r\n                        self.get_logger().info(\r\n                            f\'Detected object at: ({world_x:.2f}, {world_y:.2f}, {world_z:.2f})\')\r\n\r\n    def pixel_to_world(self, u, v, depth):\r\n        """Convert pixel coordinates to world coordinates"""\r\n        if self.camera_matrix is None:\r\n            return 0, 0, depth\r\n\r\n        # Camera intrinsic parameters\r\n        fx = self.camera_matrix[0, 0]\r\n        fy = self.camera_matrix[1, 1]\r\n        cx = self.camera_matrix[0, 2]\r\n        cy = self.camera_matrix[1, 2]\r\n\r\n        # Convert to world coordinates\r\n        x = (u - cx) * depth / fx\r\n        y = (v - cy) * depth / fy\r\n        z = depth\r\n\r\n        return x, y, z\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    processor = RGBDProcessor()\r\n    rclpy.spin(processor)\r\n    processor.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(r.h3,{id:"lidar-processing",children:"LiDAR Processing"}),"\n",(0,a.jsx)(r.h4,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\r\n#include <sensor_msgs/msg/point_cloud2.hpp>\r\n#include <pcl_conversions/pcl_conversions.h>\r\n#include <pcl/point_cloud.h>\r\n#include <pcl/point_types.h>\r\n#include <pcl/filters/voxel_grid.h>\r\n#include <pcl/segmentation/sac_segmentation.h>\r\n#include <pcl/filters/extract_indices.h>\r\n#include <pcl/features/normal_3d.h>\r\n#include <pcl/surface/concave_hull.h>\r\n\r\nclass PointCloudProcessor : public rclcpp::Node\r\n{\r\npublic:\r\n    PointCloudProcessor() : Node("pointcloud_processor")\r\n    {\r\n        // Create subscriber for point cloud\r\n        pointcloud_sub_ = this->create_subscription<sensor_msgs::msg::PointCloud2>(\r\n            "/velodyne_points", 10,\r\n            std::bind(&PointCloudProcessor::pointcloud_callback, this, std::placeholders::_1));\r\n\r\n        // Create publisher for processed point cloud\r\n        processed_pub_ = this->create_publisher<sensor_msgs::msg::PointCloud2>(\r\n            "/processed_points", 10);\r\n\r\n        // Create publisher for ground plane\r\n        ground_pub_ = this->create_publisher<sensor_msgs::msg::PointCloud2>(\r\n            "/ground_points", 10);\r\n    }\r\n\r\nprivate:\r\n    void pointcloud_callback(const sensor_msgs::msg::PointCloud2::SharedPtr msg)\r\n    {\r\n        // Convert ROS message to PCL\r\n        pcl::PCLPointCloud2 pcl_pc2;\r\n        pcl_conversions::toPCL(*msg, pcl_pc2);\r\n\r\n        // Convert to PointXYZ\r\n        pcl::PointCloud<pcl::PointXYZ>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZ>);\r\n        pcl::fromPCLPointCloud2(pcl_pc2, *cloud);\r\n\r\n        // Downsample the point cloud\r\n        pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_filtered(new pcl::PointCloud<pcl::PointXYZ>);\r\n        pcl::VoxelGrid<pcl::PointXYZ> voxel_filter;\r\n        voxel_filter.setInputCloud(cloud);\r\n        voxel_filter.setLeafSize(0.1f, 0.1f, 0.1f);\r\n        voxel_filter.filter(*cloud_filtered);\r\n\r\n        // Segment ground plane using SAC segmentation\r\n        pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_no_ground(new pcl::PointCloud<pcl::PointXYZ>);\r\n        pcl::PointCloud<pcl::PointXYZ>::Ptr ground_cloud(new pcl::PointCloud<pcl::PointXYZ>);\r\n\r\n        pcl::ModelCoefficients::Ptr coefficients(new pcl::ModelCoefficients);\r\n        pcl::PointIndices::Ptr inliers(new pcl::PointIndices);\r\n\r\n        // Create segmentation object\r\n        pcl::SACSegmentation<pcl::PointXYZ> seg;\r\n        seg.setOptimizeCoefficients(true);\r\n        seg.setModelType(pcl::SACMODEL_PLANE);\r\n        seg.setMethodType(pcl::SAC_RANSAC);\r\n        seg.setMaxIterations(100);\r\n        seg.setDistanceThreshold(0.2); // 20cm threshold\r\n\r\n        // Segment the largest planar component\r\n        seg.setInputCloud(cloud_filtered);\r\n        seg.segment(*inliers, *coefficients);\r\n\r\n        // Extract ground points and non-ground points\r\n        pcl::ExtractIndices<pcl::PointXYZ> extract;\r\n        extract.setInputCloud(cloud_filtered);\r\n\r\n        // Extract ground\r\n        extract.setIndices(inliers);\r\n        extract.setNegative(false);\r\n        extract.filter(*ground_cloud);\r\n\r\n        // Extract non-ground\r\n        extract.setNegative(true);\r\n        extract.filter(*cloud_no_ground);\r\n\r\n        // Publish ground points\r\n        sensor_msgs::msg::PointCloud2 ground_msg;\r\n        pcl::toPCLPointCloud2(*ground_cloud, pcl_pc2);\r\n        pcl_conversions::fromPCL(pcl_pc2, ground_msg);\r\n        ground_msg.header = msg->header;\r\n        ground_pub_->publish(ground_msg);\r\n\r\n        // Publish processed (non-ground) points\r\n        sensor_msgs::msg::PointCloud2 processed_msg;\r\n        pcl::toPCLPointCloud2(*cloud_no_ground, pcl_pc2);\r\n        pcl_conversions::fromPCL(pcl_pc2, processed_msg);\r\n        processed_msg.header = msg->header;\r\n        processed_pub_->publish(processed_msg);\r\n\r\n        RCLCPP_INFO(this->get_logger(),\r\n            "Processed point cloud: %zu points, %zu ground points",\r\n            cloud_no_ground->size(), ground_cloud->size());\r\n    }\r\n\r\n    rclcpp::Subscription<sensor_msgs::msg::PointCloud2>::SharedPtr pointcloud_sub_;\r\n    rclcpp::Publisher<sensor_msgs::msg::PointCloud2>::SharedPtr processed_pub_;\r\n    rclcpp::Publisher<sensor_msgs::msg::PointCloud2>::SharedPtr ground_pub_;\r\n};\r\n\r\nint main(int argc, char * argv[])\r\n{\r\n    rclcpp::init(argc, argv);\r\n    rclcpp::spin(std::make_shared<PointCloudProcessor>());\r\n    rclcpp::shutdown();\r\n    return 0;\r\n}\n'})}),"\n",(0,a.jsx)(r.h2,{id:"navigation-and-path-planning",children:"Navigation and Path Planning"}),"\n",(0,a.jsx)(r.h3,{id:"a-path-planning-algorithm",children:"A* Path Planning Algorithm"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nimport heapq\r\nfrom typing import List, Tuple, Optional\r\n\r\nclass AStarPlanner:\r\n    def __init__(self, occupancy_grid: np.ndarray, resolution: float = 1.0):\r\n        """\r\n        Initialize A* path planner\r\n\r\n        Args:\r\n            occupancy_grid: 2D numpy array where 0=free, 1=occupied\r\n            resolution: Size of each grid cell in meters\r\n        """\r\n        self.grid = occupancy_grid\r\n        self.resolution = resolution\r\n        self.height, self.width = occupancy_grid.shape\r\n\r\n        # 8-directional movement (including diagonals)\r\n        self.movements = [\r\n            (-1, -1), (-1, 0), (-1, 1),  # Up-left, Up, Up-right\r\n            (0, -1),           (0, 1),   # Left, Right\r\n            (1, -1),  (1, 0),  (1, 1)    # Down-left, Down, Down-right\r\n        ]\r\n\r\n        # Movement costs (diagonal = sqrt(2), straight = 1)\r\n        self.movement_costs = [\r\n            np.sqrt(2), 1, np.sqrt(2),  # Up-left, Up, Up-right\r\n            1,          1,              # Left, Right\r\n            np.sqrt(2), 1, np.sqrt(2)   # Down-left, Down, Down-right\r\n        ]\r\n\r\n    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> float:\r\n        """Calculate heuristic distance (Euclidean) between two points"""\r\n        return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\r\n\r\n    def is_valid(self, x: int, y: int) -> bool:\r\n        """Check if coordinates are valid and not occupied"""\r\n        return (0 <= x < self.width and\r\n                0 <= y < self.height and\r\n                self.grid[y, x] == 0)  # 0 means free space\r\n\r\n    def plan(self, start: Tuple[float, float], goal: Tuple[float, float]) -> Optional[List[Tuple[float, float]]]:\r\n        """\r\n        Plan path using A* algorithm\r\n\r\n        Args:\r\n            start: Start position (x, y) in meters\r\n            goal: Goal position (x, y) in meters\r\n\r\n        Returns:\r\n            List of (x, y) coordinates in meters, or None if no path found\r\n        """\r\n        # Convert meters to grid coordinates\r\n        start_grid = (int(start[0] / self.resolution), int(start[1] / self.resolution))\r\n        goal_grid = (int(goal[0] / self.resolution), int(goal[1] / self.resolution))\r\n\r\n        # Validate start and goal positions\r\n        if not self.is_valid(start_grid[0], start_grid[1]) or not self.is_valid(goal_grid[0], goal_grid[1]):\r\n            print("Start or goal position is occupied or out of bounds")\r\n            return None\r\n\r\n        # Initialize open and closed sets\r\n        open_set = [(0, start_grid)]  # (f_score, (x, y))\r\n        heapq.heapify(open_set)\r\n\r\n        came_from = {}  # For path reconstruction\r\n        g_score = {start_grid: 0}\r\n        f_score = {start_grid: self.heuristic(start_grid, goal_grid)}\r\n\r\n        while open_set:\r\n            current = heapq.heappop(open_set)[1]\r\n\r\n            if current == goal_grid:\r\n                # Reconstruct path\r\n                path = self._reconstruct_path(came_from, current)\r\n                # Convert grid coordinates back to meters\r\n                path_meters = [(x * self.resolution, y * self.resolution) for x, y in path]\r\n                return path_meters\r\n\r\n            for i, (dx, dy) in enumerate(self.movements):\r\n                neighbor = (current[0] + dx, current[1] + dy)\r\n\r\n                if not self.is_valid(neighbor[0], neighbor[1]):\r\n                    continue\r\n\r\n                tentative_g_score = g_score[current] + self.movement_costs[i]\r\n\r\n                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\r\n                    came_from[neighbor] = current\r\n                    g_score[neighbor] = tentative_g_score\r\n                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor, goal_grid)\r\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\r\n\r\n        print("No path found")\r\n        return None\r\n\r\n    def _reconstruct_path(self, came_from: dict, current: Tuple[int, int]) -> List[Tuple[int, int]]:\r\n        """Reconstruct path from came_from dictionary"""\r\n        path = [current]\r\n        while current in came_from:\r\n            current = came_from[current]\r\n            path.append(current)\r\n        path.reverse()\r\n        return path\r\n\r\n# Example usage\r\ndef example_usage():\r\n    # Create a simple occupancy grid (0 = free, 1 = occupied)\r\n    grid = np.array([\r\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n        [0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\r\n        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\r\n        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\r\n        [0, 1, 0, 1, 1, 1, 1, 1, 1, 0],\r\n        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\r\n        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\r\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\r\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\r\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n    ])\r\n\r\n    planner = AStarPlanner(grid, resolution=0.5)\r\n\r\n    start = (0.5, 0.5)  # Start at (0.5m, 0.5m)\r\n    goal = (4.5, 9.5)   # Goal at (4.5m, 9.5m)\r\n\r\n    path = planner.plan(start, goal)\r\n\r\n    if path:\r\n        print(f"Path found with {len(path)} waypoints:")\r\n        for i, (x, y) in enumerate(path):\r\n            print(f"  Waypoint {i}: ({x:.2f}, {y:.2f})")\r\n    else:\r\n        print("No path found")\r\n\r\nif __name__ == "__main__":\r\n    example_usage()\n'})}),"\n",(0,a.jsx)(r.h3,{id:"dwa-dynamic-window-approach-local-planner",children:"DWA (Dynamic Window Approach) Local Planner"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nfrom typing import Tuple, List\r\nimport math\r\n\r\nclass DWAPlanner:\r\n    def __init__(self, robot_radius: float = 0.3, max_speed: float = 1.0, min_speed: float = -0.5):\r\n        """\r\n        Initialize Dynamic Window Approach local planner\r\n\r\n        Args:\r\n            robot_radius: Robot radius for collision checking\r\n            max_speed: Maximum linear speed\r\n            min_speed: Minimum linear speed (can be negative for backward motion)\r\n        """\r\n        self.robot_radius = robot_radius\r\n        self.max_speed = max_speed\r\n        self.min_speed = min_speed\r\n        self.max_yaw_rate = np.pi / 3  # Maximum angular velocity (60 degrees/s)\r\n        self.max_accel = 1.0  # Maximum acceleration\r\n        self.max_delta_yaw_rate = np.pi / 6  # Maximum angular acceleration\r\n        self.dt = 0.1  # Time step for simulation\r\n        self.predict_time = 3.0  # Prediction horizon\r\n        self.to_goal_cost_gain = 0.15\r\n        self.speed_cost_gain = 1.0\r\n        self.obstacle_cost_gain = 1.0\r\n\r\n    def plan(self, state: np.ndarray, goal: np.ndarray, obstacle_list: List[np.ndarray]) -> Tuple[float, float]:\r\n        """\r\n        Plan local trajectory using DWA\r\n\r\n        Args:\r\n            state: Current state [x, y, yaw, v, omega]\r\n            goal: Goal position [x, y]\r\n            obstacle_list: List of obstacles [[x, y], ...]\r\n\r\n        Returns:\r\n            Tuple of (linear_velocity, angular_velocity)\r\n        """\r\n        # Calculate dynamic window\r\n        window = self.calc_dynamic_window(state)\r\n\r\n        # Evaluate trajectories in the window\r\n        best_traj = None\r\n        best_score = float(\'inf\')\r\n\r\n        # Discretize the search space\r\n        v_samples = np.linspace(window[0], window[1], 10)\r\n        omega_samples = np.linspace(window[2], window[3], 10)\r\n\r\n        for v in v_samples:\r\n            for omega in omega_samples:\r\n                # Simulate trajectory\r\n                traj = self.predict_trajectory(state, v, omega)\r\n\r\n                # Calculate costs\r\n                to_goal_cost = self.calc_to_goal_cost(traj, goal)\r\n                speed_cost = self.calc_speed_cost(traj)\r\n                obstacle_cost = self.calc_obstacle_cost(traj, obstacle_list)\r\n\r\n                # Total cost (lower is better)\r\n                total_cost = (self.to_goal_cost_gain * to_goal_cost +\r\n                             self.speed_cost_gain * speed_cost +\r\n                             self.obstacle_cost_gain * obstacle_cost)\r\n\r\n                if total_cost < best_score:\r\n                    best_score = total_cost\r\n                    best_traj = [v, omega]\r\n\r\n        if best_traj is None:\r\n            # If no valid trajectory found, stop\r\n            return 0.0, 0.0\r\n\r\n        return best_traj[0], best_traj[1]\r\n\r\n    def calc_dynamic_window(self, state: np.ndarray) -> np.ndarray:\r\n        """\r\n        Calculate dynamic window based on current state and constraints\r\n\r\n        Args:\r\n            state: Current state [x, y, yaw, v, omega]\r\n\r\n        Returns:\r\n            Dynamic window [v_min, v_max, omega_min, omega_max]\r\n        """\r\n        vs = np.array([self.min_speed, self.max_speed,\r\n                      -self.max_yaw_rate, self.max_yaw_rate])\r\n\r\n        vd = np.array([state[3] - self.max_accel * self.dt,\r\n                      state[3] + self.max_accel * self.dt,\r\n                      state[4] - self.max_delta_yaw_rate * self.dt,\r\n                      state[4] + self.max_delta_yaw_rate * self.dt])\r\n\r\n        # Dynamic window is intersection of velocity space and acceleration space\r\n        dw = np.array([max(vs[0], vd[0]), min(vs[1], vd[1]),\r\n                      max(vs[2], vd[2]), min(vs[3], vd[3])])\r\n        return dw\r\n\r\n    def predict_trajectory(self, state: np.ndarray, v: float, omega: float) -> np.ndarray:\r\n        """\r\n        Predict trajectory for given velocity commands\r\n\r\n        Args:\r\n            state: Current state [x, y, yaw, v, omega]\r\n            v: Linear velocity\r\n            omega: Angular velocity\r\n\r\n        Returns:\r\n            Trajectory array [n_steps x 4] with [x, y, yaw, v]\r\n        """\r\n        state = state.copy()\r\n        trajectory = np.zeros((int(self.predict_time / self.dt), 4))\r\n\r\n        for i in range(len(trajectory)):\r\n            state = self.motion(state, v, omega)\r\n            trajectory[i] = state[:4]  # x, y, yaw, v\r\n\r\n        return trajectory\r\n\r\n    def motion(self, state: np.ndarray, v: float, omega: float) -> np.ndarray:\r\n        """\r\n        Motion model for robot\r\n\r\n        Args:\r\n            state: Current state [x, y, yaw, v, omega]\r\n            v: Linear velocity\r\n            omega: Angular velocity\r\n\r\n        Returns:\r\n            New state after dt time\r\n        """\r\n        state[0] += v * math.cos(state[2]) * self.dt  # x\r\n        state[1] += v * math.sin(state[2]) * self.dt  # y\r\n        state[2] += omega * self.dt  # yaw\r\n        state[3] = v  # v\r\n        state[4] = omega  # omega\r\n        return state\r\n\r\n    def calc_to_goal_cost(self, traj: np.ndarray, goal: np.ndarray) -> float:\r\n        """Calculate cost related to distance to goal"""\r\n        dx = goal[0] - traj[-1, 0]\r\n        dy = goal[1] - traj[-1, 1]\r\n        error_angle = math.atan2(dy, dx)\r\n        cost_angle = error_angle - traj[-1, 2]\r\n        cost = abs(math.atan2(math.sin(cost_angle), math.cos(cost_angle)))\r\n        return cost\r\n\r\n    def calc_speed_cost(self, traj: np.ndarray) -> float:\r\n        """Calculate cost related to speed (prefer higher speeds)"""\r\n        return abs(self.max_speed - traj[-1, 3])\r\n\r\n    def calc_obstacle_cost(self, traj: np.ndarray, obstacle_list: List[np.ndarray]) -> float:\r\n        """Calculate cost related to obstacles"""\r\n        min_dist = float(\'inf\')\r\n\r\n        for point in traj:\r\n            for obs in obstacle_list:\r\n                dist = math.sqrt((point[0] - obs[0])**2 + (point[1] - obs[1])**2)\r\n                if dist <= self.robot_radius:\r\n                    return float(\'inf\')  # Collision\r\n                min_dist = min(min_dist, dist)\r\n\r\n        return 1.0 / min_dist if min_dist != float(\'inf\') else float(\'inf\')\r\n\r\n# Example usage\r\ndef dwa_example():\r\n    planner = DWAPlanner()\r\n\r\n    # Initial state [x, y, yaw, v, omega]\r\n    state = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\r\n\r\n    # Goal position\r\n    goal = np.array([10.0, 10.0])\r\n\r\n    # Obstacle positions\r\n    obstacles = [\r\n        np.array([5.0, 5.0]),\r\n        np.array([7.0, 6.0]),\r\n        np.array([3.0, 8.0])\r\n    ]\r\n\r\n    # Simulate navigation\r\n    path = [state[:2].copy()]\r\n\r\n    for _ in range(100):  # Maximum 100 steps\r\n        v, omega = planner.plan(state, goal, obstacles)\r\n\r\n        # Apply control and update state\r\n        state = planner.motion(state, v, omega)\r\n        path.append(state[:2].copy())\r\n\r\n        # Check if reached goal\r\n        dist_to_goal = np.linalg.norm(state[:2] - goal)\r\n        if dist_to_goal < 0.5:  # Within 0.5m of goal\r\n            print("Goal reached!")\r\n            break\r\n\r\n    print(f"Path length: {len(path)} steps")\r\n    return np.array(path)\r\n\r\nif __name__ == "__main__":\r\n    path = dwa_example()\n'})}),"\n",(0,a.jsx)(r.h2,{id:"manipulation-and-control",children:"Manipulation and Control"}),"\n",(0,a.jsx)(r.h3,{id:"inverse-kinematics",children:"Inverse Kinematics"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nfrom scipy.spatial.transform import Rotation as R\r\nfrom typing import Tuple, List\r\n\r\nclass InverseKinematics:\r\n    def __init__(self, dh_parameters: List[Tuple[float, float, float, float]]):\r\n        """\r\n        Initialize Inverse Kinematics solver using DH parameters\r\n\r\n        Args:\r\n            dh_parameters: List of DH parameters [(a, alpha, d, theta_offset), ...]\r\n        """\r\n        self.dh_params = dh_parameters\r\n        self.n_joints = len(dh_params)\r\n\r\n    def forward_kinematics(self, joint_angles: np.ndarray) -> np.ndarray:\r\n        """\r\n        Calculate forward kinematics\r\n\r\n        Args:\r\n            joint_angles: Joint angles in radians\r\n\r\n        Returns:\r\n            4x4 transformation matrix\r\n        """\r\n        T = np.eye(4)  # Identity matrix\r\n\r\n        for i, (a, alpha, d, theta_offset) in enumerate(self.dh_params):\r\n            theta = joint_angles[i] + theta_offset\r\n\r\n            # DH transformation matrix\r\n            T_i = np.array([\r\n                [np.cos(theta), -np.sin(theta)*np.cos(alpha), np.sin(theta)*np.sin(alpha), a*np.cos(theta)],\r\n                [np.sin(theta), np.cos(theta)*np.cos(alpha), -np.cos(theta)*np.sin(alpha), a*np.sin(theta)],\r\n                [0, np.sin(alpha), np.cos(alpha), d],\r\n                [0, 0, 0, 1]\r\n            ])\r\n\r\n            T = T @ T_i  # Matrix multiplication\r\n\r\n        return T\r\n\r\n    def jacobian(self, joint_angles: np.ndarray) -> np.ndarray:\r\n        """\r\n        Calculate geometric Jacobian matrix\r\n\r\n        Args:\r\n            joint_angles: Joint angles in radians\r\n\r\n        Returns:\r\n            6xN Jacobian matrix (linear and angular velocities)\r\n        """\r\n        n = len(joint_angles)\r\n        J = np.zeros((6, n))\r\n\r\n        # Get end-effector position and orientation\r\n        T_end = self.forward_kinematics(joint_angles)\r\n        end_pos = T_end[:3, 3]\r\n\r\n        # Calculate Jacobian columns\r\n        T_current = np.eye(4)\r\n\r\n        for i in range(n):\r\n            a, alpha, d, theta_offset = self.dh_params[i]\r\n            theta = joint_angles[i] + theta_offset\r\n\r\n            # Joint transformation\r\n            T_i = np.array([\r\n                [np.cos(theta), -np.sin(theta)*np.cos(alpha), np.sin(theta)*np.sin(alpha), a*np.cos(theta)],\r\n                [np.sin(theta), np.cos(theta)*np.cos(alpha), -np.cos(theta)*np.sin(alpha), a*np.sin(theta)],\r\n                [0, np.sin(alpha), np.cos(alpha), d],\r\n                [0, 0, 0, 1]\r\n            ])\r\n\r\n            # Calculate joint position\r\n            joint_pos = T_current[:3, 3]\r\n\r\n            # Calculate z-axis of joint frame\r\n            z_axis = T_current[:3, 2]\r\n\r\n            # Calculate Jacobian column\r\n            if i < n-1:  # Not the last joint (usually rotational)\r\n                # Linear velocity component\r\n                J[:3, i] = np.cross(z_axis, end_pos - joint_pos)\r\n                # Angular velocity component\r\n                J[3:, i] = z_axis\r\n            else:  # Last joint (could be prismatic)\r\n                # Linear velocity component (prismatic joint)\r\n                J[:3, i] = z_axis\r\n                # Angular velocity component\r\n                J[3:, i] = np.array([0, 0, 0])\r\n\r\n            T_current = T_current @ T_i\r\n\r\n        return J\r\n\r\n    def inverse_kinematics(self, target_pose: np.ndarray, initial_joints: np.ndarray,\r\n                          max_iterations: int = 1000, tolerance: float = 1e-4) -> Tuple[np.ndarray, bool]:\r\n        """\r\n        Solve inverse kinematics using Jacobian transpose method\r\n\r\n        Args:\r\n            target_pose: 4x4 target transformation matrix\r\n            initial_joints: Initial joint angles\r\n            max_iterations: Maximum number of iterations\r\n            tolerance: Position/orientation tolerance\r\n\r\n        Returns:\r\n            Tuple of (joint_angles, success_flag)\r\n        """\r\n        joints = initial_joints.copy()\r\n\r\n        for iteration in range(max_iterations):\r\n            # Calculate current pose\r\n            current_pose = self.forward_kinematics(joints)\r\n\r\n            # Calculate error\r\n            pos_error = target_pose[:3, 3] - current_pose[:3, 3]\r\n\r\n            # Calculate orientation error (using rotation matrix difference)\r\n            R_current = current_pose[:3, :3]\r\n            R_target = target_pose[:3, :3]\r\n\r\n            # Use logarithmic map for rotation error (simplified)\r\n            R_error = R_target @ R_current.T\r\n            rotation_error = self.rotation_matrix_to_axis_angle(R_error)\r\n\r\n            # Combine position and orientation errors\r\n            error = np.concatenate([pos_error, rotation_error])\r\n\r\n            # Check convergence\r\n            if np.linalg.norm(error) < tolerance:\r\n                return joints, True\r\n\r\n            # Calculate Jacobian\r\n            J = self.jacobian(joints)\r\n\r\n            # Update joints using Jacobian transpose method\r\n            # Note: For better convergence, consider using damped least squares\r\n            joints += 0.1 * J.T @ error  # Learning rate of 0.1\r\n\r\n            # Apply joint limits if needed\r\n            # joints = np.clip(joints, joint_limits_min, joint_limits_max)\r\n\r\n        return joints, False  # Failed to converge\r\n\r\n    def rotation_matrix_to_axis_angle(self, R: np.ndarray) -> np.ndarray:\r\n        """Convert rotation matrix to axis-angle representation"""\r\n        # Extract angle\r\n        angle = np.arccos(np.clip((np.trace(R) - 1) / 2, -1, 1))\r\n\r\n        if angle < 1e-6:  # Small angle approximation\r\n            return np.array([0, 0, 0])\r\n\r\n        # Extract axis\r\n        axis = np.array([\r\n            R[2, 1] - R[1, 2],\r\n            R[0, 2] - R[2, 0],\r\n            R[1, 0] - R[0, 1]\r\n        ]) / (2 * np.sin(angle))\r\n\r\n        return angle * axis\r\n\r\n# Example usage for a simple 3-DOF planar manipulator\r\ndef ik_example():\r\n    # Define DH parameters for a 3-DOF planar manipulator\r\n    # [a, alpha, d, theta_offset]\r\n    dh_params = [\r\n        (0.5, 0, 0, 0),    # Joint 1\r\n        (0.4, 0, 0, 0),    # Joint 2\r\n        (0.3, 0, 0, 0)     # Joint 3\r\n    ]\r\n\r\n    ik_solver = InverseKinematics(dh_params)\r\n\r\n    # Target pose (simplified - just position in 2D plane)\r\n    target_pose = np.eye(4)\r\n    target_pose[0, 3] = 0.8  # x position\r\n    target_pose[1, 3] = 0.6  # y position\r\n    target_pose[2, 3] = 0.0  # z position\r\n\r\n    # Initial joint angles\r\n    initial_joints = np.array([0.0, 0.0, 0.0])\r\n\r\n    # Solve inverse kinematics\r\n    solution, success = ik_solver.inverse_kinematics(target_pose, initial_joints)\r\n\r\n    if success:\r\n        print(f"Solution found: {solution}")\r\n\r\n        # Verify solution\r\n        final_pose = ik_solver.forward_kinematics(solution)\r\n        print(f"Final position: [{final_pose[0,3]:.3f}, {final_pose[1,3]:.3f}]")\r\n        print(f"Target position: [{target_pose[0,3]:.3f}, {target_pose[1,3]:.3f}]")\r\n    else:\r\n        print("Failed to find solution")\r\n\r\nif __name__ == "__main__":\r\n    ik_example()\n'})}),"\n",(0,a.jsx)(r.h2,{id:"ai-and-machine-learning-integration",children:"AI and Machine Learning Integration"}),"\n",(0,a.jsx)(r.h3,{id:"vision-language-action-vla-system",children:"Vision-Language-Action (VLA) System"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom typing import Dict, List, Tuple, Optional\r\nimport cv2\r\nfrom transformers import CLIPProcessor, CLIPModel\r\nimport openai\r\n\r\nclass VisionLanguageActionSystem(nn.Module):\r\n    def __init__(self, clip_model_name: str = \"openai/clip-vit-base-patch32\"):\r\n        \"\"\"\r\n        Initialize Vision-Language-Action system\r\n\r\n        Args:\r\n            clip_model_name: Name of the CLIP model to use\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        # Load pre-trained CLIP model for vision-language understanding\r\n        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\r\n        self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\r\n\r\n        # Action prediction head\r\n        self.action_predictor = nn.Sequential(\r\n            nn.Linear(512, 256),  # CLIP vision embedding dimension\r\n            nn.ReLU(),\r\n            nn.Linear(256, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 64),   # Action space dimension\r\n            nn.Tanh()             # Normalize to [-1, 1] range\r\n        )\r\n\r\n        # Task planning module\r\n        self.task_planner = nn.Sequential(\r\n            nn.Linear(512 + 64, 256),  # CLIP text + action embedding\r\n            nn.ReLU(),\r\n            nn.Linear(256, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 32)    # Task sequence length\r\n        )\r\n\r\n    def forward(self, images: torch.Tensor, text: List[str]) -> Dict[str, torch.Tensor]:\r\n        \"\"\"\r\n        Forward pass for VLA system\r\n\r\n        Args:\r\n            images: Batch of images [B, C, H, W]\r\n            text: List of text descriptions\r\n\r\n        Returns:\r\n            Dictionary with predictions\r\n        \"\"\"\r\n        # Process images through CLIP vision encoder\r\n        image_features = self.clip_model.get_image_features(pixel_values=images)\r\n\r\n        # Process text through CLIP text encoder\r\n        inputs = self.clip_processor(text=text, return_tensors=\"pt\", padding=True)\r\n        text_features = self.clip_model.get_text_features(**inputs)\r\n\r\n        # Predict actions based on visual input\r\n        action_predictions = self.action_predictor(image_features)\r\n\r\n        # Plan task sequence combining vision and language\r\n        combined_features = torch.cat([image_features, action_predictions], dim=1)\r\n        task_sequence = self.task_planner(combined_features)\r\n\r\n        return {\r\n            'action_predictions': action_predictions,\r\n            'task_sequence': task_sequence,\r\n            'image_features': image_features,\r\n            'text_features': text_features\r\n        }\r\n\r\n    def process_command(self, image: np.ndarray, command: str) -> Dict[str, any]:\r\n        \"\"\"\r\n        Process a natural language command with visual input\r\n\r\n        Args:\r\n            image: Input image (BGR format from OpenCV)\r\n            command: Natural language command\r\n\r\n        Returns:\r\n            Dictionary with action and task information\r\n        \"\"\"\r\n        # Convert image to PIL format for CLIP\r\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n        pil_image = Image.fromarray(image_rgb)\r\n\r\n        # Process through VLA system\r\n        inputs = self.clip_processor(\r\n            text=[command],\r\n            images=[pil_image],\r\n            return_tensors=\"pt\",\r\n            padding=True\r\n        )\r\n\r\n        with torch.no_grad():\r\n            outputs = self.forward(\r\n                images=inputs['pixel_values'],\r\n                text=[command]\r\n            )\r\n\r\n        # Extract action predictions\r\n        action_raw = outputs['action_predictions'][0].numpy()\r\n\r\n        # Convert to meaningful robot actions\r\n        robot_action = self.convert_to_robot_action(action_raw, command)\r\n\r\n        return {\r\n            'command': command,\r\n            'robot_action': robot_action,\r\n            'action_vector': action_raw,\r\n            'confidence': float(torch.nn.functional.softmax(outputs['task_sequence'][0], dim=0).max())\r\n        }\r\n\r\n    def convert_to_robot_action(self, action_vector: np.ndarray, command: str) -> Dict[str, any]:\r\n        \"\"\"\r\n        Convert action vector to meaningful robot actions\r\n\r\n        Args:\r\n            action_vector: Raw action vector from network\r\n            command: Original command for context\r\n\r\n        Returns:\r\n            Dictionary with robot action parameters\r\n        \"\"\"\r\n        # Normalize action vector to meaningful ranges\r\n        # Assuming action_vector contains [dx, dy, dz, rx, ry, rz, gripper] for manipulator\r\n        position_delta = action_vector[:3] * 0.5  # Scale to max 0.5m movement\r\n        rotation_delta = action_vector[3:6] * 0.5  # Scale to max 0.5 rad rotation\r\n        gripper_action = action_vector[6] if len(action_vector) > 6 else 0.0\r\n\r\n        # Determine action type based on command\r\n        if 'pick' in command.lower() or 'grasp' in command.lower():\r\n            action_type = 'grasp'\r\n        elif 'move' in command.lower() or 'go' in command.lower():\r\n            action_type = 'navigate'\r\n        elif 'place' in command.lower() or 'put' in command.lower():\r\n            action_type = 'place'\r\n        else:\r\n            action_type = 'custom'\r\n\r\n        return {\r\n            'type': action_type,\r\n            'position_delta': position_delta.tolist(),\r\n            'rotation_delta': rotation_delta.tolist(),\r\n            'gripper_action': float(gripper_action),\r\n            'command_context': command\r\n        }\r\n\r\n# Advanced VLA with memory and planning\r\nclass AdvancedVLA(nn.Module):\r\n    def __init__(self, clip_model_name: str = \"openai/clip-vit-base-patch32\"):\r\n        super().__init__()\r\n\r\n        # Base VLA system\r\n        self.vla_base = VisionLanguageActionSystem(clip_model_name)\r\n\r\n        # Memory module for context\r\n        self.memory_encoder = nn.LSTM(512, 256, batch_first=True)\r\n        self.context_attention = nn.MultiheadAttention(256, 8)\r\n\r\n        # Sequential planning\r\n        self.sequential_planner = nn.Sequential(\r\n            nn.Linear(512 + 256, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 64)  # Sequence of 64 steps\r\n        )\r\n\r\n    def forward_with_memory(self, images: torch.Tensor, text: List[str],\r\n                           previous_states: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\r\n        \"\"\"\r\n        Forward pass with memory of previous states\r\n        \"\"\"\r\n        # Get base VLA outputs\r\n        base_outputs = self.vla_base(images, text)\r\n\r\n        # Process memory if available\r\n        if previous_states is not None:\r\n            memory_out, _ = self.memory_encoder(previous_states)\r\n            # Apply attention to focus on relevant memory\r\n            attended_memory, _ = self.context_attention(\r\n                base_outputs['image_features'].unsqueeze(0),\r\n                memory_out, memory_out\r\n            )\r\n            attended_memory = attended_memory.squeeze(0)\r\n        else:\r\n            attended_memory = torch.zeros_like(base_outputs['image_features'])\r\n\r\n        # Plan sequential actions\r\n        combined_features = torch.cat([\r\n            base_outputs['image_features'],\r\n            attended_memory\r\n        ], dim=1)\r\n\r\n        sequential_plan = self.sequential_planner(combined_features)\r\n\r\n        base_outputs['sequential_plan'] = sequential_plan\r\n        base_outputs['memory_attention'] = attended_memory\r\n\r\n        return base_outputs\r\n\r\n# Example usage\r\ndef vla_example():\r\n    # Initialize VLA system\r\n    vla_system = VisionLanguageActionSystem()\r\n\r\n    # Example: Process an image with a command\r\n    # In practice, you would load a real image\r\n    sample_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n    command = \"Pick up the red cup on the table\"\r\n\r\n    result = vla_system.process_command(sample_image, command)\r\n\r\n    print(f\"Command: {result['command']}\")\r\n    print(f\"Action type: {result['robot_action']['type']}\")\r\n    print(f\"Position delta: {result['robot_action']['position_delta']}\")\r\n    print(f\"Confidence: {result['confidence']:.3f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    # Import PIL for image processing\r\n    from PIL import Image\r\n\r\n    vla_example()\n"})}),"\n",(0,a.jsx)(r.h2,{id:"simulation-integration",children:"Simulation Integration"}),"\n",(0,a.jsx)(r.h3,{id:"gazebo-ros2-interface",children:"Gazebo ROS2 Interface"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Twist, Pose\r\nfrom nav_msgs.msg import Odometry\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nfrom std_msgs.msg import String\r\nfrom tf2_ros import TransformBroadcaster\r\nimport tf2_geometry_msgs\r\nimport numpy as np\r\nimport math\r\n\r\nclass GazeboRobotInterface(Node):\r\n    def __init__(self):\r\n        super().__init__('gazebo_robot_interface')\r\n\r\n        # Robot state\r\n        self.position = np.array([0.0, 0.0, 0.0])  # x, y, theta\r\n        self.velocity = np.array([0.0, 0.0])        # linear, angular\r\n\r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.odom_pub = self.create_publisher(Odometry, '/odom', 10)\r\n\r\n        # Subscribers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, '/scan', self.scan_callback, 10)\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/image_raw', self.image_callback, 10)\r\n\r\n        # TF broadcaster\r\n        self.tf_broadcaster = TransformBroadcaster(self)\r\n\r\n        # Timer for publishing odometry\r\n        self.timer = self.create_timer(0.1, self.publish_odometry)  # 10 Hz\r\n\r\n        self.get_logger().info('Gazebo Robot Interface initialized')\r\n\r\n    def scan_callback(self, msg):\r\n        \"\"\"Process laser scan data\"\"\"\r\n        # Example: Find minimum distance to obstacles\r\n        min_distance = min(msg.ranges)\r\n\r\n        if min_distance < 0.5:  # Less than 0.5m to obstacle\r\n            self.get_logger().warn(f'Obstacle detected at {min_distance:.2f}m')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process camera image data\"\"\"\r\n        # Image processing would happen here\r\n        # For now, just log that we received an image\r\n        self.get_logger().info(f'Received image: {msg.width}x{msg.height}')\r\n\r\n    def publish_odometry(self):\r\n        \"\"\"Publish odometry data\"\"\"\r\n        msg = Odometry()\r\n        msg.header.stamp = self.get_clock().now().to_msg()\r\n        msg.header.frame_id = 'odom'\r\n        msg.child_frame_id = 'base_link'\r\n\r\n        # Set position\r\n        msg.pose.pose.position.x = float(self.position[0])\r\n        msg.pose.pose.position.y = float(self.position[1])\r\n        msg.pose.pose.position.z = 0.0\r\n\r\n        # Convert theta to quaternion\r\n        from tf_transformations import quaternion_from_euler\r\n        quat = quaternion_from_euler(0, 0, self.position[2])\r\n        msg.pose.pose.orientation.x = quat[0]\r\n        msg.pose.pose.orientation.y = quat[1]\r\n        msg.pose.pose.orientation.z = quat[2]\r\n        msg.pose.pose.orientation.w = quat[3]\r\n\r\n        # Set velocities\r\n        msg.twist.twist.linear.x = float(self.velocity[0])\r\n        msg.twist.twist.angular.z = float(self.velocity[1])\r\n\r\n        self.odom_pub.publish(msg)\r\n\r\n        # Broadcast transform\r\n        from geometry_msgs.msg import TransformStamped\r\n        t = TransformStamped()\r\n        t.header.stamp = self.get_clock().now().to_msg()\r\n        t.header.frame_id = 'odom'\r\n        t.child_frame_id = 'base_link'\r\n\r\n        t.transform.translation.x = float(self.position[0])\r\n        t.transform.translation.y = float(self.position[1])\r\n        t.transform.translation.z = 0.0\r\n\r\n        t.transform.rotation.x = quat[0]\r\n        t.transform.rotation.y = quat[1]\r\n        t.transform.rotation.z = quat[2]\r\n        t.transform.rotation.w = quat[3]\r\n\r\n        self.tf_broadcaster.sendTransform(t)\r\n\r\n    def move_robot(self, linear_vel, angular_vel):\r\n        \"\"\"Send velocity commands to robot\"\"\"\r\n        msg = Twist()\r\n        msg.linear.x = linear_vel\r\n        msg.angular.z = angular_vel\r\n\r\n        self.cmd_vel_pub.publish(msg)\r\n\r\n        # Update internal state (simplified)\r\n        dt = 0.1  # Time step from timer\r\n        self.position[0] += linear_vel * math.cos(self.position[2]) * dt\r\n        self.position[1] += linear_vel * math.sin(self.position[2]) * dt\r\n        self.position[2] += angular_vel * dt\r\n\r\n        # Normalize angle\r\n        self.position[2] = math.atan2(\r\n            math.sin(self.position[2]),\r\n            math.cos(self.position[2])\r\n        )\r\n\r\n        self.velocity[0] = linear_vel\r\n        self.velocity[1] = angular_vel\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    interface = GazeboRobotInterface()\r\n\r\n    # Example: Move robot in a square pattern\r\n    def move_square():\r\n        interface.get_logger().info('Moving robot in square pattern')\r\n\r\n        # Define square movement: move forward, turn 90 degrees, repeat\r\n        for i in range(4):\r\n            interface.get_logger().info(f'Moving side {i+1} of square')\r\n\r\n            # Move forward for 2 seconds\r\n            for _ in range(20):  # 20 iterations * 0.1s = 2 seconds\r\n                interface.move_robot(0.5, 0.0)  # 0.5 m/s linear\r\n                rclpy.spin_once(interface, timeout_sec=0.1)\r\n\r\n            # Turn 90 degrees\r\n            interface.get_logger().info('Turning 90 degrees')\r\n            for _ in range(15):  # Turn for 1.5 seconds (adjust as needed)\r\n                interface.move_robot(0.0, math.pi/2/1.5)  # Angular velocity for 90 deg in 1.5s\r\n                rclpy.spin_once(interface, timeout_sec=0.1)\r\n\r\n    # Run the square movement\r\n    move_square()\r\n\r\n    interface.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(r.h2,{id:"hardware-integration",children:"Hardware Integration"}),"\n",(0,a.jsx)(r.h3,{id:"jetson-ai-inference",children:"Jetson AI Inference"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'import jetson.inference\r\nimport jetson.utils\r\nimport numpy as np\r\nimport cv2\r\nfrom PIL import Image\r\nimport torch\r\nimport torchvision.transforms as transforms\r\n\r\nclass JetsonAIProcessor:\r\n    def __init__(self, model_path: str = "ssd-mobilenet-v2", input_size: Tuple[int, int] = (300, 300)):\r\n        """\r\n        Initialize AI processor for Jetson platform\r\n\r\n        Args:\r\n            model_path: Path to the detection model\r\n            input_size: Input size for the model\r\n        """\r\n        self.input_size = input_size\r\n\r\n        # Initialize Jetson inference\r\n        try:\r\n            self.net = jetson.inference.detectNet(model_path)\r\n        except Exception as e:\r\n            print(f"Error loading detection model: {e}")\r\n            # Fallback to a default model\r\n            self.net = jetson.inference.detectNet("ssd-mobilenet-v2")\r\n\r\n        # Initialize camera for Jetson\r\n        self.camera = jetson.utils.gstCamera(*input_size)\r\n        self.display = jetson.utils.glDisplay()\r\n\r\n        # PyTorch transforms for additional processing\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                               std=[0.229, 0.224, 0.225])\r\n        ])\r\n\r\n    def detect_objects(self, image: np.ndarray) -> List[Dict]:\r\n        """\r\n        Detect objects in the image using Jetson inference\r\n\r\n        Args:\r\n            image: Input image (BGR format)\r\n\r\n        Returns:\r\n            List of detected objects with bounding boxes and confidence\r\n        """\r\n        # Convert numpy array to CUDA image\r\n        img_cuda = jetson.utils.cudaFromNumpy(image)\r\n\r\n        # Perform object detection\r\n        detections = self.net.Detect(img_cuda)\r\n\r\n        results = []\r\n        for detection in detections:\r\n            results.append({\r\n                \'class_id\': int(detection.ClassID),\r\n                \'confidence\': float(detection.Confidence),\r\n                \'left\': int(detection.Left),\r\n                \'top\': int(detection.Top),\r\n                \'right\': int(detection.Right),\r\n                \'bottom\': int(detection.Bottom),\r\n                \'width\': int(detection.Width),\r\n                \'height\': int(detection.Height),\r\n                \'area\': int(detection.Area)\r\n            })\r\n\r\n        return results\r\n\r\n    def preprocess_for_pytorch(self, image: np.ndarray) -> torch.Tensor:\r\n        """Preprocess image for additional PyTorch processing"""\r\n        # Convert BGR to RGB\r\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n\r\n        # Convert to PIL and apply transforms\r\n        pil_image = Image.fromarray(image_rgb)\r\n        tensor = self.transform(pil_image)\r\n\r\n        # Add batch dimension\r\n        return tensor.unsqueeze(0)\r\n\r\n    def run_inference_loop(self):\r\n        """Run continuous inference loop"""\r\n        self.camera.Open()\r\n\r\n        try:\r\n            while self.display.IsOpen():\r\n                # Capture image from camera\r\n                img, width, height = self.camera.CaptureRGBA()\r\n\r\n                # Convert CUDA image to numpy array\r\n                img_np = jetson.utils.cudaToNumpy(img, width, height, 4)  # RGBA\r\n\r\n                # Convert RGBA to BGR for OpenCV\r\n                img_bgr = cv2.cvtColor(img_np.astype(np.uint8), cv2.COLOR_RGBA2BGR)\r\n\r\n                # Perform object detection\r\n                detections = self.detect_objects(img_bgr)\r\n\r\n                # Draw bounding boxes on image\r\n                for detection in detections:\r\n                    cv2.rectangle(\r\n                        img_bgr,\r\n                        (detection[\'left\'], detection[\'top\']),\r\n                        (detection[\'right\'], detection[\'bottom\']),\r\n                        (0, 255, 0),\r\n                        2\r\n                    )\r\n\r\n                    # Add label\r\n                    label = f"Class {detection[\'class_id\']}: {detection[\'confidence\']:.2f}"\r\n                    cv2.putText(\r\n                        img_bgr,\r\n                        label,\r\n                        (detection[\'left\'], detection[\'top\'] - 10),\r\n                        cv2.FONT_HERSHEY_SIMPLEX,\r\n                        0.5,\r\n                        (0, 255, 0),\r\n                        1\r\n                    )\r\n\r\n                # Display the image\r\n                img_cuda = jetson.utils.cudaFromNumpy(img_bgr)\r\n                self.display.Render(img_cuda)\r\n\r\n                # Update display\r\n                self.display.SetTitle(f"Object Detection | {self.net.GetNetworkName()}")\r\n\r\n        except KeyboardInterrupt:\r\n            print("Inference loop interrupted")\r\n        finally:\r\n            self.camera.Close()\r\n\r\n# Example usage for a custom AI model\r\nclass CustomJetsonModel:\r\n    def __init__(self, model_path: str):\r\n        """\r\n        Load and run a custom PyTorch model on Jetson\r\n\r\n        Args:\r\n            model_path: Path to the PyTorch model file\r\n        """\r\n        # Load model\r\n        self.model = torch.load(model_path)\r\n        self.model.eval()\r\n\r\n        # Move to GPU if available\r\n        if torch.cuda.is_available():\r\n            self.model = self.model.cuda()\r\n\r\n        # Initialize TensorRT optimization (if available)\r\n        try:\r\n            import torch_tensorrt\r\n            self.model = torch_tensorrt.compile(\r\n                self.model,\r\n                inputs=[torch_tensorrt.Input((1, 3, 224, 224))],\r\n                enabled_precisions={torch.float}\r\n            )\r\n            print("Model compiled with TensorRT")\r\n        except ImportError:\r\n            print("TensorRT not available, using standard PyTorch")\r\n\r\n    def preprocess_input(self, image: np.ndarray) -> torch.Tensor:\r\n        """Preprocess input image for the model"""\r\n        # Resize image\r\n        image_resized = cv2.resize(image, (224, 224))\r\n\r\n        # Convert BGR to RGB\r\n        image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\r\n\r\n        # Convert to tensor and normalize\r\n        tensor = torch.from_numpy(image_rgb.astype(np.float32)).permute(2, 0, 1) / 255.0\r\n\r\n        # Normalize with ImageNet stats\r\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\r\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\r\n        tensor = (tensor - mean) / std\r\n\r\n        # Add batch dimension\r\n        return tensor.unsqueeze(0)\r\n\r\n    def predict(self, image: np.ndarray) -> torch.Tensor:\r\n        """Run prediction on the image"""\r\n        input_tensor = self.preprocess_input(image)\r\n\r\n        # Move to GPU if available\r\n        if torch.cuda.is_available():\r\n            input_tensor = input_tensor.cuda()\r\n\r\n        # Run inference\r\n        with torch.no_grad():\r\n            output = self.model(input_tensor)\r\n\r\n        return output\r\n\r\ndef jetson_example():\r\n    """Example usage of Jetson AI processing"""\r\n    try:\r\n        # Initialize Jetson processor\r\n        processor = JetsonAIProcessor()\r\n\r\n        # Example: Load a custom model\r\n        # custom_model = CustomJetsonModel("path/to/your/model.pth")\r\n\r\n        print("Starting inference loop...")\r\n        processor.run_inference_loop()\r\n\r\n    except Exception as e:\r\n        print(f"Error in Jetson example: {e}")\r\n\r\nif __name__ == "__main__":\r\n    jetson_example()\n'})}),"\n",(0,a.jsx)(r.h2,{id:"safety-and-monitoring",children:"Safety and Monitoring"}),"\n",(0,a.jsx)(r.h3,{id:"robot-safety-monitor",children:"Robot Safety Monitor"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Imu\r\nfrom geometry_msgs.msg import Twist\r\nfrom std_msgs.msg import Bool\r\nfrom builtin_interfaces.msg import Time\r\nimport numpy as np\r\nimport threading\r\nimport time\r\n\r\nclass RobotSafetyMonitor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'robot_safety_monitor\')\r\n\r\n        # Safety parameters\r\n        self.safety_distance = 0.5  # meters\r\n        self.max_linear_vel = 0.5   # m/s\r\n        self.max_angular_vel = 1.0  # rad/s\r\n        self.emergency_stop = False\r\n        self.safety_lock = threading.Lock()\r\n\r\n        # Sensor data storage\r\n        self.laser_data = None\r\n        self.imu_data = None\r\n        self.last_cmd_time = self.get_clock().now()\r\n\r\n        # Publishers and subscribers\r\n        self.cmd_sub = self.create_subscription(\r\n            Twist, \'/cmd_vel_raw\', self.cmd_vel_callback, 10)\r\n        self.safety_pub = self.create_publisher(Bool, \'/safety_status\', 10)\r\n        self.vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n\r\n        # Sensor subscriptions\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.scan_callback, 10)\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu/data\', self.imu_callback, 10)\r\n\r\n        # Timer for safety checks\r\n        self.safety_timer = self.create_timer(0.1, self.safety_check)\r\n\r\n        self.get_logger().info(\'Robot Safety Monitor initialized\')\r\n\r\n    def cmd_vel_callback(self, msg):\r\n        """Receive raw velocity commands"""\r\n        with self.safety_lock:\r\n            self.raw_cmd = msg\r\n            self.last_cmd_time = self.get_clock().now()\r\n\r\n    def scan_callback(self, msg):\r\n        """Process laser scan data"""\r\n        self.laser_data = msg\r\n\r\n    def imu_callback(self, msg):\r\n        """Process IMU data"""\r\n        self.imu_data = msg\r\n\r\n    def safety_check(self):\r\n        """Perform safety checks and publish safe commands"""\r\n        current_time = self.get_clock().now()\r\n\r\n        # Check if emergency stop is active\r\n        if self.emergency_stop:\r\n            self.publish_safe_command(0.0, 0.0)\r\n            return\r\n\r\n        # Check for sensor timeouts\r\n        if current_time.nanoseconds - self.last_cmd_time.nanoseconds > 1e9:  # 1 second timeout\r\n            self.get_logger().warn(\'Command timeout - stopping robot\')\r\n            self.publish_safe_command(0.0, 0.0)\r\n            return\r\n\r\n        # Check laser data for obstacles\r\n        if self.laser_data is not None:\r\n            min_distance = min(self.laser_data.ranges)\r\n\r\n            if min_distance < self.safety_distance:\r\n                self.get_logger().warn(f\'Obstacle detected at {min_distance:.2f}m - emergency stop\')\r\n                self.emergency_stop = True\r\n                self.publish_safe_command(0.0, 0.0)\r\n                return\r\n\r\n        # Check IMU for abnormal accelerations\r\n        if self.imu_data is not None:\r\n            linear_acc = np.sqrt(\r\n                self.imu_data.linear_acceleration.x**2 +\r\n                self.imu_data.linear_acceleration.y**2 +\r\n                self.imu_data.linear_acceleration.z**2\r\n            )\r\n\r\n            # Check for excessive acceleration (indicating collision or instability)\r\n            if linear_acc > 10.0:  # 10 m/s^2 threshold\r\n                self.get_logger().warn(f\'Excessive acceleration detected: {linear_acc:.2f} m/s^2\')\r\n                self.emergency_stop = True\r\n                self.publish_safe_command(0.0, 0.0)\r\n                return\r\n\r\n        # If all checks pass, publish processed command\r\n        if hasattr(self, \'raw_cmd\'):\r\n            safe_cmd = self.process_command(self.raw_cmd)\r\n            self.vel_pub.publish(safe_cmd)\r\n\r\n    def process_command(self, cmd):\r\n        """Process and limit raw command"""\r\n        safe_cmd = Twist()\r\n\r\n        # Limit linear velocity\r\n        safe_cmd.linear.x = max(-self.max_linear_vel,\r\n                               min(self.max_linear_vel, cmd.linear.x))\r\n        safe_cmd.linear.y = max(-self.max_linear_vel,\r\n                               min(self.max_linear_vel, cmd.linear.y))\r\n        safe_cmd.linear.z = max(-self.max_linear_vel,\r\n                               min(self.max_linear_vel, cmd.linear.z))\r\n\r\n        # Limit angular velocity\r\n        safe_cmd.angular.x = max(-self.max_angular_vel,\r\n                                min(self.max_angular_vel, cmd.angular.x))\r\n        safe_cmd.angular.y = max(-self.max_angular_vel,\r\n                                min(self.max_angular_vel, cmd.angular.y))\r\n        safe_cmd.angular.z = max(-self.max_angular_vel,\r\n                                min(self.max_angular_vel, cmd.angular.z))\r\n\r\n        return safe_cmd\r\n\r\n    def publish_safe_command(self, linear_vel, angular_vel):\r\n        """Publish safe stop command"""\r\n        cmd = Twist()\r\n        cmd.linear.x = linear_vel\r\n        cmd.angular.z = angular_vel\r\n        self.vel_pub.publish(cmd)\r\n\r\n        # Publish safety status\r\n        status_msg = Bool()\r\n        status_msg.data = not self.emergency_stop\r\n        self.safety_pub.publish(status_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    safety_monitor = RobotSafetyMonitor()\r\n\r\n    try:\r\n        rclpy.spin(safety_monitor)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        safety_monitor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(r.h2,{id:"utilities-and-helper-functions",children:"Utilities and Helper Functions"}),"\n",(0,a.jsx)(r.h3,{id:"common-robotics-utilities",children:"Common Robotics Utilities"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nimport math\r\nfrom typing import Tuple, List, Optional\r\nimport transforms3d as tf3d\r\n\r\nclass RobotUtils:\r\n    """Common robotics utility functions"""\r\n\r\n    @staticmethod\r\n    def quaternion_to_euler(q: np.ndarray) -> Tuple[float, float, float]:\r\n        """Convert quaternion [x, y, z, w] to Euler angles [roll, pitch, yaw]"""\r\n        return tf3d.euler.quat2euler(q, axes=\'sxyz\')\r\n\r\n    @staticmethod\r\n    def euler_to_quaternion(roll: float, pitch: float, yaw: float) -> np.ndarray:\r\n        """Convert Euler angles [roll, pitch, yaw] to quaternion [x, y, z, w]"""\r\n        return tf3d.euler.euler2quat(roll, pitch, yaw, axes=\'sxyz\')\r\n\r\n    @staticmethod\r\n    def transform_point(point: np.ndarray, translation: np.ndarray,\r\n                       rotation_quat: np.ndarray) -> np.ndarray:\r\n        """Transform a point by translation and rotation"""\r\n        # Convert quaternion to rotation matrix\r\n        rotation_matrix = tf3d.quaternions.quat2mat(rotation_quat)\r\n\r\n        # Apply rotation and translation\r\n        transformed = rotation_matrix @ point + translation\r\n\r\n        return transformed\r\n\r\n    @staticmethod\r\n    def distance_3d(p1: np.ndarray, p2: np.ndarray) -> float:\r\n        """Calculate 3D distance between two points"""\r\n        return np.linalg.norm(p2 - p1)\r\n\r\n    @staticmethod\r\n    def angle_between_vectors(v1: np.ndarray, v2: np.ndarray) -> float:\r\n        """Calculate angle between two vectors in radians"""\r\n        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\r\n        return np.arccos(np.clip(cos_angle, -1.0, 1.0))\r\n\r\n    @staticmethod\r\n    def rotation_matrix_to_axis_angle(R: np.ndarray) -> Tuple[np.ndarray, float]:\r\n        """Convert rotation matrix to axis-angle representation"""\r\n        angle = np.arccos(np.clip((np.trace(R) - 1) / 2, -1, 1))\r\n\r\n        if angle < 1e-6:  # Identity rotation\r\n            return np.array([0, 0, 1]), 0.0\r\n\r\n        axis = np.array([\r\n            R[2, 1] - R[1, 2],\r\n            R[0, 2] - R[2, 0],\r\n            R[1, 0] - R[0, 1]\r\n        ]) / (2 * np.sin(angle))\r\n\r\n        return axis / np.linalg.norm(axis), angle\r\n\r\nclass TrajectoryGenerator:\r\n    """Generate smooth trajectories for robot motion"""\r\n\r\n    @staticmethod\r\n    def cubic_polynomial_trajectory(start_pos: float, end_pos: float,\r\n                                  duration: float, dt: float = 0.01) -> List[float]:\r\n        """Generate cubic polynomial trajectory"""\r\n        # Cubic polynomial: s(t) = a0 + a1*t + a2*t^2 + a3*t^3\r\n        # Boundary conditions: s(0)=start, s(T)=end, s\'(0)=0, s\'(T)=0\r\n\r\n        T = duration\r\n        a0 = start_pos\r\n        a1 = 0  # Initial velocity = 0\r\n        a2 = 3 * (end_pos - start_pos) / T**2\r\n        a3 = -2 * (end_pos - start_pos) / T**3\r\n\r\n        times = np.arange(0, T, dt)\r\n        positions = []\r\n\r\n        for t in times:\r\n            pos = a0 + a1*t + a2*t**2 + a3*t**3\r\n            positions.append(pos)\r\n\r\n        return positions\r\n\r\n    @staticmethod\r\n    def quintic_polynomial_trajectory(start_pos: float, end_pos: float,\r\n                                    start_vel: float = 0.0, end_vel: float = 0.0,\r\n                                    start_acc: float = 0.0, end_acc: float = 0.0,\r\n                                    duration: float, dt: float = 0.01) -> Tuple[List[float], List[float], List[float]]:\r\n        """Generate quintic polynomial trajectory with velocity and acceleration constraints"""\r\n        T = duration\r\n\r\n        # Coefficients for quintic polynomial\r\n        a0 = start_pos\r\n        a1 = start_vel\r\n        a2 = start_acc / 2\r\n\r\n        a3 = (20*(end_pos - start_pos) - (8*end_vel + 12*start_vel)*T - (3*start_acc - end_acc)*T**2) / (2*T**3)\r\n        a4 = (30*(start_pos - end_pos) + (14*end_vel + 16*start_vel)*T + (3*start_acc - 2*end_acc)*T**2) / (2*T**4)\r\n        a5 = (12*(end_pos - start_pos) - 6*(end_vel + start_vel)*T - (end_acc - start_acc)*T**2) / (2*T**5)\r\n\r\n        times = np.arange(0, T, dt)\r\n        positions = []\r\n        velocities = []\r\n        accelerations = []\r\n\r\n        for t in times:\r\n            # Position\r\n            pos = a0 + a1*t + a2*t**2 + a3*t**3 + a4*t**4 + a5*t**5\r\n            positions.append(pos)\r\n\r\n            # Velocity\r\n            vel = a1 + 2*a2*t + 3*a3*t**2 + 4*a4*t**3 + 5*a5*t**4\r\n            velocities.append(vel)\r\n\r\n            # Acceleration\r\n            acc = 2*a2 + 6*a3*t + 12*a4*t**2 + 20*a5*t**3\r\n            accelerations.append(acc)\r\n\r\n        return positions, velocities, accelerations\r\n\r\n# Example usage\r\ndef utilities_example():\r\n    """Example usage of utility functions"""\r\n\r\n    # Quaternion-Euler conversions\r\n    quat = RobotUtils.euler_to_quaternion(0.1, 0.2, 0.3)\r\n    euler = RobotUtils.quaternion_to_euler(quat)\r\n    print(f"Quaternion: {quat}")\r\n    print(f"Euler angles: {euler}")\r\n\r\n    # Point transformation\r\n    point = np.array([1, 0, 0])\r\n    translation = np.array([0, 0, 1])\r\n    rotation = RobotUtils.euler_to_quaternion(0, 0, math.pi/2)  # 90-degree rotation around Z\r\n    transformed_point = RobotUtils.transform_point(point, translation, rotation)\r\n    print(f"Transformed point: {transformed_point}")\r\n\r\n    # Trajectory generation\r\n    positions = TrajectoryGenerator.cubic_polynomial_trajectory(0, 1, 2.0)\r\n    print(f"Cubic trajectory has {len(positions)} points")\r\n\r\n    pos, vel, acc = TrajectoryGenerator.quintic_polynomial_trajectory(0, 1, duration=2.0)\r\n    print(f"Quintic trajectory has {len(pos)} points")\r\n\r\nif __name__ == "__main__":\r\n    utilities_example()\n'})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsxs)(r.p,{children:["Continue with ",(0,a.jsx)(r.a,{href:"/Book-ai-native/docs/appendices/simulation-assets",children:"Simulation Assets Guide"})," to explore the creation and utilization of 3D models, environments, and other assets for robotics simulation."]})]})}function d(n={}){const{wrapper:r}={...(0,o.R)(),...n.components};return r?(0,a.jsx)(r,{...n,children:(0,a.jsx)(p,{...n})}):p(n)}},8453(n,r,e){e.d(r,{R:()=>i,x:()=>s});var t=e(6540);const a={},o=t.createContext(a);function i(n){const r=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(r):{...r,...n}},[r,n])}function s(n){let r;return r=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),t.createElement(o.Provider,{value:r},n.children)}}}]);