"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[1162],{7098(e,n,i){i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>c,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module-4-vla/multimodal-embeddings","title":"Multimodal Embeddings and Representation","description":"Learning Objectives","source":"@site/docs/module-4-vla/multimodal-embeddings.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/multimodal-embeddings","permalink":"/Book-ai-native/docs/module-4-vla/multimodal-embeddings","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/module-4-vla/multimodal-embeddings.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Book-ai-native/docs/module-4-vla/intro"},"next":{"title":"Instruction Following and Task Planning","permalink":"/Book-ai-native/docs/module-4-vla/instruction-following"}}');var t=i(4848),s=i(8453);const a={sidebar_position:2},o="Multimodal Embeddings and Representation",d={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Multimodal Embeddings",id:"introduction-to-multimodal-embeddings",level:2},{value:"Theoretical Foundations",id:"theoretical-foundations",level:2},{value:"Embedding Spaces and Modalities",id:"embedding-spaces-and-modalities",level:3},{value:"Cross-Modal Alignment",id:"cross-modal-alignment",level:3},{value:"Architectural Approaches",id:"architectural-approaches",level:2},{value:"Early Fusion vs. Late Fusion",id:"early-fusion-vs-late-fusion",level:3},{value:"Early Fusion",id:"early-fusion",level:4},{value:"Late Fusion",id:"late-fusion",level:4},{value:"Cross-Modal Attention Mechanisms",id:"cross-modal-attention-mechanisms",level:3},{value:"Vision-Language Embeddings",id:"vision-language-embeddings",level:2},{value:"CLIP-Inspired Architecture",id:"clip-inspired-architecture",level:3},{value:"Vision-Language-Action Extension",id:"vision-language-action-extension",level:3},{value:"Action Embedding Representations",id:"action-embedding-representations",level:2},{value:"Continuous Action Spaces",id:"continuous-action-spaces",level:3},{value:"Discrete Action Spaces",id:"discrete-action-spaces",level:3},{value:"Training Strategies",id:"training-strategies",level:2},{value:"Contrastive Learning",id:"contrastive-learning",level:3},{value:"Triplet Loss",id:"triplet-loss",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Cross-Modal Retrieval",id:"cross-modal-retrieval",level:3},{value:"Real-Time Optimization",id:"real-time-optimization",level:2},{value:"Efficient Embedding Computation",id:"efficient-embedding-computation",level:3},{value:"Caching and Precomputation",id:"caching-and-precomputation",level:3},{value:"Isaac Integration",id:"isaac-integration",level:2},{value:"Isaac ROS for Multimodal Processing",id:"isaac-ros-for-multimodal-processing",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"multimodal-embeddings-and-representation",children:"Multimodal Embeddings and Representation"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the principles of multimodal embeddings for vision-language-action systems"}),"\n",(0,t.jsx)(n.li,{children:"Implement unified embedding spaces that connect visual, linguistic, and action modalities"}),"\n",(0,t.jsx)(n.li,{children:"Design cross-modal attention mechanisms for information fusion"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate multimodal embedding quality and alignment"}),"\n",(0,t.jsx)(n.li,{children:"Optimize embedding architectures for real-time robotic applications"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-multimodal-embeddings",children:"Introduction to Multimodal Embeddings"}),"\n",(0,t.jsx)(n.p,{children:"Multimodal embeddings form the foundation of Vision-Language-Action (VLA) systems by creating unified representations that capture relationships across different modalities. Unlike unimodal embeddings that operate within a single domain (e.g., text embeddings for language or visual embeddings for images), multimodal embeddings enable cross-domain reasoning by mapping different types of information to a shared semantic space."}),"\n",(0,t.jsx)(n.p,{children:"The challenge in multimodal embeddings lies in creating representations that preserve the unique characteristics of each modality while enabling meaningful comparisons and interactions between them. For VLA systems, this means connecting:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Embeddings"}),": Representations of images, video, and sensor data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Linguistic Embeddings"}),": Representations of text, commands, and descriptions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Embeddings"}),": Representations of robot movements, trajectories, and behaviors"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"theoretical-foundations",children:"Theoretical Foundations"}),"\n",(0,t.jsx)(n.h3,{id:"embedding-spaces-and-modalities",children:"Embedding Spaces and Modalities"}),"\n",(0,t.jsx)(n.p,{children:"An embedding space is a continuous vector space where discrete inputs (words, images, actions) are mapped to dense vector representations. In multimodal systems, we have multiple embedding spaces that need to be aligned:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Visual Space:    R^D_v  \u2190 Images, video, sensor data\r\nLinguistic Space: R^D_l  \u2190 Text, commands, descriptions\r\nAction Space:     R^D_a  \u2190 Motor commands, trajectories\r\nJoint Space:      R^D_j  \u2190 Unified multimodal representations\n"})}),"\n",(0,t.jsx)(n.p,{children:"The goal is to learn mappings between these spaces such that semantically related concepts across modalities are close in the joint embedding space."}),"\n",(0,t.jsx)(n.h3,{id:"cross-modal-alignment",children:"Cross-Modal Alignment"}),"\n",(0,t.jsx)(n.p,{children:"Cross-modal alignment ensures that related concepts from different modalities are mapped to nearby locations in the embedding space. For example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'The image of a "red ball" should be close to the text "red ball"'}),"\n",(0,t.jsx)(n.li,{children:'The action "pick up the ball" should be close to both the image and text representations'}),"\n",(0,t.jsx)(n.li,{children:"The trajectory of reaching toward a ball should be semantically related to both visual and linguistic concepts"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architectural-approaches",children:"Architectural Approaches"}),"\n",(0,t.jsx)(n.h3,{id:"early-fusion-vs-late-fusion",children:"Early Fusion vs. Late Fusion"}),"\n",(0,t.jsx)(n.p,{children:"Two primary approaches exist for combining multimodal information:"}),"\n",(0,t.jsx)(n.h4,{id:"early-fusion",children:"Early Fusion"}),"\n",(0,t.jsx)(n.p,{children:"In early fusion, different modalities are combined at an early stage of processing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass EarlyFusionMultimodalEncoder(nn.Module):\r\n    def __init__(self, visual_dim, text_dim, action_dim, joint_dim):\r\n        super().__init__()\r\n\r\n        # Individual modality encoders\r\n        self.visual_encoder = nn.Linear(visual_dim, joint_dim)\r\n        self.text_encoder = nn.Linear(text_dim, joint_dim)\r\n        self.action_encoder = nn.Linear(action_dim, joint_dim)\r\n\r\n        # Fusion layer that combines all modalities\r\n        self.fusion_layer = nn.Linear(joint_dim * 3, joint_dim)\r\n        self.norm = nn.LayerNorm(joint_dim)\r\n\r\n    def forward(self, visual_input, text_input, action_input):\r\n        # Encode each modality to joint space\r\n        visual_emb = self.visual_encoder(visual_input)\r\n        text_emb = self.text_encoder(text_input)\r\n        action_emb = self.action_encoder(action_input)\r\n\r\n        # Concatenate and fuse\r\n        combined = torch.cat([visual_emb, text_emb, action_emb], dim=-1)\r\n        fused = self.fusion_layer(combined)\r\n\r\n        return self.norm(fused)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"late-fusion",children:"Late Fusion"}),"\n",(0,t.jsx)(n.p,{children:"In late fusion, modalities are processed separately and combined at a later stage:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class LateFusionMultimodalEncoder(nn.Module):\r\n    def __init__(self, visual_dim, text_dim, action_dim, joint_dim):\r\n        super().__init__()\r\n\r\n        # Individual encoders\r\n        self.visual_encoder = nn.Linear(visual_dim, joint_dim)\r\n        self.text_encoder = nn.Linear(text_dim, joint_dim)\r\n        self.action_encoder = nn.Linear(action_dim, joint_dim)\r\n\r\n        # Cross-attention for fusion\r\n        self.cross_attention = nn.MultiheadAttention(joint_dim, num_heads=8)\r\n        self.fusion_transformer = nn.TransformerEncoderLayer(\r\n            d_model=joint_dim, nhead=8, dim_feedforward=joint_dim*2\r\n        )\r\n\r\n    def forward(self, visual_input, text_input, action_input):\r\n        # Encode each modality separately\r\n        visual_emb = self.visual_encoder(visual_input)  # [B, seq_len_v, D]\r\n        text_emb = self.text_encoder(text_input)       # [B, seq_len_t, D]\r\n        action_emb = self.action_encoder(action_input) # [B, seq_len_a, D]\r\n\r\n        # Concatenate sequences\r\n        combined_seq = torch.cat([visual_emb, text_emb, action_emb], dim=1)\r\n\r\n        # Apply cross-modal attention\r\n        fused_output = self.fusion_transformer(combined_seq)\r\n\r\n        return fused_output\n"})}),"\n",(0,t.jsx)(n.h3,{id:"cross-modal-attention-mechanisms",children:"Cross-Modal Attention Mechanisms"}),"\n",(0,t.jsx)(n.p,{children:"Cross-modal attention allows information from one modality to influence the representation of another:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class CrossModalAttention(nn.Module):\r\n    def __init__(self, dim, num_heads=8):\r\n        super().__init__()\r\n        self.dim = dim\r\n        self.num_heads = num_heads\r\n        self.head_dim = dim // num_heads\r\n\r\n        # Query, key, value projections for cross-attention\r\n        self.q_proj = nn.Linear(dim, dim)\r\n        self.k_proj = nn.Linear(dim, dim)\r\n        self.v_proj = nn.Linear(dim, dim)\r\n        self.out_proj = nn.Linear(dim, dim)\r\n\r\n    def forward(self, query_modality, key_modality, value_modality):\r\n        B, N, C = query_modality.shape\r\n\r\n        # Project to query, key, value\r\n        q = self.q_proj(query_modality).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\r\n        k = self.k_proj(key_modality).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\r\n        v = self.v_proj(value_modality).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\r\n\r\n        # Compute attention\r\n        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\r\n        attn_weights = F.softmax(attn_weights, dim=-1)\r\n\r\n        # Apply attention to values\r\n        output = torch.matmul(attn_weights, v)\r\n        output = output.transpose(1, 2).contiguous().view(B, N, C)\r\n\r\n        return self.out_proj(output)\r\n\r\nclass MultimodalFusionBlock(nn.Module):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.visual_to_text = CrossModalAttention(dim)\r\n        self.text_to_visual = CrossModalAttention(dim)\r\n        self.action_to_joint = CrossModalAttention(dim)\r\n\r\n        self.norm1 = nn.LayerNorm(dim)\r\n        self.norm2 = nn.LayerNorm(dim)\r\n        self.norm3 = nn.LayerNorm(dim)\r\n\r\n    def forward(self, visual_features, text_features, action_features):\r\n        # Visual features influence text representation\r\n        text_updated = self.norm1(text_features + self.visual_to_text(\r\n            text_features, visual_features, visual_features\r\n        ))\r\n\r\n        # Text features influence visual representation\r\n        visual_updated = self.norm2(visual_features + self.text_to_visual(\r\n            visual_features, text_features, text_features\r\n        ))\r\n\r\n        # Action features are updated based on joint representation\r\n        joint_repr = torch.cat([visual_updated, text_updated], dim=-1)\r\n        action_updated = self.norm3(action_features + self.action_to_joint(\r\n            action_features, joint_repr, joint_repr\r\n        ))\r\n\r\n        return visual_updated, text_updated, action_updated\n"})}),"\n",(0,t.jsx)(n.h2,{id:"vision-language-embeddings",children:"Vision-Language Embeddings"}),"\n",(0,t.jsx)(n.h3,{id:"clip-inspired-architecture",children:"CLIP-Inspired Architecture"}),"\n",(0,t.jsx)(n.p,{children:"CLIP (Contrastive Language-Image Pre-training) provides a foundation for vision-language alignment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class VisionLanguageEncoder(nn.Module):\r\n    def __init__(self, vision_model, text_model, embed_dim):\r\n        super().__init__()\r\n\r\n        self.vision_encoder = vision_model\r\n        self.text_encoder = text_model\r\n        self.visual_projection = nn.Linear(vision_model.dim, embed_dim)\r\n        self.text_projection = nn.Linear(text_model.dim, embed_dim)\r\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\r\n\r\n    def forward(self, images, texts):\r\n        # Encode visual and text features\r\n        visual_features = self.vision_encoder(images)  # [B, D_v]\r\n        text_features = self.text_encoder(texts)      # [B, D_t]\r\n\r\n        # Project to common embedding space\r\n        visual_embeds = self.visual_projection(visual_features)  # [B, D]\r\n        text_embeds = self.text_projection(text_features)      # [B, D]\r\n\r\n        # Normalize embeddings\r\n        visual_embeds = F.normalize(visual_embeds, dim=-1)\r\n        text_embeds = F.normalize(text_embeds, dim=-1)\r\n\r\n        # Compute similarity matrix\r\n        logits_per_image = self.logit_scale * visual_embeds @ text_embeds.t()\r\n        logits_per_text = logits_per_image.t()\r\n\r\n        return {\r\n            'visual_embeds': visual_embeds,\r\n            'text_embeds': text_embeds,\r\n            'logits_per_image': logits_per_image,\r\n            'logits_per_text': logits_per_text\r\n        }\n"})}),"\n",(0,t.jsx)(n.h3,{id:"vision-language-action-extension",children:"Vision-Language-Action Extension"}),"\n",(0,t.jsx)(n.p,{children:"Extending the vision-language framework to include action embeddings:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class VisionLanguageActionEncoder(nn.Module):\r\n    def __init__(self, vision_dim, text_dim, action_dim, embed_dim):\r\n        super().__init__()\r\n\r\n        # Individual encoders\r\n        self.vision_encoder = nn.Linear(vision_dim, embed_dim)\r\n        self.text_encoder = nn.Linear(text_dim, embed_dim)\r\n        self.action_encoder = nn.Linear(action_dim, embed_dim)\r\n\r\n        # Projection layers for contrastive learning\r\n        self.vision_proj = nn.Linear(embed_dim, embed_dim)\r\n        self.text_proj = nn.Linear(embed_dim, embed_dim)\r\n        self.action_proj = nn.Linear(embed_dim, embed_dim)\r\n\r\n        # Learnable temperature parameter\r\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\r\n\r\n    def encode_vision(self, images):\r\n        features = self.vision_encoder(images)\r\n        return F.normalize(self.vision_proj(features), dim=-1)\r\n\r\n    def encode_text(self, texts):\r\n        features = self.text_encoder(texts)\r\n        return F.normalize(self.text_proj(features), dim=-1)\r\n\r\n    def encode_action(self, actions):\r\n        features = self.action_encoder(actions)\r\n        return F.normalize(self.action_proj(features), dim=-1)\r\n\r\n    def forward(self, images, texts, actions):\r\n        # Encode all modalities\r\n        image_embeds = self.encode_vision(images)\r\n        text_embeds = self.encode_text(texts)\r\n        action_embeds = self.encode_action(actions)\r\n\r\n        # Compute similarity matrices for contrastive learning\r\n        # Image-Text similarities\r\n        logits_i2t = self.logit_scale * image_embeds @ text_embeds.t()\r\n        logits_t2i = logits_i2t.t()\r\n\r\n        # Image-Action similarities\r\n        logits_i2a = self.logit_scale * image_embeds @ action_embeds.t()\r\n        logits_a2i = logits_i2a.t()\r\n\r\n        # Text-Action similarities\r\n        logits_t2a = self.logit_scale * text_embeds @ action_embeds.t()\r\n        logits_a2t = logits_t2a.t()\r\n\r\n        return {\r\n            'image_embeds': image_embeds,\r\n            'text_embeds': text_embeds,\r\n            'action_embeds': action_embeds,\r\n            'logits_i2t': logits_i2t,\r\n            'logits_t2i': logits_t2i,\r\n            'logits_i2a': logits_i2a,\r\n            'logits_a2i': logits_a2i,\r\n            'logits_t2a': logits_t2a,\r\n            'logits_a2t': logits_a2t\r\n        }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"action-embedding-representations",children:"Action Embedding Representations"}),"\n",(0,t.jsx)(n.h3,{id:"continuous-action-spaces",children:"Continuous Action Spaces"}),"\n",(0,t.jsx)(n.p,{children:"For robotic systems, actions can be represented as continuous motor commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ActionEncoder(nn.Module):\r\n    def __init__(self, action_dim, hidden_dim, embed_dim):\r\n        super().__init__()\r\n\r\n        self.action_encoder = nn.Sequential(\r\n            nn.Linear(action_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, embed_dim)\r\n        )\r\n\r\n    def forward(self, actions):\r\n        return self.action_encoder(actions)\r\n\r\nclass TrajectoryEncoder(nn.Module):\r\n    def __init__(self, action_dim, hidden_dim, embed_dim, max_length=100):\r\n        super().__init__()\r\n\r\n        self.action_dim = action_dim\r\n        self.max_length = max_length\r\n\r\n        # Embed each action in the trajectory\r\n        self.action_embedding = nn.Linear(action_dim, hidden_dim)\r\n\r\n        # Process sequence with transformer\r\n        self.transformer = nn.TransformerEncoder(\r\n            nn.TransformerEncoderLayer(\r\n                d_model=hidden_dim,\r\n                nhead=8,\r\n                dim_feedforward=hidden_dim * 2,\r\n                batch_first=True\r\n            ),\r\n            num_layers=3\r\n        )\r\n\r\n        # Project to final embedding space\r\n        self.projection = nn.Linear(hidden_dim, embed_dim)\r\n\r\n    def forward(self, trajectories):\r\n        # trajectories: [B, T, action_dim]\r\n        B, T, _ = trajectories.shape\r\n\r\n        # Embed each action\r\n        action_embeds = self.action_embedding(trajectories)  # [B, T, hidden_dim]\r\n\r\n        # Apply transformer to model temporal dependencies\r\n        traj_embeds = self.transformer(action_embeds)  # [B, T, hidden_dim]\r\n\r\n        # Aggregate over time dimension (e.g., mean pooling)\r\n        final_embed = traj_embeds.mean(dim=1)  # [B, hidden_dim]\r\n\r\n        # Project to embedding space\r\n        return self.projection(final_embed)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"discrete-action-spaces",children:"Discrete Action Spaces"}),"\n",(0,t.jsx)(n.p,{children:"For systems with discrete action spaces:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class DiscreteActionEncoder(nn.Module):\r\n    def __init__(self, num_actions, embed_dim):\r\n        super().__init__()\r\n        self.action_embedding = nn.Embedding(num_actions, embed_dim)\r\n\r\n    def forward(self, action_indices):\r\n        return self.action_embedding(action_indices)\r\n\r\nclass ActionSequenceEncoder(nn.Module):\r\n    def __init__(self, num_actions, embed_dim, hidden_dim):\r\n        super().__init__()\r\n\r\n        self.action_embedding = nn.Embedding(num_actions, embed_dim)\r\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\r\n        self.projection = nn.Linear(hidden_dim, embed_dim)\r\n\r\n    def forward(self, action_sequences):\r\n        # action_sequences: [B, T] where T is sequence length\r\n        embedded = self.action_embedding(action_sequences)  # [B, T, embed_dim]\r\n        lstm_out, (hidden, _) = self.lstm(embedded)\r\n\r\n        # Use final hidden state as sequence embedding\r\n        return self.projection(hidden[-1])  # [B, embed_dim]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"training-strategies",children:"Training Strategies"}),"\n",(0,t.jsx)(n.h3,{id:"contrastive-learning",children:"Contrastive Learning"}),"\n",(0,t.jsx)(n.p,{children:"Contrastive learning is a key technique for training multimodal embeddings:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def contrastive_loss(similarities, labels=None):\r\n    \"\"\"\r\n    Compute contrastive loss for multimodal alignment\r\n    similarities: similarity matrix between modalities\r\n    \"\"\"\r\n    # For positive pairs (diagonal), we want high similarity\r\n    # For negative pairs (off-diagonal), we want low similarity\r\n\r\n    batch_size = similarities.size(0)\r\n\r\n    # Create labels for contrastive learning\r\n    if labels is None:\r\n        labels = torch.arange(batch_size, device=similarities.device)\r\n\r\n    # Compute cross-entropy loss\r\n    loss = F.cross_entropy(similarities, labels)\r\n\r\n    return loss\r\n\r\nclass MultimodalTrainer:\r\n    def __init__(self, model, learning_rate=1e-4):\r\n        self.model = model\r\n        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\r\n\r\n    def train_step(self, images, texts, actions):\r\n        self.optimizer.zero_grad()\r\n\r\n        outputs = self.model(images, texts, actions)\r\n\r\n        # Compute contrastive losses for all modality pairs\r\n        loss_i2t = contrastive_loss(outputs['logits_i2t'])\r\n        loss_t2i = contrastive_loss(outputs['logits_t2i'])\r\n        loss_i2a = contrastive_loss(outputs['logits_i2a'])\r\n        loss_a2i = contrastive_loss(outputs['logits_a2i'])\r\n        loss_t2a = contrastive_loss(outputs['logits_t2a'])\r\n        loss_a2t = contrastive_loss(outputs['logits_a2t'])\r\n\r\n        # Total loss\r\n        total_loss = (loss_i2t + loss_t2i + loss_i2a +\r\n                     loss_a2i + loss_t2a + loss_a2t) / 6\r\n\r\n        total_loss.backward()\r\n        self.optimizer.step()\r\n\r\n        return total_loss.item()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"triplet-loss",children:"Triplet Loss"}),"\n",(0,t.jsx)(n.p,{children:"Triplet loss can be used to ensure that related samples are closer than unrelated ones:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def triplet_loss(anchor, positive, negative, margin=0.2):\r\n    """\r\n    Compute triplet loss: d(anchor, positive) + margin < d(anchor, negative)\r\n    """\r\n    pos_dist = F.pairwise_distance(anchor, positive)\r\n    neg_dist = F.pairwise_distance(anchor, negative)\r\n\r\n    loss = F.relu(pos_dist - neg_dist + margin)\r\n    return loss.mean()\r\n\r\nclass TripletMultimodalTrainer:\r\n    def __init__(self, model, learning_rate=1e-4):\r\n        self.model = model\r\n        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\r\n\r\n    def train_step(self, anchors, positives, negatives):\r\n        """\r\n        anchors: [images, texts, or actions]\r\n        positives: corresponding modality samples\r\n        negatives: non-corresponding modality samples\r\n        """\r\n        self.optimizer.zero_grad()\r\n\r\n        anchor_embeds = self.model.encode_vision(anchors)  # or text/action encoder\r\n        pos_embeds = self.model.encode_text(positives)    # or appropriate encoder\r\n        neg_embeds = self.model.encode_action(negatives)  # or appropriate encoder\r\n\r\n        loss = triplet_loss(anchor_embeds, pos_embeds, neg_embeds)\r\n\r\n        loss.backward()\r\n        self.optimizer.step()\r\n\r\n        return loss.item()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,t.jsx)(n.h3,{id:"cross-modal-retrieval",children:"Cross-Modal Retrieval"}),"\n",(0,t.jsx)(n.p,{children:"Cross-modal retrieval evaluates how well embeddings from one modality can retrieve relevant samples from another:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def evaluate_retrieval(embeddings1, embeddings2, k=1):\r\n    \"\"\"\r\n    Evaluate cross-modal retrieval performance\r\n    embeddings1: embeddings from modality 1\r\n    embeddings2: embeddings from modality 2\r\n    k: number of top results to consider\r\n    \"\"\"\r\n    # Compute similarity matrix\r\n    similarities = embeddings1 @ embeddings2.t()  # [N, N]\r\n\r\n    # For each sample, find the rank of its corresponding sample\r\n    ranks = []\r\n    for i in range(len(embeddings1)):\r\n        # Get similarities for sample i\r\n        sample_similarities = similarities[i]\r\n\r\n        # Sort in descending order and get indices\r\n        _, indices = torch.sort(sample_similarities, descending=True)\r\n\r\n        # Find rank of corresponding sample (diagonal element)\r\n        rank = (indices == i).nonzero(as_tuple=True)[0].item() + 1\r\n        ranks.append(rank)\r\n\r\n    # Compute metrics\r\n    ranks = torch.tensor(ranks)\r\n    r1 = (ranks <= 1).float().mean()\r\n    r5 = (ranks <= 5).float().mean()\r\n    r10 = (ranks <= 10).float().mean()\r\n    medr = ranks.median().item()\r\n    meanr = ranks.mean().item()\r\n\r\n    return {\r\n        'R@1': r1.item(),\r\n        'R@5': r5.item(),\r\n        'R@10': r10.item(),\r\n        'MedR': medr,\r\n        'MeanR': meanr\r\n    }\r\n\r\ndef evaluate_vla_system(vla_model, test_dataset):\r\n    \"\"\"Evaluate complete VLA system\"\"\"\r\n    all_image_embeds = []\r\n    all_text_embeds = []\r\n    all_action_embeds = []\r\n\r\n    with torch.no_grad():\r\n        for batch in test_dataset:\r\n            images, texts, actions = batch\r\n            outputs = vla_model(images, texts, actions)\r\n\r\n            all_image_embeds.append(outputs['image_embeds'])\r\n            all_text_embeds.append(outputs['text_embeds'])\r\n            all_action_embeds.append(outputs['action_embeds'])\r\n\r\n    # Concatenate all embeddings\r\n    all_image_embeds = torch.cat(all_image_embeds, dim=0)\r\n    all_text_embeds = torch.cat(all_text_embeds, dim=0)\r\n    all_action_embeds = torch.cat(all_action_embeds, dim=0)\r\n\r\n    # Evaluate all cross-modal retrieval tasks\r\n    results = {}\r\n    results['image_to_text'] = evaluate_retrieval(all_image_embeds, all_text_embeds)\r\n    results['text_to_image'] = evaluate_retrieval(all_text_embeds, all_image_embeds)\r\n    results['image_to_action'] = evaluate_retrieval(all_image_embeds, all_action_embeds)\r\n    results['action_to_image'] = evaluate_retrieval(all_action_embeds, all_image_embeds)\r\n    results['text_to_action'] = evaluate_retrieval(all_text_embeds, all_action_embeds)\r\n    results['action_to_text'] = evaluate_retrieval(all_action_embeds, all_text_embeds)\r\n\r\n    return results\n"})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-optimization",children:"Real-Time Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"efficient-embedding-computation",children:"Efficient Embedding Computation"}),"\n",(0,t.jsx)(n.p,{children:"For real-time robotic applications, efficient embedding computation is crucial:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class EfficientMultimodalEncoder(nn.Module):\r\n    def __init__(self, embed_dim=512):\r\n        super().__init__()\r\n\r\n        # Lightweight encoders for real-time performance\r\n        self.visual_encoder = nn.Sequential(\r\n            nn.Conv2d(3, 32, 3, stride=2, padding=1),  # Downsample\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 64, 3, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\r\n            nn.Flatten(),\r\n            nn.Linear(64, embed_dim),\r\n            nn.LayerNorm(embed_dim)\r\n        )\r\n\r\n        self.text_encoder = nn.Sequential(\r\n            nn.Linear(768, embed_dim),  # Assuming BERT-like input\r\n            nn.ReLU(),\r\n            nn.Linear(embed_dim, embed_dim),\r\n            nn.LayerNorm(embed_dim)\r\n        )\r\n\r\n        self.action_encoder = nn.Sequential(\r\n            nn.Linear(7, embed_dim),  # Assuming 7-DOF robot joint positions\r\n            nn.ReLU(),\r\n            nn.Linear(embed_dim, embed_dim),\r\n            nn.LayerNorm(embed_dim)\r\n        )\r\n\r\n    def forward(self, visual_input, text_input, action_input):\r\n        visual_embed = self.visual_encoder(visual_input)\r\n        text_embed = self.text_encoder(text_input)\r\n        action_embed = self.action_encoder(action_input)\r\n\r\n        return visual_embed, text_embed, action_embed\r\n\r\n# Quantization for even faster inference\r\ndef quantize_model(model):\r\n    """Apply quantization to reduce model size and improve inference speed"""\r\n    import torch.quantization as quant\r\n\r\n    model.eval()\r\n\r\n    # Specify layers to quantize\r\n    quant_backend = \'qnnpack\'  # Use QNNPACK for mobile/edge\r\n    model.qconfig = quant.get_default_qconfig(quant_backend)\r\n\r\n    # Fuse operations for better quantization\r\n    quant.quantize_dynamic(model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8)\r\n\r\n    return model\n'})}),"\n",(0,t.jsx)(n.h3,{id:"caching-and-precomputation",children:"Caching and Precomputation"}),"\n",(0,t.jsx)(n.p,{children:"For frequently accessed embeddings, caching can significantly improve performance:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class CachedMultimodalSystem:\r\n    def __init__(self, model, cache_size=1000):\r\n        self.model = model\r\n        self.cache_size = cache_size\r\n        self.embedding_cache = {}\r\n        self.access_times = {}\r\n\r\n    def get_or_compute_embedding(self, input_tensor, modality_type):\r\n        """Get embedding from cache or compute if not present"""\r\n        # Create a hashable key from the input tensor\r\n        tensor_hash = hash(input_tensor.mean().item())  # Simplified hashing\r\n\r\n        cache_key = f"{modality_type}_{tensor_hash}"\r\n\r\n        if cache_key in self.embedding_cache:\r\n            # Update access time for LRU\r\n            self.access_times[cache_key] = time.time()\r\n            return self.embedding_cache[cache_key]\r\n\r\n        # Compute embedding\r\n        with torch.no_grad():\r\n            if modality_type == \'visual\':\r\n                embedding = self.model.encode_vision(input_tensor)\r\n            elif modality_type == \'text\':\r\n                embedding = self.model.encode_text(input_tensor)\r\n            elif modality_type == \'action\':\r\n                embedding = self.model.encode_action(input_tensor)\r\n\r\n        # Add to cache\r\n        self.embedding_cache[cache_key] = embedding\r\n        self.access_times[cache_key] = time.time()\r\n\r\n        # Evict oldest entries if cache is full\r\n        if len(self.embedding_cache) > self.cache_size:\r\n            oldest_key = min(self.access_times.keys(),\r\n                           key=lambda k: self.access_times[k])\r\n            del self.embedding_cache[oldest_key]\r\n            del self.access_times[oldest_key]\r\n\r\n        return embedding\n'})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-integration",children:"Isaac Integration"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-for-multimodal-processing",children:"Isaac ROS for Multimodal Processing"}),"\n",(0,t.jsx)(n.p,{children:"Integrating multimodal embeddings with Isaac ROS systems:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport numpy as np\r\n\r\nclass IsaacMultimodalNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_multimodal_node\')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Load pre-trained multimodal model\r\n        self.model = self.load_multimodal_model()\r\n        self.model.eval()\r\n\r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'camera/image_rect_color\', self.image_callback, 10\r\n        )\r\n\r\n        self.text_sub = self.create_subscription(\r\n            String, \'natural_language_command\', self.text_callback, 10\r\n        )\r\n\r\n        self.command_pub = self.create_publisher(Twist, \'robot_velocity\', 10)\r\n\r\n        # Store latest embeddings\r\n        self.latest_visual_embed = None\r\n        self.latest_text_embed = None\r\n\r\n    def load_multimodal_model(self):\r\n        """Load pre-trained multimodal model"""\r\n        # This would load your trained VLA model\r\n        model = VisionLanguageActionEncoder(\r\n            vision_dim=512, text_dim=512, action_dim=7, embed_dim=512\r\n        )\r\n\r\n        # Load pre-trained weights\r\n        # model.load_state_dict(torch.load(\'vla_model.pth\'))\r\n\r\n        return model\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera image"""\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n\r\n            # Preprocess image\r\n            image_tensor = self.preprocess_image(cv_image)\r\n\r\n            # Compute visual embedding\r\n            with torch.no_grad():\r\n                self.latest_visual_embed = self.model.encode_vision(image_tensor)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error processing image: {e}\')\r\n\r\n    def text_callback(self, msg):\r\n        """Process incoming text command"""\r\n        try:\r\n            # Tokenize and encode text\r\n            text_tensor = self.tokenize_text(msg.data)\r\n\r\n            # Compute text embedding\r\n            with torch.no_grad():\r\n                self.latest_text_embed = self.model.encode_text(text_tensor)\r\n\r\n            # If we have both visual and text embeddings, generate action\r\n            if self.latest_visual_embed is not None and self.latest_text_embed is not None:\r\n                self.generate_action()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error processing text: {e}\')\r\n\r\n    def preprocess_image(self, image):\r\n        """Preprocess image for model input"""\r\n        # Resize, normalize, convert to tensor\r\n        import cv2\r\n        image_resized = cv2.resize(image, (224, 224))\r\n        image_normalized = image_resized.astype(np.float32) / 255.0\r\n        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).unsqueeze(0)\r\n        return image_tensor\r\n\r\n    def tokenize_text(self, text):\r\n        """Tokenize text for model input"""\r\n        # This would use your text tokenizer\r\n        # For now, return a dummy tensor\r\n        return torch.randn(1, 768)  # Placeholder\r\n\r\n    def generate_action(self):\r\n        """Generate robot action based on visual and text embeddings"""\r\n        # Compute similarity between visual and text embeddings\r\n        similarity = F.cosine_similarity(\r\n            self.latest_visual_embed, self.latest_text_embed\r\n        )\r\n\r\n        # Based on similarity and text content, generate appropriate action\r\n        command = Twist()\r\n\r\n        if similarity > 0.8:  # High similarity indicates relevant command\r\n            # This is a simplified example - in practice, you\'d have more sophisticated logic\r\n            command.linear.x = 0.5  # Move forward\r\n            command.angular.z = 0.0  # No rotation\r\n\r\n        self.command_pub.publish(command)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Multimodal embeddings form the backbone of Vision-Language-Action systems by creating unified representations that connect visual, linguistic, and action modalities. Through careful design of embedding architectures, training strategies, and optimization techniques, we can build systems that understand the relationships between what robots see, what humans say, and what robots do."}),"\n",(0,t.jsx)(n.p,{children:"The next section will explore instruction following and task planning, which builds upon these embedding foundations to enable robots to interpret and execute natural language commands."}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsx)(n.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>o});var r=i(6540);const t={},s=r.createContext(t);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);