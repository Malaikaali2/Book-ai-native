"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[3783],{8453(e,n,r){r.d(n,{R:()=>i,x:()=>o});var t=r(6540);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}},9751(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/lab-vla-integration","title":"Lab: VLA System Integration","description":"Learning Objectives","source":"@site/docs/module-4-vla/lab-vla-integration.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/lab-vla-integration","permalink":"/Book-ai-native/docs/module-4-vla/lab-vla-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/module-4-vla/lab-vla-integration.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Natural Language to Robot Action Mapping","permalink":"/Book-ai-native/docs/module-4-vla/nlp-robot-mapping"},"next":{"title":"Module 4 Summary: Vision-Language-Action (VLA) Systems","permalink":"/Book-ai-native/docs/module-4-vla/summary"}}');var s=r(4848),a=r(8453);const i={sidebar_position:8},o="Lab: VLA System Integration",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Lab Overview",id:"lab-overview",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Lab Duration",id:"lab-duration",level:3},{value:"Setting Up the Development Environment",id:"setting-up-the-development-environment",level:2},{value:"Required Software Stack",id:"required-software-stack",level:3},{value:"Creating the VLA Package Structure",id:"creating-the-vla-package-structure",level:3},{value:"Creating the Complete VLA System",id:"creating-the-complete-vla-system",level:2},{value:"Creating the Launch File",id:"creating-the-launch-file",level:2},{value:"Creating Test Scripts",id:"creating-test-scripts",level:2},{value:"Performance Evaluation Script",id:"performance-evaluation-script",level:2},{value:"Running the Complete VLA System",id:"running-the-complete-vla-system",level:2},{value:"Validation and Testing",id:"validation-and-testing",level:2},{value:"Summary and Next Steps",id:"summary-and-next-steps",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Further Enhancements",id:"further-enhancements",level:3},{value:"References",id:"references",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lab-vla-system-integration",children:"Lab: VLA System Integration"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By completing this lab, you will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate vision, language, and action components into a complete VLA system"}),"\n",(0,s.jsx)(n.li,{children:"Implement multimodal data processing pipelines for real-time operation"}),"\n",(0,s.jsx)(n.li,{children:"Deploy and test a complete vision-language-action system on a robot platform"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the performance and accuracy of the integrated VLA system"}),"\n",(0,s.jsx)(n.li,{children:"Debug and optimize VLA system performance for practical applications"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-overview",children:"Lab Overview"}),"\n",(0,s.jsx)(n.p,{children:"This lab provides hands-on experience with implementing a complete Vision-Language-Action (VLA) system. You will build upon the individual components developed in previous sections to create an integrated system that can understand natural language commands, perceive the environment, and execute appropriate robotic actions."}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting this lab, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of Modules 1-3 (ROS 2, Digital Twin, NVIDIA Isaac)"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with Module 4 concepts (VLA, multimodal embeddings, instruction following)"}),"\n",(0,s.jsx)(n.li,{children:"Access to a robotic platform (physical or simulated)"}),"\n",(0,s.jsx)(n.li,{children:"Development environment with ROS 2, Python, and necessary libraries"}),"\n",(0,s.jsx)(n.li,{children:"NVIDIA GPU for acceleration (recommended)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"lab-duration",children:"Lab Duration"}),"\n",(0,s.jsx)(n.p,{children:"This lab should take approximately 6-8 hours to complete, depending on your familiarity with the components and debugging requirements."}),"\n",(0,s.jsx)(n.h2,{id:"setting-up-the-development-environment",children:"Setting Up the Development Environment"}),"\n",(0,s.jsx)(n.h3,{id:"required-software-stack",children:"Required Software Stack"}),"\n",(0,s.jsx)(n.p,{children:"First, let's set up the complete software stack for the VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\r\n# setup_vla_system.sh - Setup script for VLA system\r\n\r\necho "Setting up VLA system environment..."\r\n\r\n# Install Python dependencies\r\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\r\npip install transformers==4.21.0\r\npip install python-speech-features\r\npip install webrtcvad\r\npip install pyaudio\r\npip install opencv-python\r\npip install scipy\r\npip install numpy\r\npip install nltk\r\n\r\n# Install ROS 2 dependencies\r\nsudo apt update\r\nsudo apt install -y ros-humble-vision-msgs\r\nsudo apt install -y ros-humble-sensor-msgs\r\nsudo apt install -y ros-humble-geometry-msgs\r\nsudo apt install -y ros-humble-action-msgs\r\n\r\n# Install Isaac ROS packages (if using Isaac)\r\nsudo apt install -y ros-humble-isaac-ros-common\r\nsudo apt install -y ros-humble-isaac-ros-dnn-inference\r\nsudo apt install -y ros-humble-isaac-ros-image-pipeline\r\n\r\necho "VLA system environment setup complete."\n'})}),"\n",(0,s.jsx)(n.h3,{id:"creating-the-vla-package-structure",children:"Creating the VLA Package Structure"}),"\n",(0,s.jsx)(n.p,{children:"Let's create the ROS 2 package for our VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create the VLA system package\r\nmkdir -p ~/vla_ws/src\r\ncd ~/vla_ws/src\r\nros2 pkg create --build-type ament_python vla_system\r\ncd vla_system\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now let's create the main VLA system node:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: vla_system/vla_system/vla_integrated_node.py\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image, CameraInfo, JointState\r\nfrom geometry_msgs.msg import Pose, Twist\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\r\nfrom cv_bridge import CvBridge\r\nimport message_filters\r\nimport time\r\nfrom collections import deque\r\n\r\nclass VLAIntegratedNode(Node):\r\n    def __init__(self):\r\n        super().__init__('vla_integrated_node')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Publishers\r\n        self.cmd_vel_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\r\n        self.status_publisher = self.create_publisher(String, 'vla_status', 10)\r\n\r\n        # Subscribers with synchronization\r\n        self.image_sub = message_filters.Subscriber(\r\n            self, Image, 'camera/image_rect_color',\r\n            qos_profile=QoSProfile(\r\n                reliability=ReliabilityPolicy.BEST_EFFORT,\r\n                history=HistoryPolicy.KEEP_LAST,\r\n                depth=1\r\n            )\r\n        )\r\n        self.info_sub = message_filters.Subscriber(\r\n            self, CameraInfo, 'camera/camera_info',\r\n            qos_profile=QoSProfile(\r\n                reliability=ReliabilityPolicy.BEST_EFFORT,\r\n                history=HistoryPolicy.KEEP_LAST,\r\n                depth=1\r\n            )\r\n        )\r\n\r\n        # Synchronize image and camera info\r\n        self.sync = message_filters.ApproximateTimeSynchronizer(\r\n            [self.image_sub, self.info_sub], queue_size=5, slop=0.1\r\n        )\r\n        self.sync.registerCallback(self.process_vision_data)\r\n\r\n        # Language command subscriber\r\n        self.language_sub = self.create_subscription(\r\n            String, 'natural_language_command', self.language_command_callback, 10\r\n        )\r\n\r\n        # Initialize VLA components\r\n        self.vision_processor = VisionProcessor()\r\n        self.language_processor = LanguageProcessor()\r\n        self.action_planner = ActionPlanner()\r\n        self.action_executor = ActionExecutor(self)\r\n\r\n        # System state\r\n        self.current_context = {\r\n            'last_image': None,\r\n            'last_detections': None,\r\n            'last_command': None,\r\n            'robot_state': {}\r\n        }\r\n\r\n        # Performance tracking\r\n        self.processing_times = deque(maxlen=50)\r\n        self.frame_count = 0\r\n\r\n        self.get_logger().info('VLA Integrated Node initialized')\r\n\r\n    def process_vision_data(self, image_msg, info_msg):\r\n        \"\"\"Process synchronized vision data\"\"\"\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\r\n\r\n            # Process image through vision pipeline\r\n            detections = self.vision_processor.process_image(cv_image)\r\n\r\n            # Update context\r\n            self.current_context['last_image'] = cv_image\r\n            self.current_context['last_detections'] = detections\r\n\r\n            # If we have a pending command, process it with current vision data\r\n            if self.current_context['last_command']:\r\n                self.process_command_with_context(\r\n                    self.current_context['last_command'],\r\n                    cv_image, detections\r\n                )\r\n\r\n            # Track performance\r\n            self.frame_count += 1\r\n            if self.frame_count % 30 == 0:  # Log every 30 frames\r\n                avg_time = np.mean(self.processing_times) if self.processing_times else 0\r\n                fps = 30 / avg_time if avg_time > 0 else 0\r\n                self.get_logger().info(f'VLA vision processing: {fps:.2f} FPS')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing vision data: {e}')\r\n\r\n    def language_command_callback(self, msg):\r\n        \"\"\"Handle incoming language commands\"\"\"\r\n        try:\r\n            command = msg.data\r\n            self.get_logger().info(f'Received command: {command}')\r\n\r\n            # Update context\r\n            self.current_context['last_command'] = command\r\n\r\n            # Process command if we have current vision data\r\n            if self.current_context['last_image'] is not None:\r\n                self.process_command_with_context(\r\n                    command,\r\n                    self.current_context['last_image'],\r\n                    self.current_context['last_detections']\r\n                )\r\n            else:\r\n                # Store command for later processing when vision data is available\r\n                self.get_logger().info('Command stored, waiting for vision data')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing language command: {e}')\r\n\r\n    def process_command_with_context(self, command, image, detections):\r\n        \"\"\"Process command with current vision context\"\"\"\r\n        start_time = time.time()\r\n\r\n        try:\r\n            # Parse language command\r\n            parsed_command = self.language_processor.parse_command(command)\r\n\r\n            # Ground command in visual context\r\n            grounded_command = self.language_processor.ground_command(\r\n                parsed_command, detections, image\r\n            )\r\n\r\n            # Plan action\r\n            action_plan = self.action_planner.plan_action(grounded_command, self.current_context)\r\n\r\n            # Execute action\r\n            execution_result = self.action_executor.execute_action_plan(action_plan)\r\n\r\n            # Log results\r\n            self.get_logger().info(f'Command executed: {command} -> {execution_result}')\r\n\r\n            # Publish status\r\n            status_msg = String()\r\n            status_msg.data = f'Command executed: {command}'\r\n            self.status_publisher.publish(status_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing command with context: {e}')\r\n\r\n        # Track processing time\r\n        processing_time = time.time() - start_time\r\n        self.processing_times.append(processing_time)\r\n\r\n    def get_current_robot_state(self):\r\n        \"\"\"Get current robot state\"\"\"\r\n        # This would interface with robot state publisher\r\n        return self.current_context['robot_state']\r\n\r\nclass VisionProcessor:\r\n    def __init__(self):\r\n        # Initialize vision models\r\n        # For this lab, we'll use OpenCV for basic processing\r\n        # In a real system, you'd load pre-trained models\r\n        pass\r\n\r\n    def process_image(self, image):\r\n        \"\"\"Process image and detect objects\"\"\"\r\n        # Convert to grayscale for simple processing\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n        # Simple circle detection (for demonstration)\r\n        # In real system, use object detection models\r\n        circles = cv2.HoughCircles(\r\n            gray, cv2.HOUGH_GRADIENT, 1, 20,\r\n            param1=50, param2=30, minRadius=10, maxRadius=100\r\n        )\r\n\r\n        detections = []\r\n        if circles is not None:\r\n            circles = np.round(circles[0, :]).astype(\"int\")\r\n            for (x, y, r) in circles:\r\n                detection = {\r\n                    'type': 'circle',\r\n                    'center': (x, y),\r\n                    'radius': r,\r\n                    'bbox': [x-r, y-r, 2*r, 2*r]\r\n                }\r\n                detections.append(detection)\r\n\r\n        return detections\r\n\r\nclass LanguageProcessor:\r\n    def __init__(self):\r\n        # Initialize language models\r\n        # For this lab, we'll use simple pattern matching\r\n        # In a real system, you'd load transformer models\r\n        self.command_patterns = {\r\n            'navigation': [\r\n                r'go to (?:the )?(?P<location>\\w+)',\r\n                r'move to (?:the )?(?P<location>\\w+)',\r\n                r'go (?:to )?(?P<location>\\w+)'\r\n            ],\r\n            'manipulation': [\r\n                r'(?:grasp|pick up|take|grab) (?:the )?(?P<object>\\w+)',\r\n                r'(?:place|put) (?:the )?(?P<object>\\w+)'\r\n            ]\r\n        }\r\n\r\n    def parse_command(self, command):\r\n        \"\"\"Parse natural language command\"\"\"\r\n        command_lower = command.lower()\r\n\r\n        for action_type, patterns in self.command_patterns.items():\r\n            for pattern in patterns:\r\n                import re\r\n                match = re.search(pattern, command_lower)\r\n                if match:\r\n                    result = {'action_type': action_type}\r\n                    result.update(match.groupdict())\r\n                    return result\r\n\r\n        return {'action_type': 'unknown', 'raw_command': command}\r\n\r\n    def ground_command(self, parsed_command, detections, image):\r\n        \"\"\"Ground command in visual context\"\"\"\r\n        if 'object' in parsed_command and detections:\r\n            # Find the object in detections\r\n            target_obj = self.find_object_in_detections(\r\n                parsed_command['object'], detections\r\n            )\r\n            if target_obj:\r\n                parsed_command['target_object'] = target_obj\r\n\r\n        if 'location' in parsed_command:\r\n            # In a real system, this would ground locations in the map\r\n            parsed_command['target_location'] = parsed_command['location']\r\n\r\n        return parsed_command\r\n\r\n    def find_object_in_detections(self, object_name, detections):\r\n        \"\"\"Find object in detections\"\"\"\r\n        for detection in detections:\r\n            if object_name in detection['type']:\r\n                return detection\r\n        return None\r\n\r\nclass ActionPlanner:\r\n    def __init__(self):\r\n        pass\r\n\r\n    def plan_action(self, grounded_command, context):\r\n        \"\"\"Plan action based on grounded command\"\"\"\r\n        action_type = grounded_command['action_type']\r\n\r\n        if action_type == 'navigation':\r\n            return self.plan_navigation(grounded_command, context)\r\n        elif action_type == 'manipulation':\r\n            return self.plan_manipulation(grounded_command, context)\r\n        else:\r\n            return self.plan_default_action(grounded_command, context)\r\n\r\n    def plan_navigation(self, command, context):\r\n        \"\"\"Plan navigation action\"\"\"\r\n        target_location = command.get('target_location', 'unknown')\r\n        return [{\r\n            'action_type': 'navigate_to',\r\n            'target_location': target_location,\r\n            'parameters': {'speed': 0.5, 'avoid_obstacles': True}\r\n        }]\r\n\r\n    def plan_manipulation(self, command, context):\r\n        \"\"\"Plan manipulation action\"\"\"\r\n        target_object = command.get('target_object')\r\n        if target_object:\r\n            return [{\r\n                'action_type': 'grasp_object',\r\n                'target_object': target_object,\r\n                'parameters': {'approach_distance': 0.1, 'gripper_width': 0.05}\r\n            }]\r\n        return []\r\n\r\n    def plan_default_action(self, command, context):\r\n        \"\"\"Plan default action for unknown commands\"\"\"\r\n        return [{\r\n            'action_type': 'request_clarification',\r\n            'message': f\"I don't know how to '{command.get('raw_command', '')}'\",\r\n            'parameters': {}\r\n        }]\r\n\r\nclass ActionExecutor:\r\n    def __init__(self, node):\r\n        self.node = node\r\n        self.cmd_vel_publisher = node.cmd_vel_publisher\r\n\r\n    def execute_action_plan(self, action_plan):\r\n        \"\"\"Execute the planned actions\"\"\"\r\n        results = []\r\n\r\n        for action in action_plan:\r\n            result = self.execute_single_action(action)\r\n            results.append(result)\r\n\r\n            # If action failed, stop execution\r\n            if not result['success']:\r\n                break\r\n\r\n        return results\r\n\r\n    def execute_single_action(self, action):\r\n        \"\"\"Execute a single action\"\"\"\r\n        action_type = action['action_type']\r\n\r\n        if action_type == 'navigate_to':\r\n            return self.execute_navigation(action)\r\n        elif action_type == 'grasp_object':\r\n            return self.execute_grasp(action)\r\n        elif action_type == 'request_clarification':\r\n            return self.execute_request_clarification(action)\r\n        else:\r\n            return {'success': False, 'error': f'Unknown action type: {action_type}'}\r\n\r\n    def execute_navigation(self, action):\r\n        \"\"\"Execute navigation action\"\"\"\r\n        # Publish velocity command for simple navigation\r\n        twist = Twist()\r\n        twist.linear.x = 0.2  # Move forward at 0.2 m/s\r\n        twist.angular.z = 0.0  # No rotation\r\n\r\n        # Publish for 2 seconds (simple demonstration)\r\n        start_time = time.time()\r\n        while time.time() - start_time < 2.0:\r\n            self.cmd_vel_publisher.publish(twist)\r\n            time.sleep(0.1)\r\n\r\n        # Stop\r\n        twist.linear.x = 0.0\r\n        self.cmd_vel_publisher.publish(twist)\r\n\r\n        return {'success': True, 'action': 'navigation_completed'}\r\n\r\n    def execute_grasp(self, action):\r\n        \"\"\"Execute grasp action\"\"\"\r\n        # In a real system, this would control the gripper\r\n        # For simulation, just return success\r\n        return {'success': True, 'action': 'grasp_completed'}\r\n\r\n    def execute_request_clarification(self, action):\r\n        \"\"\"Execute clarification request\"\"\"\r\n        message = action.get('message', 'Please clarify your command')\r\n        self.node.get_logger().info(f'Clarification needed: {message}')\r\n        return {'success': True, 'action': 'clarification_requested'}\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    vla_node = VLAIntegratedNode()\r\n\r\n    try:\r\n        rclpy.spin(vla_node)\r\n    except KeyboardInterrupt:\r\n        vla_node.get_logger().info('Shutting down VLA node')\r\n    finally:\r\n        vla_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"creating-the-complete-vla-system",children:"Creating the Complete VLA System"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a more sophisticated version with proper multimodal integration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: vla_system/vla_system/vla_advanced_node.py\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String, Bool\r\nfrom sensor_msgs.msg import Image, CameraInfo, JointState\r\nfrom geometry_msgs.msg import Twist, Pose\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom audio_msgs.msg import AudioData\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\nfrom cv_bridge import CvBridge\r\nimport message_filters\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nimport time\r\nfrom collections import deque\r\nimport threading\r\nimport queue\r\n\r\nclass AdvancedVLANode(Node):\r\n    def __init__(self):\r\n        super().__init__('advanced_vla_node')\r\n\r\n        # Initialize components\r\n        self.bridge = CvBridge()\r\n        self.vision_system = VisionSystem()\r\n        self.language_system = LanguageSystem()\r\n        self.action_system = ActionSystem(self)\r\n        self.voice_system = VoiceSystem()\r\n\r\n        # Publishers\r\n        self.cmd_vel_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\r\n        self.status_publisher = self.create_publisher(String, 'vla_status', 10)\r\n        self.action_publisher = self.create_publisher(String, 'executed_action', 10)\r\n\r\n        # Subscribers with synchronization\r\n        self.image_sub = message_filters.Subscriber(\r\n            self, Image, 'camera/image_rect_color',\r\n            qos_profile=QoSProfile(\r\n                reliability=ReliabilityPolicy.BEST_EFFORT,\r\n                history=HistoryPolicy.KEEP_LAST,\r\n                depth=1\r\n            )\r\n        )\r\n        self.info_sub = message_filters.Subscriber(\r\n            self, CameraInfo, 'camera/camera_info',\r\n            qos_profile=QoSProfile(\r\n                reliability=ReliabilityPolicy.BEST_EFFORT,\r\n                history=HistoryPolicy.KEEP_LAST,\r\n                depth=1\r\n            )\r\n        )\r\n\r\n        # Synchronize image and camera info\r\n        self.sync = message_filters.ApproximateTimeSynchronizer(\r\n            [self.image_sub, self.info_sub], queue_size=5, slop=0.1\r\n        )\r\n        self.sync.registerCallback(self.process_vision_data)\r\n\r\n        # Language command subscriber\r\n        self.language_sub = self.create_subscription(\r\n            String, 'natural_language_command', self.language_command_callback, 10\r\n        )\r\n\r\n        # Voice command subscriber\r\n        self.voice_sub = self.create_subscription(\r\n            AudioData, 'audio_input', self.voice_command_callback, 10\r\n        )\r\n\r\n        # Joint state subscriber\r\n        self.joint_sub = self.create_subscription(\r\n            JointState, 'joint_states', self.joint_state_callback, 10\r\n        )\r\n\r\n        # System state\r\n        self.system_state = {\r\n            'current_image': None,\r\n            'current_detections': [],\r\n            'current_pose': None,\r\n            'current_joints': [],\r\n            'command_history': deque(maxlen=10),\r\n            'execution_queue': queue.Queue(),\r\n            'is_executing': False\r\n        }\r\n\r\n        # Performance tracking\r\n        self.performance_metrics = {\r\n            'vision_fps': deque(maxlen=50),\r\n            'language_latency': deque(maxlen=50),\r\n            'action_success_rate': deque(maxlen=50)\r\n        }\r\n\r\n        self.get_logger().info('Advanced VLA Node initialized')\r\n\r\n    def process_vision_data(self, image_msg, info_msg):\r\n        \"\"\"Process synchronized vision data\"\"\"\r\n        start_time = time.time()\r\n\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\r\n\r\n            # Process through vision system\r\n            vision_result = self.vision_system.process(cv_image)\r\n\r\n            # Update system state\r\n            self.system_state['current_image'] = cv_image\r\n            self.system_state['current_detections'] = vision_result['detections']\r\n\r\n            # Calculate vision processing FPS\r\n            processing_time = time.time() - start_time\r\n            fps = 1.0 / processing_time if processing_time > 0 else 0\r\n            self.performance_metrics['vision_fps'].append(fps)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing vision data: {e}')\r\n\r\n    def language_command_callback(self, msg):\r\n        \"\"\"Handle incoming language commands\"\"\"\r\n        try:\r\n            command = msg.data\r\n            self.get_logger().info(f'Received language command: {command}')\r\n\r\n            # Add to command history\r\n            self.system_state['command_history'].append({\r\n                'command': command,\r\n                'timestamp': time.time(),\r\n                'source': 'language'\r\n            })\r\n\r\n            # Process command\r\n            self.process_command(command, 'language')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing language command: {e}')\r\n\r\n    def voice_command_callback(self, msg):\r\n        \"\"\"Handle incoming voice commands\"\"\"\r\n        try:\r\n            # Convert audio data to numpy\r\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n            # Process through voice system\r\n            transcription = self.voice_system.transcribe(audio_data)\r\n\r\n            if transcription and len(transcription.strip()) > 0:\r\n                self.get_logger().info(f'Received voice command: {transcription}')\r\n\r\n                # Add to command history\r\n                self.system_state['command_history'].append({\r\n                    'command': transcription,\r\n                    'timestamp': time.time(),\r\n                    'source': 'voice'\r\n                })\r\n\r\n                # Process command\r\n                self.process_command(transcription, 'voice')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing voice command: {e}')\r\n\r\n    def joint_state_callback(self, msg):\r\n        \"\"\"Handle joint state updates\"\"\"\r\n        self.system_state['current_joints'] = list(msg.position)\r\n\r\n    def process_command(self, command, source):\r\n        \"\"\"Process a command through the complete VLA pipeline\"\"\"\r\n        start_time = time.time()\r\n\r\n        try:\r\n            # Step 1: Language understanding\r\n            language_result = self.language_system.understand(command)\r\n\r\n            # Step 2: Context grounding (using current vision data)\r\n            grounded_result = self.language_system.ground_in_context(\r\n                language_result,\r\n                self.system_state['current_detections'],\r\n                self.system_state['current_image']\r\n            )\r\n\r\n            # Step 3: Action planning\r\n            action_plan = self.action_system.plan(grounded_result, self.system_state)\r\n\r\n            # Step 4: Action execution\r\n            execution_result = self.action_system.execute(action_plan)\r\n\r\n            # Log results\r\n            latency = time.time() - start_time\r\n            self.performance_metrics['language_latency'].append(latency)\r\n\r\n            success = execution_result.get('success', False)\r\n            self.performance_metrics['action_success_rate'].append(1.0 if success else 0.0)\r\n\r\n            self.get_logger().info(\r\n                f'Command processed: {command} | Success: {success} | '\r\n                f'Latency: {latency:.3f}s'\r\n            )\r\n\r\n            # Publish execution result\r\n            result_msg = String()\r\n            result_msg.data = str(execution_result)\r\n            self.action_publisher.publish(result_msg)\r\n\r\n            # Publish status\r\n            status_msg = String()\r\n            status_msg.data = f'Processed: {command[:50]}...' if len(command) > 50 else command\r\n            self.status_publisher.publish(status_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing command: {e}')\r\n\r\n    def get_system_status(self):\r\n        \"\"\"Get current system status\"\"\"\r\n        return {\r\n            'vision_fps': np.mean(self.performance_metrics['vision_fps']) if self.performance_metrics['vision_fps'] else 0,\r\n            'avg_language_latency': np.mean(self.performance_metrics['language_latency']) if self.performance_metrics['language_latency'] else 0,\r\n            'action_success_rate': np.mean(self.performance_metrics['action_success_rate']) if self.performance_metrics['action_success_rate'] else 0,\r\n            'command_queue_size': len(self.system_state['command_history']),\r\n            'current_detections_count': len(self.system_state['current_detections'])\r\n        }\r\n\r\nclass VisionSystem:\r\n    def __init__(self):\r\n        # Initialize vision models\r\n        # In a real system, you'd load pre-trained models here\r\n        pass\r\n\r\n    def process(self, image):\r\n        \"\"\"Process image through vision pipeline\"\"\"\r\n        # Object detection (simplified for this lab)\r\n        detections = self.detect_objects(image)\r\n\r\n        # Scene understanding\r\n        scene_context = self.understand_scene(image, detections)\r\n\r\n        return {\r\n            'detections': detections,\r\n            'scene_context': scene_context,\r\n            'timestamp': time.time()\r\n        }\r\n\r\n    def detect_objects(self, image):\r\n        \"\"\"Detect objects in image\"\"\"\r\n        # Use OpenCV for simple detection in this lab\r\n        # In real system, use YOLO, Detectron2, or similar\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n        # Simple shape detection\r\n        circles = cv2.HoughCircles(\r\n            gray, cv2.HOUGH_GRADIENT, 1, 20,\r\n            param1=50, param2=30, minRadius=10, maxRadius=100\r\n        )\r\n\r\n        detections = []\r\n        if circles is not None:\r\n            circles = np.round(circles[0, :]).astype(\"int\")\r\n            for (x, y, r) in circles:\r\n                detection = {\r\n                    'type': 'object',\r\n                    'class': 'circle',\r\n                    'center': [x, y],\r\n                    'radius': r,\r\n                    'bbox': [x-r, y-r, 2*r, 2*r],\r\n                    'confidence': 0.8\r\n                }\r\n                detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def understand_scene(self, image, detections):\r\n        \"\"\"Understand scene context\"\"\"\r\n        # Analyze spatial relationships between objects\r\n        relationships = []\r\n        for i, obj1 in enumerate(detections):\r\n            for j, obj2 in enumerate(detections[i+1:], i+1):\r\n                rel = self.compute_spatial_relationship(obj1, obj2)\r\n                relationships.append(rel)\r\n\r\n        return {\r\n            'object_relationships': relationships,\r\n            'scene_center': [image.shape[1]//2, image.shape[0]//2],\r\n            'dominant_colors': self.extract_dominant_colors(image)\r\n        }\r\n\r\n    def compute_spatial_relationship(self, obj1, obj2):\r\n        \"\"\"Compute spatial relationship between two objects\"\"\"\r\n        center1 = np.array(obj1['center'])\r\n        center2 = np.array(obj2['center'])\r\n\r\n        vector = center2 - center1\r\n        distance = np.linalg.norm(vector)\r\n\r\n        # Determine direction\r\n        angle = np.arctan2(vector[1], vector[0]) * 180 / np.pi\r\n\r\n        return {\r\n            'object1': obj1['class'],\r\n            'object2': obj2['class'],\r\n            'distance': distance,\r\n            'angle': angle,\r\n            'relationship': self.angle_to_direction(angle)\r\n        }\r\n\r\n    def angle_to_direction(self, angle):\r\n        \"\"\"Convert angle to directional string\"\"\"\r\n        if -45 <= angle < 45:\r\n            return 'right'\r\n        elif 45 <= angle < 135:\r\n            return 'down'\r\n        elif 135 <= angle < 225 or -225 <= angle < -135:\r\n            return 'left'\r\n        else:\r\n            return 'up'\r\n\r\n    def extract_dominant_colors(self, image):\r\n        \"\"\"Extract dominant colors from image\"\"\"\r\n        # Simple color extraction (in real system, use clustering)\r\n        avg_color = cv2.mean(image)[:3]\r\n        return [int(c) for c in avg_color]\r\n\r\nclass LanguageSystem:\r\n    def __init__(self):\r\n        # Initialize language models\r\n        # For this lab, we'll use rule-based parsing\r\n        # In real system, use transformers\r\n        self.patterns = {\r\n            'navigation': [\r\n                r'go to (?:the )?(?P<target>[\\w\\s]+?)(?:\\s|$)',\r\n                r'move to (?:the )?(?P<target>[\\w\\s]+?)(?:\\s|$)',\r\n                r'go (?:to )?(?P<target>[\\w\\s]+?)(?:\\s|$)'\r\n            ],\r\n            'manipulation': [\r\n                r'(?:grasp|pick up|take|grab) (?:the )?(?P<object>[\\w\\s]+?)(?:\\s|$)',\r\n                r'(?:place|put|set) (?:the )?(?P<object>[\\w\\s]+?)(?:\\s|$)'\r\n            ],\r\n            'action': [\r\n                r'(?:stop|wait|help|start|continue)(?:\\s|$)'\r\n            ]\r\n        }\r\n\r\n    def understand(self, command):\r\n        \"\"\"Understand natural language command\"\"\"\r\n        command_lower = command.lower().strip()\r\n\r\n        for action_type, patterns in self.patterns.items():\r\n            for pattern in patterns:\r\n                import re\r\n                match = re.search(pattern, command_lower)\r\n                if match:\r\n                    result = {\r\n                        'action_type': action_type,\r\n                        'command': command,\r\n                        'parameters': match.groupdict(),\r\n                        'confidence': 0.9\r\n                    }\r\n                    return result\r\n\r\n        # If no pattern matches, return as general command\r\n        return {\r\n            'action_type': 'general',\r\n            'command': command,\r\n            'parameters': {},\r\n            'confidence': 0.1\r\n        }\r\n\r\n    def ground_in_context(self, language_result, detections, image):\r\n        \"\"\"Ground language result in visual context\"\"\"\r\n        grounded_result = language_result.copy()\r\n        grounded_result['grounding'] = {}\r\n\r\n        if language_result['action_type'] == 'navigation':\r\n            # Ground navigation target in visual scene\r\n            target_name = language_result['parameters'].get('target', '')\r\n            if target_name:\r\n                grounded_location = self.find_location_in_scene(target_name, detections, image)\r\n                grounded_result['grounding']['target_location'] = grounded_location\r\n\r\n        elif language_result['action_type'] == 'manipulation':\r\n            # Ground manipulation object in visual scene\r\n            object_name = language_result['parameters'].get('object', '')\r\n            if object_name:\r\n                grounded_object = self.find_object_in_scene(object_name, detections, image)\r\n                grounded_result['grounding']['target_object'] = grounded_object\r\n\r\n        return grounded_result\r\n\r\n    def find_location_in_scene(self, location_name, detections, image):\r\n        \"\"\"Find location in visual scene\"\"\"\r\n        # In a real system, this would use semantic mapping\r\n        # For this lab, return image center as default location\r\n        return {\r\n            'name': location_name,\r\n            'position': [image.shape[1]//2, image.shape[0]//2],\r\n            'found': True\r\n        }\r\n\r\n    def find_object_in_scene(self, object_name, detections, image):\r\n        \"\"\"Find object in visual scene\"\"\"\r\n        # Match object name with detections\r\n        for detection in detections:\r\n            if object_name.lower() in detection['class'].lower():\r\n                return {\r\n                    'name': object_name,\r\n                    'detection': detection,\r\n                    'found': True\r\n                }\r\n\r\n        # If not found in detections, return as unknown\r\n        return {\r\n            'name': object_name,\r\n            'detection': None,\r\n            'found': False\r\n        }\r\n\r\nclass ActionSystem:\r\n    def __init__(self, node):\r\n        self.node = node\r\n        self.action_library = {\r\n            'navigate_to': self.execute_navigate_to,\r\n            'grasp_object': self.execute_grasp_object,\r\n            'place_object': self.execute_place_object,\r\n            'stop': self.execute_stop,\r\n            'general': self.execute_general\r\n        }\r\n\r\n    def plan(self, grounded_result, system_state):\r\n        \"\"\"Plan action sequence based on grounded result\"\"\"\r\n        action_type = grounded_result['action_type']\r\n\r\n        if action_type in self.action_library:\r\n            return self.create_action_plan(grounded_result, action_type, system_state)\r\n        else:\r\n            return self.create_fallback_plan(grounded_result, system_state)\r\n\r\n    def create_action_plan(self, grounded_result, action_type, system_state):\r\n        \"\"\"Create action plan for specific action type\"\"\"\r\n        plan = []\r\n\r\n        if action_type == 'navigation':\r\n            target_location = grounded_result['grounding'].get('target_location')\r\n            if target_location and target_location['found']:\r\n                plan.append({\r\n                    'action_type': 'navigate_to',\r\n                    'parameters': {\r\n                        'target_position': target_location['position'],\r\n                        'speed': 0.5\r\n                    }\r\n                })\r\n\r\n        elif action_type == 'manipulation':\r\n            target_object = grounded_result['grounding'].get('target_object')\r\n            if target_object and target_object['found']:\r\n                plan.append({\r\n                    'action_type': 'grasp_object',\r\n                    'parameters': {\r\n                        'object_center': target_object['detection']['center'],\r\n                        'approach_distance': 0.1\r\n                    }\r\n                })\r\n\r\n        elif action_type == 'action':\r\n            action_cmd = grounded_result['command'].split()[0].lower()\r\n            plan.append({\r\n                'action_type': action_cmd,\r\n                'parameters': {}\r\n            })\r\n\r\n        return plan\r\n\r\n    def create_fallback_plan(self, grounded_result, system_state):\r\n        \"\"\"Create fallback plan for unknown actions\"\"\"\r\n        return [{\r\n            'action_type': 'request_clarification',\r\n            'parameters': {\r\n                'message': f\"I'm not sure how to '{grounded_result['command']}'\"\r\n            }\r\n        }]\r\n\r\n    def execute(self, action_plan):\r\n        \"\"\"Execute action plan\"\"\"\r\n        results = []\r\n\r\n        for action in action_plan:\r\n            result = self.execute_single_action(action)\r\n            results.append(result)\r\n\r\n            # If action failed, stop execution\r\n            if not result.get('success', False):\r\n                break\r\n\r\n        return {\r\n            'success': all(r.get('success', False) for r in results),\r\n            'results': results,\r\n            'plan_executed': len(results)\r\n        }\r\n\r\n    def execute_single_action(self, action):\r\n        \"\"\"Execute a single action\"\"\"\r\n        action_type = action['action_type']\r\n\r\n        if action_type in self.action_library:\r\n            try:\r\n                return self.action_library[action_type](action['parameters'])\r\n            except Exception as e:\r\n                return {\r\n                    'success': False,\r\n                    'error': str(e),\r\n                    'action_type': action_type\r\n                }\r\n        else:\r\n            return {\r\n                'success': False,\r\n                'error': f'Unknown action type: {action_type}',\r\n                'action_type': action_type\r\n            }\r\n\r\n    def execute_navigate_to(self, parameters):\r\n        \"\"\"Execute navigation action\"\"\"\r\n        target_position = parameters.get('target_position', [0, 0])\r\n        speed = parameters.get('speed', 0.5)\r\n\r\n        # Simple navigation: move toward target\r\n        # In real system, use navigation stack\r\n        twist = Twist()\r\n        twist.linear.x = speed\r\n        twist.angular.z = 0.0  # No rotation for simplicity\r\n\r\n        # Publish command\r\n        self.node.cmd_vel_publisher.publish(twist)\r\n\r\n        # Simulate movement time\r\n        time.sleep(1.0)\r\n\r\n        # Stop\r\n        twist.linear.x = 0.0\r\n        self.node.cmd_vel_publisher.publish(twist)\r\n\r\n        return {\r\n            'success': True,\r\n            'action_type': 'navigate_to',\r\n            'target_position': target_position\r\n        }\r\n\r\n    def execute_grasp_object(self, parameters):\r\n        \"\"\"Execute grasp action\"\"\"\r\n        object_center = parameters.get('object_center', [0, 0])\r\n        approach_distance = parameters.get('approach_distance', 0.1)\r\n\r\n        # In real system, control manipulator\r\n        # For simulation, just return success\r\n        return {\r\n            'success': True,\r\n            'action_type': 'grasp_object',\r\n            'object_center': object_center\r\n        }\r\n\r\n    def execute_place_object(self, parameters):\r\n        \"\"\"Execute place action\"\"\"\r\n        # In real system, control manipulator\r\n        return {\r\n            'success': True,\r\n            'action_type': 'place_object'\r\n        }\r\n\r\n    def execute_stop(self, parameters):\r\n        \"\"\"Execute stop action\"\"\"\r\n        twist = Twist()\r\n        twist.linear.x = 0.0\r\n        twist.angular.z = 0.0\r\n        self.node.cmd_vel_publisher.publish(twist)\r\n\r\n        return {\r\n            'success': True,\r\n            'action_type': 'stop'\r\n        }\r\n\r\n    def execute_general(self, parameters):\r\n        \"\"\"Execute general action\"\"\"\r\n        return {\r\n            'success': True,\r\n            'action_type': 'general'\r\n        }\r\n\r\n    def execute_request_clarification(self, parameters):\r\n        \"\"\"Execute clarification request\"\"\"\r\n        message = parameters.get('message', 'Please clarify')\r\n        self.node.get_logger().info(f'Requesting clarification: {message}')\r\n\r\n        return {\r\n            'success': True,\r\n            'action_type': 'request_clarification',\r\n            'message': message\r\n        }\r\n\r\nclass VoiceSystem:\r\n    def __init__(self):\r\n        # Initialize voice recognition\r\n        # For this lab, we'll use a simple placeholder\r\n        pass\r\n\r\n    def transcribe(self, audio_data):\r\n        \"\"\"Transcribe audio to text\"\"\"\r\n        # In a real system, use speech recognition models\r\n        # For this lab, return a simple placeholder\r\n        # This is where you'd integrate with actual ASR\r\n        return self.simple_transcribe(audio_data)\r\n\r\n    def simple_transcribe(self, audio_data):\r\n        \"\"\"Simple transcription for lab purposes\"\"\"\r\n        # In real implementation, use proper ASR\r\n        # This is just a placeholder for the lab\r\n        return \"go to kitchen\"  # Placeholder command\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    vla_node = AdvancedVLANode()\r\n\r\n    try:\r\n        rclpy.spin(vla_node)\r\n    except KeyboardInterrupt:\r\n        vla_node.get_logger().info('Shutting down Advanced VLA node')\r\n    finally:\r\n        vla_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"creating-the-launch-file",children:"Creating the Launch File"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a launch file to start the complete VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- File: vla_system/launch/vla_system.launch.py --\x3e\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    use_sim_time_arg = DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='false',\r\n        description='Use simulation (Gazebo) clock if true'\r\n    )\r\n\r\n    # Get launch configurations\r\n    use_sim_time = LaunchConfiguration('use_sim_time')\r\n\r\n    # VLA integrated node\r\n    vla_node = Node(\r\n        package='vla_system',\r\n        executable='vla_advanced_node',\r\n        name='vla_integrated_node',\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time}\r\n        ],\r\n        remappings=[\r\n            ('camera/image_rect_color', '/camera/image_raw'),\r\n            ('camera/camera_info', '/camera/camera_info'),\r\n            ('cmd_vel', '/cmd_vel'),\r\n            ('joint_states', '/joint_states')\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    # Additional nodes for complete system\r\n    perception_node = Node(\r\n        package='vla_system',\r\n        executable='vla_perception_node',\r\n        name='vla_perception_node',\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time}\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    return LaunchDescription([\r\n        use_sim_time_arg,\r\n        vla_node,\r\n        perception_node\r\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"creating-test-scripts",children:"Creating Test Scripts"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a test script to validate the VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: vla_system/test/test_vla_system.py\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nimport time\r\nimport unittest\r\n\r\nclass VLAIntegrationTest(Node):\r\n    def __init__(self):\r\n        super().__init__(\'vla_integration_test\')\r\n\r\n        # Publishers\r\n        self.command_publisher = self.create_publisher(\r\n            String, \'natural_language_command\', 10\r\n        )\r\n        self.status_subscriber = self.create_subscription(\r\n            String, \'vla_status\', self.status_callback, 10\r\n        )\r\n\r\n        self.status_received = False\r\n        self.status_message = ""\r\n\r\n    def status_callback(self, msg):\r\n        """Handle status messages"""\r\n        self.status_received = True\r\n        self.status_message = msg.data\r\n\r\n    def test_basic_navigation_command(self):\r\n        """Test basic navigation command"""\r\n        self.status_received = False\r\n        self.status_message = ""\r\n\r\n        # Send navigation command\r\n        command_msg = String()\r\n        command_msg.data = "go to kitchen"\r\n        self.command_publisher.publish(command_msg)\r\n\r\n        # Wait for response\r\n        timeout = time.time() + 5.0  # 5 second timeout\r\n        while not self.status_received and time.time() < timeout:\r\n            rclpy.spin_once(self, timeout_sec=0.1)\r\n\r\n        self.assertTrue(self.status_received, "No status received for navigation command")\r\n        self.assertIn("Processed", self.status_message, "Command was not processed successfully")\r\n\r\n    def test_manipulation_command(self):\r\n        """Test basic manipulation command"""\r\n        self.status_received = False\r\n        self.status_message = ""\r\n\r\n        # Send manipulation command\r\n        command_msg = String()\r\n        command_msg.data = "grasp the red ball"\r\n        self.command_publisher.publish(command_msg)\r\n\r\n        # Wait for response\r\n        timeout = time.time() + 5.0  # 5 second timeout\r\n        while not self.status_received and time.time() < timeout:\r\n            rclpy.spin_once(self, timeout_sec=0.1)\r\n\r\n        self.assertTrue(self.status_received, "No status received for manipulation command")\r\n        self.assertIn("Processed", self.status_message, "Command was not processed successfully")\r\n\r\ndef main():\r\n    rclpy.init()\r\n\r\n    test_node = VLAIntegrationTest()\r\n\r\n    try:\r\n        print("Testing basic navigation command...")\r\n        test_node.test_basic_navigation_command()\r\n        print("Navigation test passed!")\r\n\r\n        print("Testing manipulation command...")\r\n        test_node.test_manipulation_command()\r\n        print("Manipulation test passed!")\r\n\r\n        print("All tests passed successfully!")\r\n\r\n    except Exception as e:\r\n        print(f"Test failed: {e}")\r\n\r\n    finally:\r\n        test_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-evaluation-script",children:"Performance Evaluation Script"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a script to evaluate the VLA system performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: vla_system/scripts/evaluate_vla_performance.py\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nimport time\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom collections import deque\r\n\r\nclass VLAPerformanceEvaluator(Node):\r\n    def __init__(self):\r\n        super().__init__(\'vla_performance_evaluator\')\r\n\r\n        # Publishers and subscribers\r\n        self.command_publisher = self.create_publisher(\r\n            String, \'natural_language_command\', 10\r\n        )\r\n        self.status_subscriber = self.create_subscription(\r\n            String, \'vla_status\', self.status_callback, 10\r\n        )\r\n\r\n        # Performance tracking\r\n        self.response_times = deque(maxlen=100)\r\n        self.success_count = 0\r\n        self.total_count = 0\r\n        self.status_messages = []\r\n\r\n    def status_callback(self, msg):\r\n        """Track status messages for performance evaluation"""\r\n        self.status_messages.append({\r\n            \'message\': msg.data,\r\n            \'timestamp\': time.time()\r\n        })\r\n\r\n    def evaluate_system_performance(self, test_commands):\r\n        """Evaluate VLA system performance"""\r\n        print("Starting VLA system performance evaluation...")\r\n\r\n        for i, command in enumerate(test_commands):\r\n            print(f"Testing command {i+1}: {command}")\r\n\r\n            # Record start time\r\n            start_time = time.time()\r\n\r\n            # Send command\r\n            command_msg = String()\r\n            command_msg.data = command\r\n            self.command_publisher.publish(command_msg)\r\n\r\n            # Wait for response\r\n            timeout = time.time() + 5.0  # 5 second timeout\r\n            response_received = False\r\n\r\n            while time.time() < timeout:\r\n                rclpy.spin_once(self, timeout_sec=0.1)\r\n                if self.status_messages and self.status_messages[-1][\'timestamp\'] >= start_time:\r\n                    response_received = True\r\n                    break\r\n\r\n            # Record response time\r\n            if response_received:\r\n                response_time = time.time() - start_time\r\n                self.response_times.append(response_time)\r\n                self.success_count += 1\r\n                print(f"  Response time: {response_time:.3f}s")\r\n            else:\r\n                print(f"  Timeout - no response")\r\n                self.response_times.append(5.0)  # Use timeout value\r\n\r\n            self.total_count += 1\r\n\r\n            # Small delay between commands\r\n            time.sleep(0.5)\r\n\r\n    def generate_performance_report(self):\r\n        """Generate performance evaluation report"""\r\n        print("\\n" + "="*50)\r\n        print("VLA SYSTEM PERFORMANCE REPORT")\r\n        print("="*50)\r\n\r\n        if self.response_times:\r\n            avg_response_time = np.mean(self.response_times)\r\n            std_response_time = np.std(self.response_times)\r\n            min_response_time = np.min(self.response_times)\r\n            max_response_time = np.max(self.response_times)\r\n\r\n            success_rate = self.success_count / self.total_count if self.total_count > 0 else 0\r\n\r\n            print(f"Average Response Time: {avg_response_time:.3f}s \xb1 {std_response_time:.3f}s")\r\n            print(f"Min Response Time: {min_response_time:.3f}s")\r\n            print(f"Max Response Time: {max_response_time:.3f}s")\r\n            print(f"Success Rate: {success_rate:.2%} ({self.success_count}/{self.total_count})")\r\n\r\n            # Plot response time distribution\r\n            if len(self.response_times) > 1:\r\n                plt.figure(figsize=(10, 6))\r\n                plt.hist(self.response_times, bins=20, alpha=0.7, edgecolor=\'black\')\r\n                plt.title(\'Distribution of VLA System Response Times\')\r\n                plt.xlabel(\'Response Time (seconds)\')\r\n                plt.ylabel(\'Frequency\')\r\n                plt.grid(True, alpha=0.3)\r\n                plt.show()\r\n\r\n        print("="*50)\r\n\r\ndef main():\r\n    rclpy.init()\r\n\r\n    evaluator = VLAPerformanceEvaluator()\r\n\r\n    # Define test commands\r\n    test_commands = [\r\n        "go to kitchen",\r\n        "move to table",\r\n        "grasp the red ball",\r\n        "place object on shelf",\r\n        "find the cup",\r\n        "stop movement",\r\n        "navigate to bedroom",\r\n        "pick up box",\r\n        "go forward",\r\n        "turn left"\r\n    ]\r\n\r\n    try:\r\n        # Evaluate performance\r\n        evaluator.evaluate_system_performance(test_commands)\r\n\r\n        # Generate report\r\n        evaluator.generate_performance_report()\r\n\r\n    except KeyboardInterrupt:\r\n        print("Evaluation interrupted by user")\r\n\r\n    finally:\r\n        evaluator.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"running-the-complete-vla-system",children:"Running the Complete VLA System"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a complete run script:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\r\n# File: vla_system/run_vla_system.sh\r\n\r\n# Script to run the complete VLA system\r\n\r\necho "Starting VLA System Integration..."\r\n\r\n# Source ROS 2 environment\r\nsource /opt/ros/humble/setup.bash\r\n\r\n# Source workspace\r\ncd ~/vla_ws\r\nsource install/setup.bash\r\n\r\n# Build the workspace if not already built\r\ncolcon build --packages-select vla_system\r\n\r\n# Source again after build\r\nsource install/setup.bash\r\n\r\necho "Starting VLA system nodes..."\r\n\r\n# Run the VLA system\r\nros2 launch vla_system vla_system.launch.py\r\n\r\necho "VLA System finished."\n'})}),"\n",(0,s.jsx)(n.h2,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a comprehensive validation script:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: vla_system/scripts/validate_vla_system.py\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nimport time\r\nimport json\r\n\r\nclass VLAValidator(Node):\r\n    def __init__(self):\r\n        super().__init__(\'vla_validator\')\r\n\r\n        # Publishers and subscribers\r\n        self.command_publisher = self.create_publisher(\r\n            String, \'natural_language_command\', 10\r\n        )\r\n        self.status_subscriber = self.create_subscription(\r\n            String, \'vla_status\', self.status_callback, 10\r\n        )\r\n        self.action_subscriber = self.create_subscription(\r\n            String, \'executed_action\', self.action_callback, 10\r\n        )\r\n\r\n        self.status_received = False\r\n        self.action_received = False\r\n        self.status_message = ""\r\n        self.action_message = ""\r\n\r\n    def status_callback(self, msg):\r\n        """Handle status messages"""\r\n        self.status_received = True\r\n        self.status_message = msg.data\r\n\r\n    def action_callback(self, msg):\r\n        """Handle action messages"""\r\n        self.action_received = True\r\n        self.action_message = msg.data\r\n\r\n    def validate_vla_integration(self):\r\n        """Validate complete VLA system integration"""\r\n        print("Validating VLA System Integration...")\r\n\r\n        # Test 1: Vision-Language connection\r\n        print("\\n1. Testing Vision-Language Connection...")\r\n        if self.test_vision_language_connection():\r\n            print("   \u2713 Vision-Language connection working")\r\n        else:\r\n            print("   \u2717 Vision-Language connection failed")\r\n\r\n        # Test 2: Language-Action connection\r\n        print("\\n2. Testing Language-Action Connection...")\r\n        if self.test_language_action_connection():\r\n            print("   \u2713 Language-Action connection working")\r\n        else:\r\n            print("   \u2717 Language-Action connection failed")\r\n\r\n        # Test 3: Complete VLA pipeline\r\n        print("\\n3. Testing Complete VLA Pipeline...")\r\n        if self.test_complete_pipeline():\r\n            print("   \u2713 Complete VLA pipeline working")\r\n        else:\r\n            print("   \u2717 Complete VLA pipeline failed")\r\n\r\n        # Test 4: Context grounding\r\n        print("\\n4. Testing Context Grounding...")\r\n        if self.test_context_grounding():\r\n            print("   \u2713 Context grounding working")\r\n        else:\r\n            print("   \u2717 Context grounding failed")\r\n\r\n        print("\\nValidation complete!")\r\n\r\n    def test_vision_language_connection(self):\r\n        """Test vision-language connection"""\r\n        # This would require a simulated environment with known objects\r\n        # For this lab, we\'ll simulate the test\r\n        return True\r\n\r\n    def test_language_action_connection(self):\r\n        """Test language-action connection"""\r\n        self.reset_flags()\r\n\r\n        command_msg = String()\r\n        command_msg.data = "stop"\r\n        self.command_publisher.publish(command_msg)\r\n\r\n        # Wait for response\r\n        timeout = time.time() + 3.0\r\n        while time.time() < timeout:\r\n            rclpy.spin_once(self, timeout_sec=0.1)\r\n            if self.action_received:\r\n                try:\r\n                    action_data = json.loads(self.action_message)\r\n                    if action_data.get(\'success\', False):\r\n                        return True\r\n                except:\r\n                    pass\r\n\r\n        return False\r\n\r\n    def test_complete_pipeline(self):\r\n        """Test complete VLA pipeline"""\r\n        self.reset_flags()\r\n\r\n        command_msg = String()\r\n        command_msg.data = "go to kitchen"\r\n        self.command_publisher.publish(command_msg)\r\n\r\n        # Wait for both status and action\r\n        timeout = time.time() + 5.0\r\n        while time.time() < timeout:\r\n            rclpy.spin_once(self, timeout_sec=0.1)\r\n            if self.status_received and self.action_received:\r\n                return True\r\n\r\n        return False\r\n\r\n    def test_context_grounding(self):\r\n        """Test context grounding"""\r\n        # This would test if commands are properly grounded in visual context\r\n        # For this lab, we\'ll simulate the test\r\n        return True\r\n\r\n    def reset_flags(self):\r\n        """Reset flags for new test"""\r\n        self.status_received = False\r\n        self.action_received = False\r\n        self.status_message = ""\r\n        self.action_message = ""\r\n\r\ndef main():\r\n    rclpy.init()\r\n\r\n    validator = VLAValidator()\r\n\r\n    try:\r\n        validator.validate_vla_integration()\r\n    except KeyboardInterrupt:\r\n        print("Validation interrupted by user")\r\n    finally:\r\n        validator.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"summary-and-next-steps",children:"Summary and Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"This lab has provided hands-on experience with implementing a complete Vision-Language-Action system. You have learned to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integrate multimodal components"})," - Connecting vision, language, and action systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement real-time processing"})," - Handling data streams from multiple sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deploy complete systems"})," - Building and running integrated robotic systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Evaluate system performance"})," - Measuring accuracy and response times"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Debug complex systems"})," - Identifying and resolving integration issues"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multimodal Integration"}),": Successfully connecting different modalities requires careful attention to data synchronization and format compatibility"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": VLA systems must process data quickly enough to maintain responsive interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Grounding"}),": Language commands must be properly grounded in visual and spatial context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System Robustness"}),": Real-world systems need fallback strategies and error handling"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"further-enhancements",children:"Further Enhancements"}),"\n",(0,s.jsx)(n.p,{children:"Consider these improvements for your VLA system:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Optimization"}),": Implement quantization and optimization for edge deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advanced Grounding"}),": Implement more sophisticated spatial and semantic grounding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-turn Dialogue"}),": Enable conversational interaction with context maintenance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning from Interaction"}),": Implement systems that improve through experience"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsx)(n.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}}}]);