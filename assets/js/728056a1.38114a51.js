"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[1043],{6511(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/voice-command-system","title":"Voice Command Interpretation System","description":"Learning Objectives","source":"@site/docs/module-4-vla/voice-command-system.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-command-system","permalink":"/Book-ai-native/docs/module-4-vla/voice-command-system","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/module-4-vla/voice-command-system.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Action Grounding and Execution","permalink":"/Book-ai-native/docs/module-4-vla/action-grounding"},"next":{"title":"Natural Language to Robot Action Mapping","permalink":"/Book-ai-native/docs/module-4-vla/nlp-robot-mapping"}}');var o=r(4848),i=r(8453);const s={sidebar_position:6},a="Voice Command Interpretation System",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Voice Command Systems",id:"introduction-to-voice-command-systems",level:2},{value:"Speech Recognition Pipeline",id:"speech-recognition-pipeline",level:2},{value:"Audio Preprocessing",id:"audio-preprocessing",level:3},{value:"Automatic Speech Recognition (ASR)",id:"automatic-speech-recognition-asr",level:3},{value:"Natural Language Understanding for Voice",id:"natural-language-understanding-for-voice",level:2},{value:"Voice-Specific NLU Challenges",id:"voice-specific-nlu-challenges",level:3},{value:"Context-Aware Voice Processing",id:"context-aware-voice-processing",level:3},{value:"Real-Time Voice Command Processing",id:"real-time-voice-command-processing",level:2},{value:"Streaming Voice Recognition",id:"streaming-voice-recognition",level:3},{value:"Multimodal Voice Interface",id:"multimodal-voice-interface",level:2},{value:"Combining Voice with Visual and Gesture Inputs",id:"combining-voice-with-visual-and-gesture-inputs",level:3},{value:"Isaac Integration for Voice Commands",id:"isaac-integration-for-voice-commands",level:2},{value:"ROS 2 Interface for Voice Command Processing",id:"ros-2-interface-for-voice-command-processing",level:3},{value:"Evaluation and Validation",id:"evaluation-and-validation",level:2},{value:"Voice Command System Evaluation",id:"voice-command-system-evaluation",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-command-interpretation-system",children:"Voice Command Interpretation System"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Design and implement speech recognition systems for robotic applications"}),"\n",(0,o.jsx)(n.li,{children:"Create robust voice command processing pipelines that handle environmental noise"}),"\n",(0,o.jsx)(n.li,{children:"Develop natural language understanding modules specifically for voice input"}),"\n",(0,o.jsx)(n.li,{children:"Implement real-time voice command interpretation with low latency"}),"\n",(0,o.jsx)(n.li,{children:"Build multimodal interfaces that combine voice with visual and gesture inputs"}),"\n",(0,o.jsx)(n.li,{children:"Create context-aware voice command interpretation that considers environmental constraints"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-voice-command-systems",children:"Introduction to Voice Command Systems"}),"\n",(0,o.jsx)(n.p,{children:"Voice command interpretation systems enable natural human-robot interaction by allowing users to communicate with robots using spoken language. Unlike text-based interfaces, voice systems must handle the challenges of real-time audio processing, acoustic variability, and the natural ambiguities of spoken language."}),"\n",(0,o.jsx)(n.p,{children:"Voice command systems in robotics face unique challenges:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environmental Noise"}),": Robots operate in noisy environments that can interfere with speech recognition"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Processing"}),": Commands must be processed with minimal latency for natural interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Domain Specificity"}),": Robot commands often use domain-specific terminology and spatial references"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Systems must handle variations in accent, speaking rate, and background conditions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Privacy"}),": Voice data may contain sensitive information requiring privacy considerations"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"speech-recognition-pipeline",children:"Speech Recognition Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"audio-preprocessing",children:"Audio Preprocessing"}),"\n",(0,o.jsx)(n.p,{children:"Effective voice command systems begin with robust audio preprocessing:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport scipy.signal as signal\r\nimport webrtcvad\r\nfrom scipy.io import wavfile\r\n\r\nclass AudioPreprocessor:\r\n    def __init__(self, sample_rate=16000, frame_duration=30):\r\n        self.sample_rate = sample_rate\r\n        self.frame_duration = frame_duration  # in ms\r\n        self.frame_size = int(sample_rate * frame_duration / 1000)\r\n\r\n        # Voice Activity Detection\r\n        self.vad = webrtcvad.Vad(2)  # Aggressiveness mode 2\r\n\r\n        # Noise reduction parameters\r\n        self.noise_threshold = 0.01\r\n        self.speech_threshold = 0.1\r\n\r\n    def preprocess_audio(self, audio_data):\r\n        """Preprocess audio for speech recognition"""\r\n        # Apply pre-emphasis filter\r\n        pre_emphasis = 0.97\r\n        audio_data = np.append(audio_data[0], audio_data[1:] - pre_emphasis * audio_data[:-1])\r\n\r\n        # Normalize audio\r\n        audio_data = audio_data / np.max(np.abs(audio_data))\r\n\r\n        # Apply noise reduction\r\n        audio_data = self.apply_noise_reduction(audio_data)\r\n\r\n        return audio_data\r\n\r\n    def apply_noise_reduction(self, audio_data):\r\n        """Apply basic noise reduction"""\r\n        # Simple spectral subtraction approach\r\n        fft_data = np.fft.fft(audio_data)\r\n        magnitude = np.abs(fft_data)\r\n\r\n        # Estimate noise floor\r\n        noise_floor = np.mean(magnitude) * 0.1\r\n\r\n        # Subtract noise\r\n        enhanced_magnitude = np.maximum(magnitude - noise_floor, 0)\r\n\r\n        # Reconstruct signal\r\n        phase = np.angle(fft_data)\r\n        enhanced_fft = enhanced_magnitude * np.exp(1j * phase)\r\n        enhanced_audio = np.real(np.fft.ifft(enhanced_fft))\r\n\r\n        return enhanced_audio.astype(np.float32)\r\n\r\n    def detect_voice_activity(self, audio_data):\r\n        """Detect voice activity in audio"""\r\n        # Convert to 16-bit for VAD\r\n        audio_16bit = (audio_data * 32767).astype(np.int16)\r\n\r\n        # Split into frames\r\n        frames = self.frame_audio(audio_16bit)\r\n\r\n        # Detect voice activity in each frame\r\n        vad_results = []\r\n        for frame in frames:\r\n            if len(frame) == self.frame_size:\r\n                is_speech = self.vad.is_speech(frame.tobytes(), self.sample_rate)\r\n                vad_results.append(is_speech)\r\n\r\n        return vad_results\r\n\r\n    def frame_audio(self, audio_data):\r\n        """Split audio into frames for VAD"""\r\n        frames = []\r\n        for i in range(0, len(audio_data), self.frame_size):\r\n            frame = audio_data[i:i + self.frame_size]\r\n            if len(frame) < self.frame_size:\r\n                # Pad with zeros if frame is too short\r\n                frame = np.pad(frame, (0, self.frame_size - len(frame)))\r\n            frames.append(frame)\r\n        return frames\r\n\r\n    def extract_features(self, audio_data):\r\n        """Extract features for speech recognition"""\r\n        # Compute MFCC features\r\n        from python_speech_features import mfcc\r\n        features = mfcc(\r\n            audio_data,\r\n            samplerate=self.sample_rate,\r\n            winlen=0.025,\r\n            winstep=0.01,\r\n            numcep=13,\r\n            nfilt=26,\r\n            nfft=512,\r\n            lowfreq=0,\r\n            highfreq=None,\r\n            preemph=0.97,\r\n            ceplifter=22,\r\n            appendEnergy=True\r\n        )\r\n        return features\n'})}),"\n",(0,o.jsx)(n.h3,{id:"automatic-speech-recognition-asr",children:"Automatic Speech Recognition (ASR)"}),"\n",(0,o.jsx)(n.p,{children:"The ASR component converts audio to text:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torchaudio\r\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\r\n\r\nclass VoiceCommandASR:\r\n    def __init__(self, model_name=\"facebook/wav2vec2-large-960h-lv60-self\"):\r\n        # Load pre-trained model and processor\r\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\r\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\r\n\r\n        # Move to GPU if available\r\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n        self.model = self.model.to(self.device)\r\n        self.model.eval()\r\n\r\n        # Robot-specific vocabulary and commands\r\n        self.robot_commands = {\r\n            'navigation': ['go', 'move', 'navigate', 'walk', 'drive', 'travel', 'go to'],\r\n            'manipulation': ['grasp', 'pick', 'grab', 'take', 'place', 'put', 'drop', 'lift'],\r\n            'interaction': ['hello', 'hi', 'stop', 'wait', 'help', 'please', 'thank you'],\r\n            'locations': ['kitchen', 'living room', 'bedroom', 'office', 'bathroom', 'table', 'shelf']\r\n        }\r\n\r\n    def transcribe_audio(self, audio_data, sample_rate=16000):\r\n        \"\"\"Transcribe audio to text\"\"\"\r\n        # Resample if necessary\r\n        if sample_rate != 16000:\r\n            audio_data = torchaudio.functional.resample(\r\n                torch.tensor(audio_data), sample_rate, 16000\r\n            ).numpy()\r\n\r\n        # Process audio\r\n        input_values = self.processor(\r\n            audio_data,\r\n            sampling_rate=16000,\r\n            return_tensors=\"pt\"\r\n        ).input_values\r\n\r\n        # Move to device\r\n        input_values = input_values.to(self.device)\r\n\r\n        # Inference\r\n        with torch.no_grad():\r\n            logits = self.model(input_values).logits\r\n\r\n        # Decode\r\n        predicted_ids = torch.argmax(logits, dim=-1)\r\n        transcription = self.processor.batch_decode(predicted_ids)[0]\r\n\r\n        return transcription\r\n\r\n    def transcribe_with_confidence(self, audio_data, sample_rate=16000):\r\n        \"\"\"Transcribe audio with confidence scoring\"\"\"\r\n        # Get transcription\r\n        transcription = self.transcribe_audio(audio_data, sample_rate)\r\n\r\n        # Compute confidence based on robot command vocabulary\r\n        confidence = self.compute_command_confidence(transcription)\r\n\r\n        return {\r\n            'transcription': transcription,\r\n            'confidence': confidence,\r\n            'is_robot_command': self.is_robot_command(transcription)\r\n        }\r\n\r\n    def compute_command_confidence(self, transcription):\r\n        \"\"\"Compute confidence that transcription is a robot command\"\"\"\r\n        words = transcription.lower().split()\r\n        command_matches = 0\r\n        total_words = len(words)\r\n\r\n        for word in words:\r\n            for category, commands in self.robot_commands.items():\r\n                if word in [cmd.split()[0] if ' ' in cmd else cmd for cmd in commands]:\r\n                    command_matches += 1\r\n                    break\r\n\r\n        return command_matches / total_words if total_words > 0 else 0.0\r\n\r\n    def is_robot_command(self, transcription):\r\n        \"\"\"Check if transcription likely contains robot command\"\"\"\r\n        confidence = self.compute_command_confidence(transcription)\r\n        return confidence > 0.3  # Threshold for considering it a robot command\r\n\r\n    def continuous_transcription(self, audio_stream_callback, callback_rate=10):\r\n        \"\"\"Perform continuous transcription from audio stream\"\"\"\r\n        import threading\r\n        import queue\r\n\r\n        audio_queue = queue.Queue()\r\n\r\n        def audio_capture():\r\n            \"\"\"Capture audio in separate thread\"\"\"\r\n            while True:\r\n                audio_chunk = audio_stream_callback()\r\n                audio_queue.put(audio_chunk)\r\n\r\n        def transcription_worker():\r\n            \"\"\"Process audio chunks for transcription\"\"\"\r\n            while True:\r\n                try:\r\n                    audio_chunk = audio_queue.get(timeout=1.0)\r\n                    result = self.transcribe_with_confidence(audio_chunk)\r\n                    if result['is_robot_command'] and result['confidence'] > 0.5:\r\n                        yield result\r\n                except queue.Empty:\r\n                    continue\r\n\r\n        # Start audio capture thread\r\n        capture_thread = threading.Thread(target=audio_capture)\r\n        capture_thread.daemon = True\r\n        capture_thread.start()\r\n\r\n        return transcription_worker()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"natural-language-understanding-for-voice",children:"Natural Language Understanding for Voice"}),"\n",(0,o.jsx)(n.h3,{id:"voice-specific-nlu-challenges",children:"Voice-Specific NLU Challenges"}),"\n",(0,o.jsx)(n.p,{children:"Voice input introduces specific challenges for natural language understanding:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import re\r\nfrom typing import Dict, List, Tuple\r\n\r\nclass VoiceNLU:\r\n    def __init__(self):\r\n        self.command_patterns = self.initialize_command_patterns()\r\n        self.spatial_resolvers = SpatialReferenceResolver()\r\n        self.context_handler = ContextHandler()\r\n\r\n    def initialize_command_patterns(self):\r\n        \"\"\"Initialize patterns for common robot commands\"\"\"\r\n        return {\r\n            # Navigation patterns\r\n            'navigate': [\r\n                r'go to (?:the )?(?P<location>\\w+)',\r\n                r'move to (?:the )?(?P<location>\\w+)',\r\n                r'go (?:to )?(?P<location>\\w+)',\r\n                r'go (?:to )?(?P<location>[\\w\\s]+?)(?:\\s|$)'\r\n            ],\r\n            # Manipulation patterns\r\n            'grasp': [\r\n                r'(?:grasp|pick up|take|grab) (?:the )?(?P<object>\\w+)',\r\n                r'(?:grasp|pick up|take|grab) (?:the )?(?P<object>[\\w\\s]+?)(?:\\s|$)'\r\n            ],\r\n            'place': [\r\n                r'place (?:the )?(?P<object>\\w+) (?:on|at|in) (?:the )?(?P<location>\\w+)',\r\n                r'put (?:the )?(?P<object>\\w+) (?:on|at|in) (?:the )?(?P<location>\\w+)'\r\n            ],\r\n            # Action patterns\r\n            'stop': [r'(?:please )?stop', r'halt', r'wait'],\r\n            'help': [r'help', r'can you help', r'i need help']\r\n        }\r\n\r\n    def parse_voice_command(self, transcription: str, context: Dict) -> Dict:\r\n        \"\"\"Parse voice command and extract structured information\"\"\"\r\n        # Clean transcription\r\n        cleaned_transcription = self.clean_transcription(transcription)\r\n\r\n        # Apply pattern matching\r\n        command_structure = self.match_command_patterns(cleaned_transcription)\r\n\r\n        # Resolve spatial references using context\r\n        if 'location' in command_structure:\r\n            command_structure['resolved_location'] = self.spatial_resolvers.resolve(\r\n                command_structure['location'], context\r\n            )\r\n\r\n        # Disambiguate objects using context\r\n        if 'object' in command_structure:\r\n            command_structure['resolved_object'] = self.resolve_object_reference(\r\n                command_structure['object'], context\r\n            )\r\n\r\n        return command_structure\r\n\r\n    def clean_transcription(self, transcription: str) -> str:\r\n        \"\"\"Clean transcription from ASR artifacts\"\"\"\r\n        # Remove common ASR errors and artifacts\r\n        transcription = transcription.lower().strip()\r\n\r\n        # Fix common misrecognitions\r\n        fixes = {\r\n            'wexler': 'vexler',  # Example: ASR might misrecognize \"v\" as \"w\"\r\n            'fexler': 'vexler',  # Example: ASR might misrecognize \"v\" as \"f\"\r\n        }\r\n\r\n        for wrong, right in fixes.items():\r\n            transcription = transcription.replace(wrong, right)\r\n\r\n        # Remove filler words and hesitations\r\n        fillers = ['um', 'uh', 'er', 'ah', 'like', 'you know', 'so']\r\n        for filler in fillers:\r\n            transcription = re.sub(r'\\b' + filler + r'\\b', '', transcription)\r\n\r\n        # Remove extra whitespace\r\n        transcription = ' '.join(transcription.split())\r\n\r\n        return transcription\r\n\r\n    def match_command_patterns(self, transcription: str) -> Dict:\r\n        \"\"\"Match transcription against command patterns\"\"\"\r\n        for command_type, patterns in self.command_patterns.items():\r\n            for pattern in patterns:\r\n                match = re.search(pattern, transcription, re.IGNORECASE)\r\n                if match:\r\n                    result = {'command_type': command_type}\r\n                    result.update(match.groupdict())\r\n                    return result\r\n\r\n        # If no pattern matches, return as general command\r\n        return {\r\n            'command_type': 'general',\r\n            'raw_command': transcription\r\n        }\r\n\r\n    def resolve_object_reference(self, object_ref: str, context: Dict) -> Dict:\r\n        \"\"\"Resolve object reference in context\"\"\"\r\n        # Find object in current scene\r\n        if 'objects' in context:\r\n            for obj in context['objects']:\r\n                if self.object_matches_reference(obj, object_ref):\r\n                    return obj\r\n\r\n        # If not found, return reference as is\r\n        return {'type': object_ref, 'found': False}\r\n\r\n    def object_matches_reference(self, obj: Dict, reference: str) -> bool:\r\n        \"\"\"Check if object matches reference string\"\"\"\r\n        obj_name = obj.get('type', '').lower()\r\n        obj_color = obj.get('color', '').lower()\r\n        obj_size = obj.get('size', '').lower()\r\n\r\n        ref_parts = reference.lower().split()\r\n\r\n        # Check if all reference parts match object properties\r\n        matches = True\r\n        for part in ref_parts:\r\n            if part not in obj_name and part not in obj_color and part not in obj_size:\r\n                matches = False\r\n                break\r\n\r\n        return matches\r\n\r\n    def handle_voice_specific_ambiguities(self, transcription: str, context: Dict) -> Dict:\r\n        \"\"\"Handle ambiguities specific to voice input\"\"\"\r\n        # Handle homophones and similar-sounding words\r\n        homophones = {\r\n            'to': ['too', 'two'],\r\n            'for': ['four'],\r\n            'hear': ['here'],\r\n            'there': ['their', 'they\\'re'],\r\n            'see': ['sea'],\r\n            'right': ['write', 'rite'],\r\n            'kitchen': ['chicken'],  # Common misrecognition\r\n            'grasp': ['grass', 'grasp']  # Common misrecognition\r\n        }\r\n\r\n        words = transcription.split()\r\n        corrected_words = []\r\n\r\n        for word in words:\r\n            corrected_word = word\r\n            for correct, similar_list in homophones.items():\r\n                if word.lower() in similar_list:\r\n                    # Use context to disambiguate\r\n                    corrected_word = self.disambiguate_homophone(\r\n                        correct, similar_list, word, context\r\n                    )\r\n                    break\r\n            corrected_words.append(corrected_word)\r\n\r\n        corrected_transcription = ' '.join(corrected_words)\r\n        return self.parse_voice_command(corrected_transcription, context)\r\n\r\n    def disambiguate_homophone(self, correct: str, similar_list: List[str],\r\n                              actual: str, context: Dict) -> str:\r\n        \"\"\"Disambiguate homophones using context\"\"\"\r\n        # Simple context-based disambiguation\r\n        # In practice, this would use more sophisticated NLP\r\n        if correct == 'kitchen' and actual.lower() in ['chicken']:\r\n            # If context mentions cooking/food, likely 'chicken'\r\n            # If context mentions navigation, likely 'kitchen'\r\n            if any(word in context.get('recent_utterances', []) for word in ['go', 'navigate', 'move']):\r\n                return 'kitchen'\r\n\r\n        return correct\n"})}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-voice-processing",children:"Context-Aware Voice Processing"}),"\n",(0,o.jsx)(n.p,{children:"Voice commands must be interpreted in the context of the current situation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ContextHandler:\r\n    def __init__(self):\r\n        self.context_history = []\r\n        self.max_context_length = 10\r\n\r\n    def update_context(self, new_context: Dict):\r\n        \"\"\"Update context with new information\"\"\"\r\n        self.context_history.append(new_context)\r\n\r\n        # Maintain history size\r\n        if len(self.context_history) > self.max_context_length:\r\n            self.context_history.pop(0)\r\n\r\n    def get_current_context(self) -> Dict:\r\n        \"\"\"Get current context for interpretation\"\"\"\r\n        if not self.context_history:\r\n            return {}\r\n\r\n        # Merge recent contexts\r\n        current_context = {}\r\n        for ctx in self.context_history[-3:]:  # Use last 3 contexts\r\n            current_context.update(ctx)\r\n\r\n        return current_context\r\n\r\n    def resolve_pronouns(self, transcription: str, context: Dict) -> str:\r\n        \"\"\"Resolve pronouns in transcription using context\"\"\"\r\n        # Replace pronouns with specific references from context\r\n        words = transcription.split()\r\n        resolved_words = []\r\n\r\n        for word in words:\r\n            if word.lower() in ['it', 'that', 'this', 'them', 'those']:\r\n                # Resolve pronoun using context\r\n                resolved = self.resolve_pronoun(word.lower(), context)\r\n                resolved_words.append(resolved if resolved else word)\r\n            else:\r\n                resolved_words.append(word)\r\n\r\n        return ' '.join(resolved_words)\r\n\r\n    def resolve_pronoun(self, pronoun: str, context: Dict) -> str:\r\n        \"\"\"Resolve specific pronoun using context\"\"\"\r\n        if pronoun in ['it', 'that', 'this']:\r\n            # Look for most recently mentioned object\r\n            recent_objects = self.get_recent_objects(context)\r\n            if recent_objects:\r\n                return recent_objects[-1].get('type', 'object')\r\n\r\n        elif pronoun in ['them', 'those']:\r\n            # Look for multiple objects\r\n            recent_objects = self.get_recent_objects(context)\r\n            if len(recent_objects) > 1:\r\n                return ' '.join([obj.get('type', 'object') for obj in recent_objects])\r\n\r\n        return None\r\n\r\n    def get_recent_objects(self, context: Dict) -> List[Dict]:\r\n        \"\"\"Get recently mentioned objects from context\"\"\"\r\n        recent_objects = []\r\n\r\n        # Check current context\r\n        if 'objects' in context:\r\n            recent_objects.extend(context['objects'])\r\n\r\n        # Check recent utterances\r\n        if 'recent_utterances' in context:\r\n            for utterance in context['recent_utterances'][-3:]:\r\n                objects_in_utterance = self.extract_objects_from_utterance(utterance)\r\n                recent_objects.extend(objects_in_utterance)\r\n\r\n        return recent_objects\r\n\r\n    def extract_objects_from_utterance(self, utterance: str) -> List[Dict]:\r\n        \"\"\"Extract objects mentioned in utterance\"\"\"\r\n        # Simple extraction (in practice, use NLP)\r\n        common_objects = ['ball', 'cup', 'box', 'book', 'table', 'chair', 'robot']\r\n        found_objects = []\r\n\r\n        for obj in common_objects:\r\n            if obj in utterance.lower():\r\n                found_objects.append({'type': obj, 'confidence': 0.8})\r\n\r\n        return found_objects\r\n\r\nclass SpatialReferenceResolver:\r\n    def __init__(self):\r\n        self.spatial_keywords = {\r\n            'relative': ['left', 'right', 'front', 'back', 'near', 'far', 'close', 'next to'],\r\n            'absolute': ['kitchen', 'living room', 'bedroom', 'office', 'table', 'shelf', 'cabinet']\r\n        }\r\n\r\n    def resolve(self, location_ref: str, context: Dict) -> Dict:\r\n        \"\"\"Resolve spatial reference in context\"\"\"\r\n        location_ref = location_ref.lower()\r\n\r\n        # Check if it's an absolute location\r\n        if self.is_absolute_location(location_ref):\r\n            return self.resolve_absolute_location(location_ref, context)\r\n\r\n        # Check if it's a relative location\r\n        elif self.is_relative_location(location_ref):\r\n            return self.resolve_relative_location(location_ref, context)\r\n\r\n        # Default: return as is\r\n        return {'reference': location_ref, 'resolved': False}\r\n\r\n    def is_absolute_location(self, location_ref: str) -> bool:\r\n        \"\"\"Check if location reference is absolute\"\"\"\r\n        absolute_locations = self.spatial_keywords['absolute']\r\n        return any(loc in location_ref for loc in absolute_locations)\r\n\r\n    def is_relative_location(self, location_ref: str) -> bool:\r\n        \"\"\"Check if location reference is relative\"\"\"\r\n        relative_keywords = self.spatial_keywords['relative']\r\n        return any(keyword in location_ref for keyword in relative_keywords)\r\n\r\n    def resolve_absolute_location(self, location_ref: str, context: Dict) -> Dict:\r\n        \"\"\"Resolve absolute location reference\"\"\"\r\n        # Look up in context's known locations\r\n        known_locations = context.get('named_locations', {})\r\n\r\n        for name, pose in known_locations.items():\r\n            if location_ref in name.lower():\r\n                return {\r\n                    'reference': location_ref,\r\n                    'resolved': True,\r\n                    'type': 'absolute',\r\n                    'pose': pose\r\n                }\r\n\r\n        return {'reference': location_ref, 'resolved': False, 'type': 'absolute'}\r\n\r\n    def resolve_relative_location(self, location_ref: str, context: Dict) -> Dict:\r\n        \"\"\"Resolve relative location reference\"\"\"\r\n        # Parse relative location (e.g., \"to the left of the table\")\r\n        words = location_ref.split()\r\n\r\n        # Find spatial relation and reference object\r\n        spatial_relation = None\r\n        reference_object = None\r\n\r\n        for i, word in enumerate(words):\r\n            if word in self.spatial_keywords['relative']:\r\n                spatial_relation = word\r\n                # Look for object after the relation\r\n                if i + 2 < len(words):\r\n                    reference_object = ' '.join(words[i+2:])  # e.g., \"the table\"\r\n                break\r\n\r\n        if spatial_relation and reference_object:\r\n            # Find reference object in context\r\n            ref_obj = self.find_object_in_context(reference_object, context)\r\n            if ref_obj:\r\n                # Compute position relative to reference object\r\n                relative_pose = self.compute_relative_pose(\r\n                    ref_obj, spatial_relation, context\r\n                )\r\n\r\n                return {\r\n                    'reference': location_ref,\r\n                    'resolved': True,\r\n                    'type': 'relative',\r\n                    'spatial_relation': spatial_relation,\r\n                    'reference_object': ref_obj,\r\n                    'pose': relative_pose\r\n                }\r\n\r\n        return {'reference': location_ref, 'resolved': False, 'type': 'relative'}\r\n\r\n    def find_object_in_context(self, object_ref: str, context: Dict) -> Dict:\r\n        \"\"\"Find object in context\"\"\"\r\n        if 'objects' in context:\r\n            for obj in context['objects']:\r\n                if object_ref.lower() in obj.get('type', '').lower():\r\n                    return obj\r\n        return None\r\n\r\n    def compute_relative_pose(self, reference_object: Dict, spatial_relation: str,\r\n                            context: Dict) -> Dict:\r\n        \"\"\"Compute pose relative to reference object\"\"\"\r\n        ref_pose = reference_object.get('pose', [0, 0, 0])\r\n\r\n        # Define relative offsets\r\n        offsets = {\r\n            'left': [-0.5, 0, 0],\r\n            'right': [0.5, 0, 0],\r\n            'front': [0, 0.5, 0],\r\n            'back': [0, -0.5, 0],\r\n            'near': [0, 0, 0],  # Same position but closer\r\n            'far': [1, 1, 0]   # Further away\r\n        }\r\n\r\n        offset = offsets.get(spatial_relation, [0, 0, 0])\r\n\r\n        relative_pose = [\r\n            ref_pose[0] + offset[0],\r\n            ref_pose[1] + offset[1],\r\n            ref_pose[2] + offset[2]\r\n        ]\r\n\r\n        return {\r\n            'position': relative_pose,\r\n            'orientation': ref_pose[3:] if len(ref_pose) > 3 else [0, 0, 0, 1]\r\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-voice-command-processing",children:"Real-Time Voice Command Processing"}),"\n",(0,o.jsx)(n.h3,{id:"streaming-voice-recognition",children:"Streaming Voice Recognition"}),"\n",(0,o.jsx)(n.p,{children:"Real-time processing requires efficient streaming capabilities:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import threading\r\nimport queue\r\nimport time\r\nimport pyaudio\r\nimport numpy as np\r\n\r\nclass StreamingVoiceCommandProcessor:\r\n    def __init__(self, asr_model, nlu_model):\r\n        self.asr_model = asr_model\r\n        self.nlu_model = nlu_model\r\n        self.audio_preprocessor = AudioPreprocessor()\r\n\r\n        # Audio stream parameters\r\n        self.chunk_size = 1024\r\n        self.sample_rate = 16000\r\n        self.channels = 1\r\n\r\n        # Processing queues\r\n        self.audio_queue = queue.Queue()\r\n        self.result_queue = queue.Queue()\r\n\r\n        # Processing state\r\n        self.is_listening = False\r\n        self.is_processing = False\r\n        self.command_buffer = []\r\n        self.command_timeout = 3.0  # seconds\r\n\r\n        # Callbacks\r\n        self.command_callbacks = []\r\n\r\n    def start_listening(self):\r\n        """Start listening for voice commands"""\r\n        self.is_listening = True\r\n\r\n        # Start audio capture thread\r\n        self.audio_thread = threading.Thread(target=self._audio_capture_loop)\r\n        self.audio_thread.daemon = True\r\n        self.audio_thread.start()\r\n\r\n        # Start processing thread\r\n        self.processing_thread = threading.Thread(target=self._processing_loop)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n\r\n    def stop_listening(self):\r\n        """Stop listening for voice commands"""\r\n        self.is_listening = False\r\n\r\n    def add_command_callback(self, callback):\r\n        """Add callback for processed commands"""\r\n        self.command_callbacks.append(callback)\r\n\r\n    def _audio_capture_loop(self):\r\n        """Capture audio in real-time"""\r\n        p = pyaudio.PyAudio()\r\n\r\n        stream = p.open(\r\n            format=pyaudio.paFloat32,\r\n            channels=self.channels,\r\n            rate=self.sample_rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk_size\r\n        )\r\n\r\n        buffer = np.array([])\r\n\r\n        try:\r\n            while self.is_listening:\r\n                # Read audio chunk\r\n                data = stream.read(self.chunk_size, exception_on_overflow=False)\r\n                audio_chunk = np.frombuffer(data, dtype=np.float32)\r\n\r\n                # Preprocess audio\r\n                preprocessed = self.audio_preprocessor.preprocess_audio(audio_chunk)\r\n\r\n                # Add to buffer\r\n                buffer = np.concatenate([buffer, preprocessed])\r\n\r\n                # Check for voice activity\r\n                vad_results = self.audio_preprocessor.detect_voice_activity(audio_chunk)\r\n\r\n                if any(vad_results):  # Voice activity detected\r\n                    # Add to processing queue\r\n                    self.audio_queue.put(buffer.copy())\r\n                    buffer = np.array([])  # Clear buffer after processing\r\n                else:\r\n                    # Keep only recent audio for continuity\r\n                    if len(buffer) > self.sample_rate * 2:  # Keep max 2 seconds\r\n                        buffer = buffer[-int(self.sample_rate*0.5):]  # Keep last 0.5 seconds\r\n\r\n        except Exception as e:\r\n            print(f"Audio capture error: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n            p.terminate()\r\n\r\n    def _processing_loop(self):\r\n        """Process audio chunks in real-time"""\r\n        accumulated_audio = np.array([])\r\n        last_voice_time = time.time()\r\n\r\n        while self.is_listening:\r\n            try:\r\n                # Get audio chunk\r\n                try:\r\n                    audio_chunk = self.audio_queue.get(timeout=0.1)\r\n                    accumulated_audio = np.concatenate([accumulated_audio, audio_chunk])\r\n                    last_voice_time = time.time()\r\n                except queue.Empty:\r\n                    # Check if we have accumulated audio and it\'s been silent for a while\r\n                    if (len(accumulated_audio) > 0 and\r\n                        time.time() - last_voice_time > self.command_timeout and\r\n                        len(accumulated_audio) > self.sample_rate * 0.5):  # At least 0.5 seconds of audio\r\n                        # Process accumulated audio\r\n                        self._process_command(accumulated_audio)\r\n                        accumulated_audio = np.array([])\r\n                        continue\r\n\r\n                    continue  # No new audio, continue loop\r\n\r\n                # Process if we have enough audio\r\n                if len(accumulated_audio) > self.sample_rate * 0.5:  # At least 0.5 seconds\r\n                    # Check if this is likely a complete command\r\n                    if self._is_command_complete(accumulated_audio):\r\n                        self._process_command(accumulated_audio)\r\n                        accumulated_audio = np.array([])\r\n\r\n            except Exception as e:\r\n                print(f"Processing error: {e}")\r\n                continue\r\n\r\n    def _is_command_complete(self, audio_data):\r\n        """Determine if audio likely contains a complete command"""\r\n        # Simple heuristic: look for pause at end\r\n        # In practice, use more sophisticated methods\r\n        if len(audio_data) < self.sample_rate * 0.5:  # Too short\r\n            return False\r\n\r\n        # Check end of audio for silence\r\n        end_chunk = audio_data[-int(self.sample_rate * 0.3):]  # Last 0.3 seconds\r\n        if np.mean(np.abs(end_chunk)) < 0.01:  # Low energy indicates pause\r\n            return True\r\n\r\n        # If audio is getting long, assume it\'s complete\r\n        if len(audio_data) > self.sample_rate * 5:  # More than 5 seconds\r\n            return True\r\n\r\n        return False\r\n\r\n    def _process_command(self, audio_data):\r\n        """Process accumulated audio as a command"""\r\n        try:\r\n            # Transcribe audio\r\n            transcription_result = self.asr_model.transcribe_with_confidence(audio_data)\r\n\r\n            if transcription_result[\'confidence\'] > 0.3:  # Threshold for processing\r\n                # Get current context\r\n                context = self._get_current_context()\r\n\r\n                # Parse command\r\n                parsed_command = self.nlu_model.parse_voice_command(\r\n                    transcription_result[\'transcription\'], context\r\n                )\r\n\r\n                # Add confidence and timing info\r\n                parsed_command[\'transcription_confidence\'] = transcription_result[\'confidence\']\r\n                parsed_command[\'timestamp\'] = time.time()\r\n\r\n                # Execute callbacks\r\n                for callback in self.command_callbacks:\r\n                    try:\r\n                        callback(parsed_command)\r\n                    except Exception as e:\r\n                        print(f"Command callback error: {e}")\r\n\r\n        except Exception as e:\r\n            print(f"Command processing error: {e}")\r\n\r\n    def _get_current_context(self):\r\n        """Get current context for command processing"""\r\n        # This would integrate with robot\'s current state\r\n        return {\r\n            \'robot_pose\': [0, 0, 0, 0, 0, 0, 1],  # x, y, z, qx, qy, qz, qw\r\n            \'objects\': [],  # Currently detected objects\r\n            \'named_locations\': {},  # Known locations\r\n            \'recent_utterances\': []  # Recent voice commands\r\n        }\r\n\r\n    def get_command_history(self):\r\n        """Get recent command history"""\r\n        return self.command_buffer[-10:]  # Last 10 commands\n'})}),"\n",(0,o.jsx)(n.h2,{id:"multimodal-voice-interface",children:"Multimodal Voice Interface"}),"\n",(0,o.jsx)(n.h3,{id:"combining-voice-with-visual-and-gesture-inputs",children:"Combining Voice with Visual and Gesture Inputs"}),"\n",(0,o.jsx)(n.p,{children:"Effective voice command systems often work in conjunction with other modalities:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class MultimodalVoiceInterface:\r\n    def __init__(self, voice_processor, gesture_detector, visual_analyzer):\r\n        self.voice_processor = voice_processor\r\n        self.gesture_detector = gesture_detector\r\n        self.visual_analyzer = visual_analyzer\r\n\r\n        self.fusion_engine = MultimodalFusionEngine()\r\n        self.context_manager = ContextManager()\r\n\r\n    def process_multimodal_input(self, voice_data=None, gesture_data=None,\r\n                                visual_data=None, timestamp=None):\r\n        \"\"\"Process multimodal input and fuse information\"\"\"\r\n        # Process each modality separately\r\n        voice_result = None\r\n        if voice_data:\r\n            voice_result = self.voice_processor.process_voice(voice_data)\r\n\r\n        gesture_result = None\r\n        if gesture_data:\r\n            gesture_result = self.gesture_detector.process_gesture(gesture_data)\r\n\r\n        visual_result = None\r\n        if visual_data:\r\n            visual_result = self.visual_analyzer.analyze_visual(visual_data)\r\n\r\n        # Fuse modalities\r\n        fused_result = self.fusion_engine.fuse_modalities(\r\n            voice_result, gesture_result, visual_result, timestamp\r\n        )\r\n\r\n        # Update context\r\n        self.context_manager.update_context(fused_result, timestamp)\r\n\r\n        return fused_result\r\n\r\n    def get_pointing_reference(self, gesture_result, visual_result):\r\n        \"\"\"Get object reference from pointing gesture and visual data\"\"\"\r\n        if gesture_result and visual_result:\r\n            # Use pointing direction to identify object\r\n            pointing_direction = gesture_result.get('pointing_direction')\r\n            objects_in_view = visual_result.get('objects', [])\r\n\r\n            # Find object in pointing direction\r\n            target_object = self.find_object_in_direction(\r\n                pointing_direction, objects_in_view\r\n            )\r\n\r\n            return target_object\r\n\r\n        return None\r\n\r\n    def find_object_in_direction(self, direction, objects):\r\n        \"\"\"Find object in specified direction\"\"\"\r\n        # Convert direction to 3D vector and find closest object\r\n        for obj in objects:\r\n            obj_position = obj.get('position', [0, 0, 0])\r\n            # Calculate angle between direction and vector to object\r\n            # Return object if angle is small enough\r\n            pass\r\n\r\n        return None\r\n\r\nclass MultimodalFusionEngine:\r\n    def __init__(self):\r\n        self.confidence_weights = {\r\n            'voice': 0.4,\r\n            'gesture': 0.3,\r\n            'visual': 0.3\r\n        }\r\n\r\n    def fuse_modalities(self, voice_result, gesture_result, visual_result, timestamp):\r\n        \"\"\"Fuse information from multiple modalities\"\"\"\r\n        fused_command = {\r\n            'timestamp': timestamp,\r\n            'confidence': 0.0,\r\n            'command_type': None,\r\n            'parameters': {},\r\n            'modality_contributions': {}\r\n        }\r\n\r\n        # Extract information from each modality\r\n        voice_info = self.extract_voice_info(voice_result)\r\n        gesture_info = self.extract_gesture_info(gesture_result)\r\n        visual_info = self.extract_visual_info(visual_result)\r\n\r\n        # Combine information based on confidence and relevance\r\n        combined_info = self.combine_information(voice_info, gesture_info, visual_info)\r\n\r\n        # Compute overall confidence\r\n        fused_command['confidence'] = self.compute_fusion_confidence(\r\n            voice_result, gesture_result, visual_result\r\n        )\r\n\r\n        # Determine command type and parameters\r\n        fused_command['command_type'], fused_command['parameters'] = \\\r\n            self.determine_command(combined_info)\r\n\r\n        # Store modality contributions\r\n        fused_command['modality_contributions'] = {\r\n            'voice': voice_info,\r\n            'gesture': gesture_info,\r\n            'visual': visual_info\r\n        }\r\n\r\n        return fused_command\r\n\r\n    def extract_voice_info(self, voice_result):\r\n        \"\"\"Extract command information from voice\"\"\"\r\n        if not voice_result:\r\n            return {}\r\n\r\n        return {\r\n            'command': voice_result.get('transcription', ''),\r\n            'confidence': voice_result.get('confidence', 0.0),\r\n            'parsed_command': voice_result.get('parsed_command', {})\r\n        }\r\n\r\n    def extract_gesture_info(self, gesture_result):\r\n        \"\"\"Extract information from gesture\"\"\"\r\n        if not gesture_result:\r\n            return {}\r\n\r\n        return {\r\n            'gesture_type': gesture_result.get('type', ''),\r\n            'direction': gesture_result.get('direction'),\r\n            'target': gesture_result.get('target')\r\n        }\r\n\r\n    def extract_visual_info(self, visual_result):\r\n        \"\"\"Extract information from visual analysis\"\"\"\r\n        if not visual_result:\r\n            return {}\r\n\r\n        return {\r\n            'objects': visual_result.get('objects', []),\r\n            'scene_context': visual_result.get('scene_context', {}),\r\n            'target_object': visual_result.get('target_object')\r\n        }\r\n\r\n    def combine_information(self, voice_info, gesture_info, visual_info):\r\n        \"\"\"Combine information from all modalities\"\"\"\r\n        combined = {}\r\n\r\n        # Add voice command\r\n        if voice_info:\r\n            combined['voice_command'] = voice_info['parsed_command']\r\n\r\n        # Add gesture information\r\n        if gesture_info:\r\n            combined['gesture'] = gesture_info\r\n\r\n        # Add visual context\r\n        if visual_info:\r\n            combined['visual_context'] = visual_info\r\n\r\n        return combined\r\n\r\n    def compute_fusion_confidence(self, voice_result, gesture_result, visual_result):\r\n        \"\"\"Compute confidence of fused result\"\"\"\r\n        confidences = []\r\n\r\n        if voice_result:\r\n            confidences.append(voice_result.get('confidence', 0.0) * self.confidence_weights['voice'])\r\n\r\n        if gesture_result:\r\n            confidences.append(gesture_result.get('confidence', 0.0) * self.confidence_weights['gesture'])\r\n\r\n        if visual_result:\r\n            confidences.append(visual_result.get('confidence', 0.0) * self.confidence_weights['visual'])\r\n\r\n        return sum(confidences)\r\n\r\n    def determine_command(self, combined_info):\r\n        \"\"\"Determine final command from combined information\"\"\"\r\n        # This would implement sophisticated fusion logic\r\n        # For now, use a simple approach\r\n\r\n        command_type = None\r\n        parameters = {}\r\n\r\n        # If voice command exists, use it as primary\r\n        if 'voice_command' in combined_info:\r\n            voice_cmd = combined_info['voice_command']\r\n            command_type = voice_cmd.get('command_type', 'general')\r\n            parameters.update(voice_cmd)\r\n\r\n        # Add information from other modalities\r\n        if 'gesture' in combined_info:\r\n            gesture = combined_info['gesture']\r\n            if 'target' in gesture:\r\n                parameters['target'] = gesture['target']\r\n\r\n        if 'visual_context' in combined_info:\r\n            visual_ctx = combined_info['visual_context']\r\n            if 'target_object' in visual_ctx:\r\n                parameters['target_object'] = visual_ctx['target_object']\r\n\r\n        return command_type, parameters\r\n\r\nclass ContextManager:\r\n    def __init__(self):\r\n        self.context_history = []\r\n        self.max_history = 20\r\n\r\n    def update_context(self, fused_result, timestamp):\r\n        \"\"\"Update context with new information\"\"\"\r\n        context_entry = {\r\n            'result': fused_result,\r\n            'timestamp': timestamp,\r\n            'objects_mentioned': self.extract_objects(fused_result),\r\n            'locations_mentioned': self.extract_locations(fused_result)\r\n        }\r\n\r\n        self.context_history.append(context_entry)\r\n\r\n        # Maintain history size\r\n        if len(self.context_history) > self.max_history:\r\n            self.context_history.pop(0)\r\n\r\n    def extract_objects(self, fused_result):\r\n        \"\"\"Extract objects from fused result\"\"\"\r\n        objects = []\r\n        mod_contributions = fused_result.get('modality_contributions', {})\r\n\r\n        if 'visual' in mod_contributions:\r\n            visual_objects = mod_contributions['visual'].get('objects', [])\r\n            objects.extend([obj.get('type', '') for obj in visual_objects])\r\n\r\n        if 'voice' in mod_contributions:\r\n            voice_cmd = mod_contributions['voice'].get('parsed_command', {})\r\n            if 'object' in voice_cmd:\r\n                objects.append(voice_cmd['object'])\r\n\r\n        return objects\r\n\r\n    def extract_locations(self, fused_result):\r\n        \"\"\"Extract locations from fused result\"\"\"\r\n        locations = []\r\n        mod_contributions = fused_result.get('modality_contributions', {})\r\n\r\n        if 'voice' in mod_contributions:\r\n            voice_cmd = mod_contributions['voice'].get('parsed_command', {})\r\n            if 'location' in voice_cmd:\r\n                locations.append(voice_cmd['location'])\r\n\r\n        return locations\r\n\r\n    def get_current_context(self):\r\n        \"\"\"Get current context for command interpretation\"\"\"\r\n        if not self.context_history:\r\n            return {}\r\n\r\n        # Create context from recent history\r\n        recent_entries = self.context_history[-5:]  # Last 5 entries\r\n\r\n        context = {\r\n            'recent_objects': [],\r\n            'recent_locations': [],\r\n            'recent_commands': [],\r\n            'timestamp': time.time()\r\n        }\r\n\r\n        for entry in recent_entries:\r\n            context['recent_objects'].extend(entry.get('objects_mentioned', []))\r\n            context['recent_locations'].extend(entry.get('locations_mentioned', []))\r\n            context['recent_commands'].append(entry['result'])\r\n\r\n        return context\n"})}),"\n",(0,o.jsx)(n.h2,{id:"isaac-integration-for-voice-commands",children:"Isaac Integration for Voice Commands"}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-interface-for-voice-command-processing",children:"ROS 2 Interface for Voice Command Processing"}),"\n",(0,o.jsx)(n.p,{children:"Integrating voice command processing with ROS 2 and Isaac systems:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String, Bool\r\nfrom sensor_msgs.msg import AudioData\r\nfrom geometry_msgs.msg import Pose, Point\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\n\r\nclass IsaacVoiceCommandNode(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_voice_command')\r\n\r\n        # Publishers\r\n        self.command_publisher = self.create_publisher(String, 'parsed_voice_command', 10)\r\n        self.status_publisher = self.create_publisher(String, 'voice_system_status', 10)\r\n        self.indication_publisher = self.create_publisher(Bool, 'voice_command_detected', 10)\r\n\r\n        # Subscribers\r\n        self.audio_subscriber = self.create_subscription(\r\n            AudioData, 'audio_input', self.audio_callback, 10\r\n        )\r\n\r\n        # Initialize voice processing components\r\n        self.asr_model = VoiceCommandASR()\r\n        self.nlu_model = VoiceNLU()\r\n        self.voice_processor = StreamingVoiceCommandProcessor(\r\n            self.asr_model, self.nlu_model\r\n        )\r\n\r\n        # Add callback for processed commands\r\n        self.voice_processor.add_command_callback(self.on_voice_command_processed)\r\n\r\n        # Context provider\r\n        self.context_provider = ContextProvider(self)\r\n\r\n        # Start voice processing\r\n        self.voice_processor.start_listening()\r\n\r\n        self.get_logger().info('Isaac Voice Command Node initialized')\r\n\r\n    def audio_callback(self, msg):\r\n        \"\"\"Handle incoming audio data\"\"\"\r\n        try:\r\n            # Convert audio message to numpy array\r\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n            # Add to processing queue (in a real system, this would interface differently)\r\n            # For now, we'll process it directly\r\n            self.process_audio_chunk(audio_data)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing audio: {e}')\r\n\r\n    def process_audio_chunk(self, audio_data):\r\n        \"\"\"Process audio chunk through voice pipeline\"\"\"\r\n        try:\r\n            # Preprocess audio\r\n            preprocessed = self.voice_processor.audio_preprocessor.preprocess_audio(audio_data)\r\n\r\n            # Check for voice activity\r\n            vad_results = self.voice_processor.audio_preprocessor.detect_voice_activity(preprocessed)\r\n\r\n            if any(vad_results):  # Voice activity detected\r\n                # Indicate voice command detected\r\n                indication_msg = Bool()\r\n                indication_msg.data = True\r\n                self.indication_publisher.publish(indication_msg)\r\n\r\n                # Transcribe audio\r\n                transcription_result = self.asr_model.transcribe_with_confidence(preprocessed)\r\n\r\n                if transcription_result['confidence'] > 0.3:\r\n                    # Get current context\r\n                    context = self.context_provider.get_current_context()\r\n\r\n                    # Parse command\r\n                    parsed_command = self.nlu_model.parse_voice_command(\r\n                        transcription_result['transcription'], context\r\n                    )\r\n\r\n                    # Publish parsed command\r\n                    command_msg = String()\r\n                    command_msg.data = str(parsed_command)\r\n                    self.command_publisher.publish(command_msg)\r\n\r\n                    # Log command\r\n                    self.get_logger().info(\r\n                        f'Processed voice command: {transcription_result[\"transcription\"]} '\r\n                        f'-> {parsed_command}'\r\n                    )\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in audio processing: {e}')\r\n\r\n    def on_voice_command_processed(self, parsed_command):\r\n        \"\"\"Handle processed voice command\"\"\"\r\n        # This callback is called when a complete command is processed\r\n        command_type = parsed_command.get('command_type', 'general')\r\n\r\n        if command_type != 'general':\r\n            # Publish to command execution system\r\n            command_msg = String()\r\n            command_msg.data = str(parsed_command)\r\n            self.command_publisher.publish(command_msg)\r\n\r\n            self.get_logger().info(f'Published command: {parsed_command}')\r\n\r\n    def destroy_node(self):\r\n        \"\"\"Clean up when node is destroyed\"\"\"\r\n        self.voice_processor.stop_listening()\r\n        super().destroy_node()\r\n\r\nclass ContextProvider:\r\n    def __init__(self, node):\r\n        self.node = node\r\n        self.current_objects = []\r\n        self.current_locations = {}\r\n        self.robot_pose = [0, 0, 0, 0, 0, 0, 1]\r\n\r\n        # Create subscribers for context information\r\n        self.object_subscriber = self.node.create_subscription(\r\n            String, 'detected_objects', self.object_callback, 10\r\n        )\r\n        self.pose_subscriber = self.node.create_subscription(\r\n            Pose, 'robot_pose', self.pose_callback, 10\r\n        )\r\n\r\n    def object_callback(self, msg):\r\n        \"\"\"Update object context\"\"\"\r\n        try:\r\n            import json\r\n            objects = json.loads(msg.data)\r\n            self.current_objects = objects\r\n        except:\r\n            pass\r\n\r\n    def pose_callback(self, msg):\r\n        \"\"\"Update robot pose context\"\"\"\r\n        self.robot_pose = [\r\n            msg.position.x, msg.position.y, msg.position.z,\r\n            msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w\r\n        ]\r\n\r\n    def get_current_context(self):\r\n        \"\"\"Get current context for voice command processing\"\"\"\r\n        return {\r\n            'objects': self.current_objects,\r\n            'robot_pose': self.robot_pose,\r\n            'named_locations': self.current_locations,\r\n            'recent_utterances': []  # Would come from voice history\r\n        }\r\n\r\n# Voice activity detection node\r\nclass VoiceActivityDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_activity_detection')\r\n\r\n        # Publishers\r\n        self.vad_publisher = self.create_publisher(Bool, 'voice_activity', 10)\r\n        self.speech_start_publisher = self.create_publisher(Bool, 'speech_start', 10)\r\n        self.speech_end_publisher = self.create_publisher(Bool, 'speech_end', 10)\r\n\r\n        # Subscribers\r\n        self.audio_subscriber = self.create_subscription(\r\n            AudioData, 'audio_input', self.audio_callback, 10\r\n        )\r\n\r\n        # VAD components\r\n        self.vad_detector = AudioPreprocessor()\r\n        self.is_speaking = False\r\n        self.speech_start_time = None\r\n\r\n    def audio_callback(self, msg):\r\n        \"\"\"Process audio for voice activity detection\"\"\"\r\n        try:\r\n            # Convert audio to numpy\r\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n            # Detect voice activity\r\n            vad_results = self.vad_detector.detect_voice_activity(audio_data)\r\n\r\n            if any(vad_results) and not self.is_speaking:\r\n                # Speech started\r\n                self.is_speaking = True\r\n                self.speech_start_time = self.get_clock().now()\r\n\r\n                # Publish speech start\r\n                start_msg = Bool()\r\n                start_msg.data = True\r\n                self.speech_start_publisher.publish(start_msg)\r\n\r\n            elif not any(vad_results) and self.is_speaking:\r\n                # Speech ended\r\n                self.is_speaking = False\r\n\r\n                # Publish speech end\r\n                end_msg = Bool()\r\n                end_msg.data = True\r\n                self.speech_end_publisher.publish(end_msg)\r\n\r\n            # Publish current VAD state\r\n            vad_msg = Bool()\r\n            vad_msg.data = any(vad_results)\r\n            self.vad_publisher.publish(vad_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'VAD error: {e}')\r\n\r\n# Example usage and integration\r\ndef main():\r\n    rclpy.init()\r\n\r\n    # Create voice command node\r\n    voice_node = IsaacVoiceCommandNode()\r\n\r\n    try:\r\n        rclpy.spin(voice_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        voice_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"evaluation-and-validation",children:"Evaluation and Validation"}),"\n",(0,o.jsx)(n.h3,{id:"voice-command-system-evaluation",children:"Voice Command System Evaluation"}),"\n",(0,o.jsx)(n.p,{children:"Evaluating voice command systems requires specific metrics and benchmarks:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class VoiceCommandEvaluator:\r\n    def __init__(self, voice_system):\r\n        self.voice_system = voice_system\r\n        self.results = []\r\n\r\n    def evaluate_recognition_accuracy(self, test_audio_data):\r\n        \"\"\"Evaluate speech recognition accuracy\"\"\"\r\n        correct_recognitions = 0\r\n        total_commands = len(test_audio_data)\r\n\r\n        for audio_sample, expected_transcription in test_audio_data:\r\n            try:\r\n                result = self.voice_system.asr_model.transcribe_audio(audio_sample)\r\n                if self.transcriptions_match(result, expected_transcription):\r\n                    correct_recognitions += 1\r\n            except:\r\n                pass  # Count as incorrect\r\n\r\n        accuracy = correct_recognitions / total_commands if total_commands > 0 else 0\r\n        return accuracy\r\n\r\n    def evaluate_command_parsing(self, test_commands):\r\n        \"\"\"Evaluate command parsing accuracy\"\"\"\r\n        correct_parses = 0\r\n        total_commands = len(test_commands)\r\n\r\n        for command_text, expected_structure in test_commands:\r\n            try:\r\n                # Get current context (simplified)\r\n                context = {}\r\n                parsed = self.voice_system.nlu_model.parse_voice_command(command_text, context)\r\n\r\n                if self.command_structures_match(parsed, expected_structure):\r\n                    correct_parses += 1\r\n            except:\r\n                pass  # Count as incorrect\r\n\r\n        accuracy = correct_parses / total_commands if total_commands > 0 else 0\r\n        return accuracy\r\n\r\n    def evaluate_robustness(self, noisy_audio_data):\r\n        \"\"\"Evaluate system robustness to noise\"\"\"\r\n        success_count = 0\r\n        total_tests = len(noisy_audio_data)\r\n\r\n        for audio_sample, noise_level, expected_command in noisy_audio_data:\r\n            try:\r\n                # Process noisy audio\r\n                result = self.voice_system.process_audio_chunk(audio_sample)\r\n\r\n                # Check if correct command was identified\r\n                if result and self.command_correct(result, expected_command):\r\n                    success_count += 1\r\n            except:\r\n                pass\r\n\r\n        robustness = success_count / total_tests if total_tests > 0 else 0\r\n        return robustness\r\n\r\n    def evaluate_latency(self, test_commands):\r\n        \"\"\"Evaluate system response latency\"\"\"\r\n        latencies = []\r\n\r\n        for command in test_commands:\r\n            start_time = time.time()\r\n            try:\r\n                self.voice_system.process_audio_chunk(command['audio'])\r\n                end_time = time.time()\r\n                latencies.append(end_time - start_time)\r\n            except:\r\n                latencies.append(float('inf'))  # Failed to process\r\n\r\n        avg_latency = sum(latencies) / len(latencies) if latencies else float('inf')\r\n        return avg_latency\r\n\r\n    def run_comprehensive_evaluation(self, dataset):\r\n        \"\"\"Run comprehensive evaluation of voice command system\"\"\"\r\n        results = {}\r\n\r\n        # Recognition accuracy\r\n        recognition_tests = [item for item in dataset if 'recognition' in item.get('type', '')]\r\n        results['recognition_accuracy'] = self.evaluate_recognition_accuracy(recognition_tests)\r\n\r\n        # Command parsing accuracy\r\n        parsing_tests = [item for item in dataset if 'parsing' in item.get('type', '')]\r\n        results['parsing_accuracy'] = self.evaluate_command_parsing(parsing_tests)\r\n\r\n        # Robustness to noise\r\n        noise_tests = [item for item in dataset if 'noise' in item.get('type', '')]\r\n        results['noise_robustness'] = self.evaluate_robustness(noise_tests)\r\n\r\n        # Latency\r\n        latency_tests = [item for item in dataset if 'latency' in item.get('type', '')]\r\n        results['average_latency'] = self.evaluate_latency(latency_tests)\r\n\r\n        # Overall success rate\r\n        results['overall_success_rate'] = self.compute_overall_success(dataset)\r\n\r\n        return results\r\n\r\n    def compute_overall_success(self, dataset):\r\n        \"\"\"Compute overall system success rate\"\"\"\r\n        successful_interactions = 0\r\n        total_interactions = len(dataset)\r\n\r\n        for test_case in dataset:\r\n            try:\r\n                # Simulate full interaction\r\n                result = self.simulate_interaction(test_case)\r\n                if result['success']:\r\n                    successful_interactions += 1\r\n            except:\r\n                pass\r\n\r\n        return successful_interactions / total_interactions if total_interactions > 0 else 0.0\r\n\r\n    def simulate_interaction(self, test_case):\r\n        \"\"\"Simulate a complete voice interaction\"\"\"\r\n        # This would simulate the full pipeline: ASR -> NLU -> Action\r\n        audio = test_case.get('audio', [])\r\n        expected_command = test_case.get('expected_command', '')\r\n\r\n        # Process through full pipeline\r\n        transcription = self.voice_system.asr_model.transcribe_audio(audio)\r\n        context = test_case.get('context', {})\r\n        parsed_command = self.voice_system.nlu_model.parse_voice_command(transcription, context)\r\n\r\n        # Check if parsed command matches expectation\r\n        success = self.command_structures_match(parsed_command, expected_command)\r\n\r\n        return {'success': success, 'parsed_command': parsed_command}\r\n\r\n    def transcriptions_match(self, result, expected):\r\n        \"\"\"Check if transcriptions match (with tolerance for minor variations)\"\"\"\r\n        result_clean = self.clean_for_comparison(result)\r\n        expected_clean = self.clean_for_comparison(expected)\r\n        return result_clean == expected_clean\r\n\r\n    def command_structures_match(self, result, expected):\r\n        \"\"\"Check if command structures match\"\"\"\r\n        # Compare key elements of command structure\r\n        if isinstance(expected, dict):\r\n            for key, value in expected.items():\r\n                if key not in result or result[key] != value:\r\n                    return False\r\n            return True\r\n        else:\r\n            return str(result) == str(expected)\r\n\r\n    def clean_for_comparison(self, text):\r\n        \"\"\"Clean text for comparison (remove punctuation, normalize)\"\"\"\r\n        import re\r\n        # Remove punctuation and extra whitespace\r\n        cleaned = re.sub(r'[^\\w\\s]', ' ', text.lower())\r\n        cleaned = ' '.join(cleaned.split())\r\n        return cleaned\r\n\r\n# Example evaluation dataset\r\ndef create_voice_command_evaluation_dataset():\r\n    \"\"\"Create dataset for voice command evaluation\"\"\"\r\n    return [\r\n        # Recognition test\r\n        {\r\n            'type': 'recognition',\r\n            'audio': np.random.random(16000),  # 1 second of random audio (would be real audio)\r\n            'expected': 'go to kitchen',\r\n            'context': {}\r\n        },\r\n\r\n        # Parsing test\r\n        {\r\n            'type': 'parsing',\r\n            'command_text': 'please go to the kitchen',\r\n            'expected_structure': {'command_type': 'navigate', 'location': 'kitchen'},\r\n            'context': {}\r\n        },\r\n\r\n        # Noise robustness test\r\n        {\r\n            'type': 'noise',\r\n            'audio': np.random.random(16000),\r\n            'noise_level': 'high',\r\n            'expected_command': 'grasp the red ball',\r\n            'context': {'objects': [{'type': 'ball', 'color': 'red'}]}\r\n        }\r\n    ]\r\n\r\n# Main evaluation function\r\ndef evaluate_voice_system(voice_system, dataset=None):\r\n    \"\"\"Complete evaluation of voice command system\"\"\"\r\n    if dataset is None:\r\n        dataset = create_voice_command_evaluation_dataset()\r\n\r\n    evaluator = VoiceCommandEvaluator(voice_system)\r\n    results = evaluator.run_comprehensive_evaluation(dataset)\r\n\r\n    print(\"Voice Command System Evaluation Results:\")\r\n    print(f\"  Recognition Accuracy: {results['recognition_accuracy']:.2%}\")\r\n    print(f\"  Parsing Accuracy: {results['parsing_accuracy']:.2%}\")\r\n    print(f\"  Noise Robustness: {results['noise_robustness']:.2%}\")\r\n    print(f\"  Average Latency: {results['average_latency']:.3f}s\")\r\n    print(f\"  Overall Success Rate: {results['overall_success_rate']:.2%}\")\r\n\r\n    return results\n"})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Voice command interpretation systems enable natural and intuitive human-robot interaction by allowing users to communicate with robots using spoken language. These systems must handle the unique challenges of real-time audio processing, environmental noise, and the ambiguities inherent in spoken language."}),"\n",(0,o.jsx)(n.p,{children:"The key components of effective voice command systems include robust audio preprocessing and speech recognition, context-aware natural language understanding, real-time processing capabilities, and multimodal integration that combines voice with visual and gesture inputs for more accurate interpretation."}),"\n",(0,o.jsx)(n.p,{children:"The next section will explore natural language to robot action mapping, which builds upon voice command interpretation to convert linguistic instructions into executable robotic behaviors."}),"\n",(0,o.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,o.jsx)(n.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const o={},i=t.createContext(o);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);