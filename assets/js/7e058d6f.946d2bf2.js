"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[5446],{8453(n,e,r){r.d(e,{R:()=>s,x:()=>a});var t=r(6540);const o={},i=t.createContext(o);function s(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),t.createElement(i.Provider,{value:e},n.children)}},9884(n,e,r){r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>_,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/instruction-following","title":"Instruction Following and Task Planning","description":"Learning Objectives","source":"@site/docs/module-4-vla/instruction-following.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/instruction-following","permalink":"/Book-ai-native/docs/module-4-vla/instruction-following","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/module-4-vla/instruction-following.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Multimodal Embeddings and Representation","permalink":"/Book-ai-native/docs/module-4-vla/multimodal-embeddings"},"next":{"title":"Embodied Language Models","permalink":"/Book-ai-native/docs/module-4-vla/embodied-language"}}');var o=r(4848),i=r(8453);const s={sidebar_position:3},a="Instruction Following and Task Planning",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Instruction Following",id:"introduction-to-instruction-following",level:2},{value:"Hierarchical Task Planning Architecture",id:"hierarchical-task-planning-architecture",level:2},{value:"Three-Level Planning Hierarchy",id:"three-level-planning-hierarchy",level:3},{value:"Task Decomposition Framework",id:"task-decomposition-framework",level:3},{value:"Semantic Parsing and Natural Language Understanding",id:"semantic-parsing-and-natural-language-understanding",level:2},{value:"Grammar-Based Parsing",id:"grammar-based-parsing",level:3},{value:"Neural Semantic Parsing",id:"neural-semantic-parsing",level:3},{value:"Context-Aware Instruction Interpretation",id:"context-aware-instruction-interpretation",level:2},{value:"Environmental Context Modeling",id:"environmental-context-modeling",level:3},{value:"Temporal Context and History",id:"temporal-context-and-history",level:3},{value:"Instruction-to-Action Mapping",id:"instruction-to-action-mapping",level:2},{value:"Action Space Mapping",id:"action-space-mapping",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Robust Execution Framework",id:"robust-execution-framework",level:3},{value:"Isaac Integration for Instruction Following",id:"isaac-integration-for-instruction-following",level:2},{value:"ROS 2 Interface for Instruction Following",id:"ros-2-interface-for-instruction-following",level:3},{value:"Evaluation and Validation",id:"evaluation-and-validation",level:2},{value:"Instruction Following Benchmarks",id:"instruction-following-benchmarks",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function u(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"instruction-following-and-task-planning",children:"Instruction Following and Task Planning"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Design systems that parse and interpret natural language instructions for robots"}),"\n",(0,o.jsx)(e.li,{children:"Implement hierarchical task planning frameworks for complex robotic behaviors"}),"\n",(0,o.jsx)(e.li,{children:"Create semantic parsing mechanisms that convert language to executable actions"}),"\n",(0,o.jsx)(e.li,{children:"Develop context-aware instruction interpretation that considers environmental constraints"}),"\n",(0,o.jsx)(e.li,{children:"Build robust error handling and recovery mechanisms for failed instruction execution"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction-to-instruction-following",children:"Introduction to Instruction Following"}),"\n",(0,o.jsx)(e.p,{children:"Instruction following represents one of the most challenging aspects of Vision-Language-Action (VLA) systems. Unlike traditional programming approaches where robot behaviors are explicitly coded, instruction following systems must interpret natural language commands and translate them into appropriate sequences of robotic actions."}),"\n",(0,o.jsx)(e.p,{children:"The complexity of instruction following stems from several factors:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Ambiguity"}),": Natural language is inherently ambiguous, with multiple possible interpretations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context Dependence"}),": Instructions often depend on environmental context and previous interactions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Spatial Reasoning"}),": Many robot instructions involve spatial relationships and navigation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Temporal Dependencies"}),": Complex instructions require sequences of actions with temporal relationships"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Physical Constraints"}),": Robot capabilities and environmental constraints limit feasible actions"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"hierarchical-task-planning-architecture",children:"Hierarchical Task Planning Architecture"}),"\n",(0,o.jsx)(e.h3,{id:"three-level-planning-hierarchy",children:"Three-Level Planning Hierarchy"}),"\n",(0,o.jsx)(e.p,{children:"Effective instruction following systems typically employ a hierarchical planning architecture with three levels:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Task Level"}),": High-level goal decomposition and sequencing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Level"}),": Mid-level primitive selection and parameterization"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Motion Level"}),": Low-level trajectory generation and control"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class HierarchicalInstructionFollower:\r\n    def __init__(self):\r\n        self.task_planner = TaskPlanner()\r\n        self.action_planner = ActionPlanner()\r\n        self.motion_planner = MotionPlanner()\r\n\r\n    def follow_instruction(self, instruction, context):\r\n        """Follow a natural language instruction using hierarchical planning"""\r\n        # Parse high-level task from instruction\r\n        high_level_task = self.parse_instruction(instruction, context)\r\n\r\n        # Decompose task hierarchically\r\n        task_plan = self.task_planner.decompose(high_level_task, context)\r\n\r\n        # Execute plan at different levels\r\n        for subtask in task_plan:\r\n            action_sequence = self.action_planner.plan(subtask, context)\r\n            for action in action_sequence:\r\n                motion_commands = self.motion_planner.generate(action, context)\r\n                self.execute_motion(motion_commands)\r\n\r\n    def parse_instruction(self, instruction, context):\r\n        """Parse natural language instruction into structured task representation"""\r\n        # This would use NLP techniques to extract task structure\r\n        pass\n'})}),"\n",(0,o.jsx)(e.h3,{id:"task-decomposition-framework",children:"Task Decomposition Framework"}),"\n",(0,o.jsx)(e.p,{children:"Task decomposition breaks complex instructions into manageable subtasks:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class TaskPlanner:\r\n    def __init__(self):\r\n        self.task_library = self.load_task_library()\r\n        self.decomposition_rules = self.load_decomposition_rules()\r\n\r\n    def decompose(self, high_level_task, context):\r\n        """Decompose high-level task into executable subtasks"""\r\n        if high_level_task in self.task_library:\r\n            return self.task_library[high_level_task]\r\n\r\n        # Apply decomposition rules\r\n        subtasks = self.apply_decomposition_rules(high_level_task, context)\r\n        return subtasks\r\n\r\n    def load_task_library(self):\r\n        """Load predefined task decompositions"""\r\n        return {\r\n            \'clean_table\': [\r\n                \'identify_objects_on_table\',\r\n                \'grasp_object\',\r\n                \'move_to_trash\',\r\n                \'place_object\',\r\n                \'repeat_until_clean\'\r\n            ],\r\n            \'set_table\': [\r\n                \'identify_dining_area\',\r\n                \'fetch_plate\',\r\n                \'move_to_table\',\r\n                \'place_plate\',\r\n                \'fetch_cup\',\r\n                \'move_to_table\',\r\n                \'place_cup\'\r\n            ]\r\n        }\r\n\r\n    def apply_decomposition_rules(self, task, context):\r\n        """Apply learned decomposition rules to novel tasks"""\r\n        # Example: "bring X from Y to Z" -> navigate to Y, grasp X, navigate to Z, place X\r\n        if self.matches_pattern(task, \'bring_X_from_Y_to_Z\'):\r\n            obj = self.extract_object(task)\r\n            source = self.extract_location(task, \'source\')\r\n            target = self.extract_location(task, \'target\')\r\n\r\n            return [\r\n                NavigateToLocation(source),\r\n                GraspObject(obj),\r\n                NavigateToLocation(target),\r\n                PlaceObject(obj)\r\n            ]\r\n\r\n        return [task]  # No decomposition possible\r\n\r\n    def matches_pattern(self, task, pattern):\r\n        """Check if task matches a known pattern"""\r\n        # Implementation would use NLP pattern matching\r\n        pass\r\n\r\n    def extract_object(self, task):\r\n        """Extract object reference from task"""\r\n        # Implementation would use NLP to identify objects\r\n        pass\r\n\r\n    def extract_location(self, task, location_type):\r\n        """Extract location reference from task"""\r\n        # Implementation would identify source/target locations\r\n        pass\n'})}),"\n",(0,o.jsx)(e.h2,{id:"semantic-parsing-and-natural-language-understanding",children:"Semantic Parsing and Natural Language Understanding"}),"\n",(0,o.jsx)(e.h3,{id:"grammar-based-parsing",children:"Grammar-Based Parsing"}),"\n",(0,o.jsx)(e.p,{children:"Grammar-based parsing uses formal rules to convert natural language to structured representations:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import nltk\r\nfrom nltk import CFG\r\nfrom nltk.parse import ChartParser\r\n\r\nclass SemanticParser:\r\n    def __init__(self):\r\n        self.grammar = self.create_robot_grammar()\r\n        self.parser = ChartParser(self.grammar)\r\n\r\n    def create_robot_grammar(self):\r\n        \"\"\"Create grammar for robot instruction language\"\"\"\r\n        # Define grammar rules for robot instructions\r\n        grammar_rules = \"\"\"\r\n            S -> COMMAND\r\n            COMMAND -> ACTION OBJECT LOCATION\r\n                   | ACTION OBJECT\r\n                   | NAVIGATE LOCATION\r\n                   | ACTION NAVIGATE LOCATION\r\n            ACTION -> 'pick' | 'grasp' | 'place' | 'move' | 'go' | 'navigate' | 'bring' | 'fetch'\r\n            OBJECT -> 'ball' | 'cup' | 'book' | 'box' | 'red_ball' | 'blue_cup'\r\n            LOCATION -> 'kitchen' | 'living_room' | 'table' | 'shelf' | 'counter' | 'toilet' | 'bedroom'\r\n                     | 'left' | 'right' | 'front' | 'back' | 'near_ball' | 'by_cup'\r\n        \"\"\"\r\n        return CFG.fromstring(grammar_rules)\r\n\r\n    def parse(self, instruction):\r\n        \"\"\"Parse natural language instruction using grammar\"\"\"\r\n        tokens = instruction.lower().split()\r\n\r\n        try:\r\n            # Parse the instruction\r\n            parses = list(self.parser.parse(tokens))\r\n            if parses:\r\n                return self.convert_parse_to_action(parses[0])\r\n        except:\r\n            # Handle parsing failure\r\n            return self.handle_parsing_failure(instruction)\r\n\r\n        return None\r\n\r\n    def convert_parse_to_action(self, parse_tree):\r\n        \"\"\"Convert parse tree to executable action representation\"\"\"\r\n        # Convert parse tree to structured action\r\n        action = {}\r\n\r\n        for subtree in parse_tree.subtrees():\r\n            if subtree.label() == 'ACTION':\r\n                action['action_type'] = str(subtree[0])\r\n            elif subtree.label() == 'OBJECT':\r\n                action['object'] = str(subtree[0])\r\n            elif subtree.label() == 'LOCATION':\r\n                action['location'] = str(subtree[0])\r\n\r\n        return action\r\n\r\n    def handle_parsing_failure(self, instruction):\r\n        \"\"\"Handle cases where grammar-based parsing fails\"\"\"\r\n        # Use alternative parsing methods (e.g., neural parsing)\r\n        return self.neural_parse(instruction)\r\n\r\n    def neural_parse(self, instruction):\r\n        \"\"\"Use neural network for parsing (fallback method)\"\"\"\r\n        # Implementation would use a trained neural parser\r\n        pass\n"})}),"\n",(0,o.jsx)(e.h3,{id:"neural-semantic-parsing",children:"Neural Semantic Parsing"}),"\n",(0,o.jsx)(e.p,{children:"Neural approaches can handle more complex and ambiguous language:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass NeuralSemanticParser(nn.Module):\r\n    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\r\n        super().__init__()\r\n\r\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\r\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\r\n        self.action_classifier = nn.Linear(hidden_dim, output_dim)\r\n        self.location_classifier = nn.Linear(hidden_dim, output_dim)\r\n        self.object_classifier = nn.Linear(hidden_dim, output_dim)\r\n\r\n    def forward(self, tokens):\r\n        """Parse tokens into structured representation"""\r\n        embedded = self.embedding(tokens)  # [B, T, embed_dim]\r\n        lstm_out, (hidden, _) = self.lstm(embedded)  # [B, T, hidden_dim]\r\n\r\n        # Use final hidden state for classification\r\n        final_hidden = hidden[-1]  # [B, hidden_dim]\r\n\r\n        action_probs = F.softmax(self.action_classifier(final_hidden), dim=-1)\r\n        location_probs = F.softmax(self.location_classifier(final_hidden), dim=-1)\r\n        object_probs = F.softmax(self.object_classifier(final_hidden), dim=-1)\r\n\r\n        return {\r\n            \'action\': action_probs,\r\n            \'location\': location_probs,\r\n            \'object\': object_probs\r\n        }\r\n\r\nclass InstructionUnderstandingSystem:\r\n    def __init__(self):\r\n        self.neural_parser = NeuralSemanticParser(\r\n            vocab_size=10000, embed_dim=256, hidden_dim=512, output_dim=100\r\n        )\r\n        self.grammar_parser = SemanticParser()\r\n\r\n    def understand_instruction(self, instruction, context):\r\n        """Understand natural language instruction using multiple approaches"""\r\n        # Try grammar-based parsing first (for structured commands)\r\n        structured_action = self.grammar_parser.parse(instruction)\r\n\r\n        if structured_action:\r\n            return self.refine_with_context(structured_action, context)\r\n\r\n        # Fall back to neural parsing for complex/ambiguous commands\r\n        tokens = self.tokenize(instruction)\r\n        neural_output = self.neural_parser(tokens)\r\n\r\n        return self.convert_neural_output_to_action(neural_output, context)\r\n\r\n    def refine_with_context(self, action, context):\r\n        """Refine parsed action based on environmental context"""\r\n        # Resolve ambiguities using context\r\n        if \'location\' in action and action[\'location\'] == \'it\':\r\n            # Resolve pronoun based on context\r\n            action[\'location\'] = self.resolve_pronoun(\'it\', context)\r\n\r\n        return action\r\n\r\n    def resolve_pronoun(self, pronoun, context):\r\n        """Resolve pronouns based on context"""\r\n        # Implementation would use context to resolve pronouns\r\n        # e.g., "it" might refer to the last mentioned object\r\n        pass\n'})}),"\n",(0,o.jsx)(e.h2,{id:"context-aware-instruction-interpretation",children:"Context-Aware Instruction Interpretation"}),"\n",(0,o.jsx)(e.h3,{id:"environmental-context-modeling",children:"Environmental Context Modeling"}),"\n",(0,o.jsx)(e.p,{children:"Context-aware systems consider environmental information when interpreting instructions:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class ContextAwareInterpreter:\r\n    def __init__(self):\r\n        self.object_detector = ObjectDetectionSystem()\r\n        self.spatial_reasoner = SpatialReasoner()\r\n        self.context_memory = ContextMemory()\r\n\r\n    def interpret_instruction(self, instruction, current_context):\r\n        """Interpret instruction considering environmental context"""\r\n        # Get current environmental state\r\n        environment_state = self.get_environment_state(current_context)\r\n\r\n        # Parse instruction with context\r\n        parsed_instruction = self.parse_with_context(instruction, environment_state)\r\n\r\n        # Resolve ambiguities using context\r\n        resolved_instruction = self.resolve_ambiguities(\r\n            parsed_instruction, environment_state\r\n        )\r\n\r\n        return resolved_instruction\r\n\r\n    def get_environment_state(self, context):\r\n        """Get current environmental state"""\r\n        state = {\r\n            \'visible_objects\': self.object_detector.detect(context[\'image\']),\r\n            \'robot_pose\': context[\'robot_pose\'],\r\n            \'navigation_map\': context[\'map\'],\r\n            \'recent_interactions\': self.context_memory.get_recent()\r\n        }\r\n        return state\r\n\r\n    def parse_with_context(self, instruction, environment_state):\r\n        """Parse instruction using environmental context"""\r\n        # Example: "the red ball" -> specific red ball in view\r\n        parsed = self.semantic_parser.parse(instruction)\r\n\r\n        if \'object\' in parsed and \'the\' in instruction:\r\n            # Resolve definite reference using visual context\r\n            specific_object = self.resolve_object_reference(\r\n                parsed[\'object\'], environment_state\r\n            )\r\n            parsed[\'specific_object\'] = specific_object\r\n\r\n        return parsed\r\n\r\n    def resolve_ambiguities(self, parsed_instruction, environment_state):\r\n        """Resolve ambiguities using environmental context"""\r\n        # Resolve spatial prepositions\r\n        if \'location\' in parsed_instruction:\r\n            resolved_location = self.spatial_reasoner.resolve_location(\r\n                parsed_instruction[\'location\'], environment_state\r\n            )\r\n            parsed_instruction[\'resolved_location\'] = resolved_location\r\n\r\n        # Resolve object references\r\n        if \'specific_object\' in parsed_instruction:\r\n            # Ensure the referenced object is accessible\r\n            if not self.is_object_accessible(\r\n                parsed_instruction[\'specific_object\'], environment_state\r\n            ):\r\n                raise ValueError("Referenced object is not accessible")\r\n\r\n        return parsed_instruction\r\n\r\n    def resolve_object_reference(self, object_type, environment_state):\r\n        """Resolve object reference to specific instance"""\r\n        visible_objects = environment_state[\'visible_objects\']\r\n\r\n        # Filter by type\r\n        matching_objects = [\r\n            obj for obj in visible_objects\r\n            if obj[\'type\'] == object_type\r\n        ]\r\n\r\n        if len(matching_objects) == 1:\r\n            return matching_objects[0]\r\n        elif len(matching_objects) > 1:\r\n            # Need additional disambiguation\r\n            return self.disambiguate_objects(matching_objects, environment_state)\r\n        else:\r\n            # Object not visible, might be in memory\r\n            return self.search_memory(object_type)\r\n\r\n    def disambiguate_objects(self, objects, environment_state):\r\n        """Disambiguate between multiple objects of the same type"""\r\n        # Use spatial relationships, colors, sizes, etc. for disambiguation\r\n        # This would implement more sophisticated disambiguation logic\r\n        pass\r\n\r\n    def is_object_accessible(self, object_info, environment_state):\r\n        """Check if object is accessible to robot"""\r\n        # Check if object is within reach, not obstructed, etc.\r\n        robot_pose = environment_state[\'robot_pose\']\r\n        object_pose = object_info[\'pose\']\r\n\r\n        distance = self.calculate_distance(robot_pose, object_pose)\r\n        return distance < self.robot_reach_distance\r\n\r\n    def calculate_distance(self, pose1, pose2):\r\n        """Calculate distance between two poses"""\r\n        # Implementation would calculate 3D distance\r\n        pass\n'})}),"\n",(0,o.jsx)(e.h3,{id:"temporal-context-and-history",children:"Temporal Context and History"}),"\n",(0,o.jsx)(e.p,{children:"Maintaining interaction history helps with reference resolution and task continuity:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class ContextMemory:\r\n    def __init__(self, max_history=50):\r\n        self.max_history = max_history\r\n        self.interaction_history = []\r\n        self.object_references = {}  # Track object mentions\r\n        self.location_references = {}  # Track location mentions\r\n\r\n    def add_interaction(self, instruction, action, result):\r\n        \"\"\"Add interaction to history\"\"\"\r\n        interaction = {\r\n            'timestamp': time.time(),\r\n            'instruction': instruction,\r\n            'action': action,\r\n            'result': result,\r\n            'objects_mentioned': self.extract_objects(instruction)\r\n        }\r\n\r\n        self.interaction_history.append(interaction)\r\n\r\n        # Maintain history size\r\n        if len(self.interaction_history) > self.max_history:\r\n            self.interaction_history.pop(0)\r\n\r\n    def get_recent(self, n=5):\r\n        \"\"\"Get n most recent interactions\"\"\"\r\n        return self.interaction_history[-n:]\r\n\r\n    def resolve_reference(self, reference, current_context):\r\n        \"\"\"Resolve linguistic reference using context and history\"\"\"\r\n        if reference in ['it', 'this', 'that']:\r\n            # Resolve pronoun to most recently mentioned object\r\n            recent_objects = self.get_recent_objects()\r\n            if recent_objects:\r\n                return recent_objects[-1]\r\n\r\n        elif reference in ['there', 'here']:\r\n            # Resolve spatial reference\r\n            if reference == 'here':\r\n                return current_context['robot_pose']\r\n            elif reference == 'there':\r\n                # 'There' often refers to location mentioned in same instruction\r\n                # or location pointed to by demonstrative\r\n                pass\r\n\r\n        return None\r\n\r\n    def get_recent_objects(self):\r\n        \"\"\"Get recently mentioned objects\"\"\"\r\n        recent_objects = []\r\n        for interaction in reversed(self.interaction_history[-10:]):\r\n            if 'objects_mentioned' in interaction:\r\n                recent_objects.extend(interaction['objects_mentioned'])\r\n        return recent_objects\r\n\r\n    def extract_objects(self, instruction):\r\n        \"\"\"Extract object references from instruction\"\"\"\r\n        # Implementation would use NLP to identify objects\r\n        pass\r\n\r\n    def update_object_location(self, object_id, new_location):\r\n        \"\"\"Update object location in memory\"\"\"\r\n        if object_id in self.object_references:\r\n            self.object_references[object_id]['location'] = new_location\r\n            self.object_references[object_id]['last_seen'] = time.time()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"instruction-to-action-mapping",children:"Instruction-to-Action Mapping"}),"\n",(0,o.jsx)(e.h3,{id:"action-space-mapping",children:"Action Space Mapping"}),"\n",(0,o.jsx)(e.p,{children:"Converting parsed instructions to robot actions requires careful mapping:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class InstructionToActionMapper:\r\n    def __init__(self):\r\n        self.action_space = self.define_action_space()\r\n        self.instruction_templates = self.load_instruction_templates()\r\n\r\n    def define_action_space(self):\r\n        \"\"\"Define the robot's action space\"\"\"\r\n        return {\r\n            'navigation': {\r\n                'move_forward': {'params': ['distance']},\r\n                'turn_left': {'params': ['angle']},\r\n                'turn_right': {'params': ['angle']},\r\n                'navigate_to': {'params': ['location', 'orientation']}\r\n            },\r\n            'manipulation': {\r\n                'grasp': {'params': ['object', 'grasp_type']},\r\n                'place': {'params': ['location', 'orientation']},\r\n                'pick_and_place': {'params': ['source', 'target']},\r\n                'push': {'params': ['object', 'direction', 'force']},\r\n                'pull': {'params': ['object', 'direction', 'force']}\r\n            },\r\n            'perception': {\r\n                'look_at': {'params': ['location']},\r\n                'find_object': {'params': ['object_type']},\r\n                'scan_area': {'params': ['location', 'radius']}\r\n            }\r\n        }\r\n\r\n    def map_instruction_to_action(self, parsed_instruction, context):\r\n        \"\"\"Map parsed instruction to executable action\"\"\"\r\n        instruction_type = self.classify_instruction(parsed_instruction)\r\n\r\n        if instruction_type == 'navigation':\r\n            return self.map_navigation_instruction(parsed_instruction, context)\r\n        elif instruction_type == 'manipulation':\r\n            return self.map_manipulation_instruction(parsed_instruction, context)\r\n        elif instruction_type == 'perception':\r\n            return self.map_perception_instruction(parsed_instruction, context)\r\n        else:\r\n            return self.handle_unknown_instruction(parsed_instruction)\r\n\r\n    def map_navigation_instruction(self, parsed_instruction, context):\r\n        \"\"\"Map navigation-related instructions to actions\"\"\"\r\n        action_type = parsed_instruction.get('action_type', 'navigate_to')\r\n\r\n        if action_type in ['go', 'move', 'navigate', 'walk']:\r\n            target_location = parsed_instruction.get('location')\r\n            if target_location:\r\n                # Resolve location to specific coordinates\r\n                resolved_location = self.resolve_location(target_location, context)\r\n                return {\r\n                    'action': 'navigate_to',\r\n                    'parameters': {\r\n                        'target_pose': resolved_location,\r\n                        'avoid_obstacles': True\r\n                    }\r\n                }\r\n\r\n        elif action_type in ['forward', 'backward', 'left', 'right']:\r\n            # Relative movement\r\n            distance = parsed_instruction.get('distance', 1.0)\r\n            return {\r\n                'action': f'move_{action_type}',\r\n                'parameters': {'distance': distance}\r\n            }\r\n\r\n    def map_manipulation_instruction(self, parsed_instruction, context):\r\n        \"\"\"Map manipulation-related instructions to actions\"\"\"\r\n        action_type = parsed_instruction.get('action_type', 'grasp')\r\n\r\n        if action_type in ['grasp', 'pick', 'grab', 'take']:\r\n            object_to_grasp = parsed_instruction.get('object')\r\n            if object_to_grasp:\r\n                return {\r\n                    'action': 'grasp',\r\n                    'parameters': {\r\n                        'object': object_to_grasp,\r\n                        'grasp_type': 'precision'\r\n                    }\r\n                }\r\n\r\n        elif action_type in ['place', 'put', 'set', 'drop']:\r\n            target_location = parsed_instruction.get('location', 'default')\r\n            return {\r\n                'action': 'place',\r\n                'parameters': {\r\n                    'location': target_location,\r\n                    'orientation': 'default'\r\n                }\r\n            }\r\n\r\n    def resolve_location(self, location_desc, context):\r\n        \"\"\"Resolve location description to coordinates\"\"\"\r\n        # This would use spatial reasoning and mapping\r\n        if location_desc in context['known_locations']:\r\n            return context['known_locations'][location_desc]\r\n        else:\r\n            # Use spatial reasoning to find location\r\n            return self.spatial_reasoner.find_location(location_desc, context)\r\n\r\n    def classify_instruction(self, parsed_instruction):\r\n        \"\"\"Classify instruction type\"\"\"\r\n        action = parsed_instruction.get('action_type', '').lower()\r\n\r\n        navigation_keywords = ['go', 'move', 'navigate', 'walk', 'drive', 'travel']\r\n        manipulation_keywords = ['grasp', 'pick', 'place', 'put', 'take', 'grab']\r\n        perception_keywords = ['look', 'find', 'see', 'scan', 'search']\r\n\r\n        if any(keyword in action for keyword in navigation_keywords):\r\n            return 'navigation'\r\n        elif any(keyword in action for keyword in manipulation_keywords):\r\n            return 'manipulation'\r\n        elif any(keyword in action for keyword in perception_keywords):\r\n            return 'perception'\r\n        else:\r\n            return 'unknown'\n"})}),"\n",(0,o.jsx)(e.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,o.jsx)(e.h3,{id:"robust-execution-framework",children:"Robust Execution Framework"}),"\n",(0,o.jsx)(e.p,{children:"Handling errors gracefully is crucial for reliable instruction following:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class RobustInstructionExecutor:\r\n    def __init__(self):\r\n        self.error_handlers = self.initialize_error_handlers()\r\n        self.recovery_strategies = self.load_recovery_strategies()\r\n\r\n    def execute_with_error_handling(self, instruction_plan, context):\r\n        \"\"\"Execute instruction plan with error handling\"\"\"\r\n        for i, action in enumerate(instruction_plan):\r\n            try:\r\n                result = self.execute_action(action, context)\r\n\r\n                if not result['success']:\r\n                    # Handle failure\r\n                    recovery_plan = self.generate_recovery_plan(\r\n                        action, result['error'], context\r\n                    )\r\n                    if recovery_plan:\r\n                        self.execute_recovery_plan(recovery_plan, context)\r\n                    else:\r\n                        # Cannot recover, escalate\r\n                        raise InstructionExecutionError(\r\n                            f\"Cannot execute action: {action}, error: {result['error']}\"\r\n                        )\r\n\r\n            except Exception as e:\r\n                # Handle unexpected errors\r\n                recovery_plan = self.handle_unexpected_error(e, context)\r\n                if recovery_plan:\r\n                    self.execute_recovery_plan(recovery_plan, context)\r\n                else:\r\n                    raise\r\n\r\n    def generate_recovery_plan(self, failed_action, error, context):\r\n        \"\"\"Generate recovery plan for failed action\"\"\"\r\n        error_type = self.classify_error(error)\r\n\r\n        if error_type == 'object_not_found':\r\n            # Object not found - try alternative object or ask for clarification\r\n            return self.recovery_object_not_found(failed_action, context)\r\n\r\n        elif error_type == 'navigation_failure':\r\n            # Navigation failed - try alternative path or approach\r\n            return self.recovery_navigation_failure(failed_action, context)\r\n\r\n        elif error_type == 'grasp_failure':\r\n            # Grasp failed - try different grasp or approach\r\n            return self.recovery_grasp_failure(failed_action, context)\r\n\r\n        elif error_type == 'collision_detected':\r\n            # Collision detected - replan or ask for help\r\n            return self.recovery_collision_detected(failed_action, context)\r\n\r\n        return None\r\n\r\n    def recovery_object_not_found(self, action, context):\r\n        \"\"\"Recovery plan when object is not found\"\"\"\r\n        object_type = action['parameters'].get('object')\r\n\r\n        # Strategy 1: Search in alternative locations\r\n        alternative_locations = self.get_alternative_locations(object_type, context)\r\n        if alternative_locations:\r\n            search_actions = []\r\n            for location in alternative_locations:\r\n                search_actions.append({\r\n                    'action': 'navigate_to',\r\n                    'parameters': {'target_pose': location}\r\n                })\r\n                search_actions.append({\r\n                    'action': 'find_object',\r\n                    'parameters': {'object_type': object_type}\r\n                })\r\n\r\n            # Add original action if object found\r\n            search_actions.append(action)\r\n            return search_actions\r\n\r\n        # Strategy 2: Ask for clarification\r\n        return [{\r\n            'action': 'request_clarification',\r\n            'parameters': {\r\n                'message': f\"I couldn't find the {object_type}. Can you help me locate it?\",\r\n                'options': ['show_location', 'different_object', 'cancel_task']\r\n            }\r\n        }]\r\n\r\n    def recovery_navigation_failure(self, action, context):\r\n        \"\"\"Recovery plan when navigation fails\"\"\"\r\n        target_pose = action['parameters']['target_pose']\r\n\r\n        # Strategy 1: Try alternative path\r\n        alternative_paths = self.find_alternative_paths(target_pose, context)\r\n        if alternative_paths:\r\n            return [{\r\n                'action': 'navigate_to',\r\n                'parameters': {'target_pose': alternative_paths[0]}\r\n            }]\r\n\r\n        # Strategy 2: Approach more carefully\r\n        return [{\r\n            'action': 'navigate_to',\r\n            'parameters': {\r\n                'target_pose': target_pose,\r\n                'speed': 'slow',\r\n                'safety_margin': 'high'\r\n            }\r\n        }]\r\n\r\n    def recovery_grasp_failure(self, action, context):\r\n        \"\"\"Recovery plan when grasp fails\"\"\"\r\n        object_info = action['parameters']['object']\r\n\r\n        # Strategy 1: Try different grasp approach\r\n        grasp_strategies = [\r\n            'top_grasp', 'side_grasp', 'pinch_grasp', 'power_grasp'\r\n        ]\r\n\r\n        recovery_actions = []\r\n        for strategy in grasp_strategies[1:]:  # Skip original strategy\r\n            recovery_actions.append({\r\n                'action': 'grasp',\r\n                'parameters': {\r\n                    'object': object_info,\r\n                    'grasp_type': strategy\r\n                }\r\n            })\r\n\r\n        return recovery_actions\r\n\r\n    def classify_error(self, error):\r\n        \"\"\"Classify error type\"\"\"\r\n        error_msg = str(error).lower()\r\n\r\n        if 'not found' in error_msg or 'missing' in error_msg:\r\n            return 'object_not_found'\r\n        elif 'navigation' in error_msg or 'path' in error_msg or 'obstacle' in error_msg:\r\n            return 'navigation_failure'\r\n        elif 'grasp' in error_msg or 'gripper' in error_msg or 'slip' in error_msg:\r\n            return 'grasp_failure'\r\n        elif 'collision' in error_msg or 'crash' in error_msg:\r\n            return 'collision_detected'\r\n        else:\r\n            return 'unknown_error'\r\n\r\n    def handle_unexpected_error(self, error, context):\r\n        \"\"\"Handle unexpected errors\"\"\"\r\n        error_msg = f\"Unexpected error occurred: {error}\"\r\n\r\n        # Log error for debugging\r\n        self.log_error(error, context)\r\n\r\n        # Ask for human assistance\r\n        return [{\r\n            'action': 'request_assistance',\r\n            'parameters': {\r\n                'message': error_msg,\r\n                'context': context\r\n            }\r\n        }]\r\n\r\n    def log_error(self, error, context):\r\n        \"\"\"Log error for debugging and improvement\"\"\"\r\n        error_log = {\r\n            'timestamp': time.time(),\r\n            'error': str(error),\r\n            'context': context,\r\n            'stack_trace': traceback.format_exc()\r\n        }\r\n\r\n        # Save to persistent storage for analysis\r\n        self.save_error_log(error_log)\n"})}),"\n",(0,o.jsx)(e.h2,{id:"isaac-integration-for-instruction-following",children:"Isaac Integration for Instruction Following"}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-interface-for-instruction-following",children:"ROS 2 Interface for Instruction Following"}),"\n",(0,o.jsx)(e.p,{children:"Integrating instruction following with ROS 2 and Isaac systems:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Pose, Twist\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom action_msgs.msg import GoalStatus\r\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\n\r\nclass IsaacInstructionFollowerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_instruction_follower\')\r\n\r\n        # Publishers\r\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'cmd_vel\', 10)\r\n        self.status_publisher = self.create_publisher(String, \'instruction_status\', 10)\r\n\r\n        # Subscribers\r\n        self.instruction_subscriber = self.create_subscription(\r\n            String, \'natural_language_instruction\', self.instruction_callback, 10\r\n        )\r\n\r\n        # Action server for complex instructions\r\n        self.instruction_server = ActionServer(\r\n            self,\r\n            ExecuteInstruction,\r\n            \'execute_instruction\',\r\n            self.execute_instruction_callback,\r\n            goal_callback=self.goal_callback,\r\n            cancel_callback=self.cancel_callback\r\n        )\r\n\r\n        # Initialize instruction following system\r\n        self.instruction_system = HierarchicalInstructionFollower()\r\n        self.context_provider = ContextProvider()\r\n\r\n        self.get_logger().info(\'Isaac Instruction Follower initialized\')\r\n\r\n    def instruction_callback(self, msg):\r\n        """Handle incoming natural language instruction"""\r\n        try:\r\n            self.get_logger().info(f\'Received instruction: {msg.data}\')\r\n\r\n            # Publish status\r\n            status_msg = String()\r\n            status_msg.data = f\'Processing: {msg.data}\'\r\n            self.status_publisher.publish(status_msg)\r\n\r\n            # Get current context\r\n            context = self.context_provider.get_current_context()\r\n\r\n            # Follow instruction\r\n            result = self.instruction_system.follow_instruction(msg.data, context)\r\n\r\n            # Publish completion status\r\n            status_msg.data = f\'Completed: {msg.data}\'\r\n            self.status_publisher.publish(status_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error following instruction: {e}\')\r\n            status_msg = String()\r\n            status_msg.data = f\'Error: {str(e)}\'\r\n            self.status_publisher.publish(status_msg)\r\n\r\n    def goal_callback(self, goal_request):\r\n        """Handle instruction execution goal"""\r\n        self.get_logger().info(f\'Received instruction goal: {goal_request.instruction}\')\r\n        return GoalResponse.ACCEPT\r\n\r\n    def cancel_callback(self, goal_handle):\r\n        """Handle instruction execution cancellation"""\r\n        self.get_logger().info(\'Instruction execution cancelled\')\r\n        return CancelResponse.ACCEPT\r\n\r\n    def execute_instruction_callback(self, goal_handle):\r\n        """Execute instruction as action"""\r\n        feedback_msg = ExecuteInstruction.Feedback()\r\n        result_msg = ExecuteInstruction.Result()\r\n\r\n        try:\r\n            instruction = goal_handle.request.instruction\r\n            self.get_logger().info(f\'Executing instruction: {instruction}\')\r\n\r\n            # Get context\r\n            context = self.context_provider.get_current_context()\r\n\r\n            # Execute with progress feedback\r\n            instruction_plan = self.instruction_system.task_planner.decompose(\r\n                instruction, context\r\n            )\r\n\r\n            total_steps = len(instruction_plan)\r\n            completed_steps = 0\r\n\r\n            for i, subtask in enumerate(instruction_plan):\r\n                # Update feedback\r\n                feedback_msg.current_task = str(subtask)\r\n                feedback_msg.progress = (i + 1) / total_steps * 100.0\r\n                goal_handle.publish_feedback(feedback_msg)\r\n\r\n                # Execute subtask\r\n                action_sequence = self.instruction_system.action_planner.plan(\r\n                    subtask, context\r\n                )\r\n\r\n                for action in action_sequence:\r\n                    motion_commands = self.instruction_system.motion_planner.generate(\r\n                        action, context\r\n                    )\r\n                    self.execute_motion(motion_commands)\r\n\r\n                completed_steps += 1\r\n\r\n            # Complete successfully\r\n            result_msg.success = True\r\n            result_msg.message = f\'Successfully executed: {instruction}\'\r\n            goal_handle.succeed()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Instruction execution failed: {e}\')\r\n            result_msg.success = False\r\n            result_msg.message = f\'Failed: {str(e)}\'\r\n            goal_handle.abort()\r\n\r\n        return result_msg\r\n\r\n    def execute_motion(self, motion_commands):\r\n        """Execute motion commands"""\r\n        # Publish velocity commands or joint trajectories\r\n        # This would interface with robot controllers\r\n        pass\r\n\r\nclass ContextProvider:\r\n    def __init__(self):\r\n        # This would interface with perception and mapping systems\r\n        pass\r\n\r\n    def get_current_context(self):\r\n        """Get current environmental context"""\r\n        # This would gather information from various sensors and systems\r\n        context = {\r\n            \'robot_pose\': self.get_robot_pose(),\r\n            \'map\': self.get_current_map(),\r\n            \'objects\': self.get_detected_objects(),\r\n            \'time\': time.time()\r\n        }\r\n        return context\r\n\r\n    def get_robot_pose(self):\r\n        """Get current robot pose"""\r\n        # Implementation would use localization system\r\n        pass\r\n\r\n    def get_current_map(self):\r\n        """Get current navigation map"""\r\n        # Implementation would use mapping system\r\n        pass\r\n\r\n    def get_detected_objects(self):\r\n        """Get currently detected objects"""\r\n        # Implementation would use perception system\r\n        pass\r\n\r\n# Action definition (would be in .action file)\r\nclass ExecuteInstruction:\r\n    def __init__(self):\r\n        self.instruction = ""\r\n        self.timeout = 0.0\r\n\r\n    class Feedback:\r\n        def __init__(self):\r\n            self.current_task = ""\r\n            self.progress = 0.0\r\n\r\n    class Result:\r\n        def __init__(self):\r\n            self.success = False\r\n            self.message = ""\n'})}),"\n",(0,o.jsx)(e.h2,{id:"evaluation-and-validation",children:"Evaluation and Validation"}),"\n",(0,o.jsx)(e.h3,{id:"instruction-following-benchmarks",children:"Instruction Following Benchmarks"}),"\n",(0,o.jsx)(e.p,{children:"Evaluating instruction following systems requires comprehensive benchmarks:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class InstructionFollowingEvaluator:\r\n    def __init__(self, instruction_follower):\r\n        self.instruction_follower = instruction_follower\r\n        self.metrics = []\r\n\r\n    def evaluate_on_dataset(self, dataset):\r\n        """Evaluate instruction follower on dataset"""\r\n        results = {\r\n            \'success_rate\': 0.0,\r\n            \'average_time\': 0.0,\r\n            \'error_types\': {},\r\n            \'context_usage\': 0.0\r\n        }\r\n\r\n        total_tasks = len(dataset)\r\n        successful_tasks = 0\r\n        total_time = 0.0\r\n\r\n        for instruction, expected_result, context in dataset:\r\n            start_time = time.time()\r\n\r\n            try:\r\n                actual_result = self.instruction_follower.follow_instruction(\r\n                    instruction, context\r\n                )\r\n\r\n                if self.evaluate_result(actual_result, expected_result):\r\n                    successful_tasks += 1\r\n                    execution_time = time.time() - start_time\r\n                    total_time += execution_time\r\n                else:\r\n                    self.record_error(\'incorrect_result\', instruction, context)\r\n\r\n            except Exception as e:\r\n                self.record_error(\'execution_error\', instruction, str(e))\r\n\r\n        results[\'success_rate\'] = successful_tasks / total_tasks if total_tasks > 0 else 0\r\n        results[\'average_time\'] = total_time / successful_tasks if successful_tasks > 0 else 0\r\n\r\n        return results\r\n\r\n    def evaluate_result(self, actual, expected):\r\n        """Evaluate if actual result matches expected result"""\r\n        # This would implement domain-specific evaluation\r\n        # For navigation: check if robot reached correct location\r\n        # For manipulation: check if object was moved correctly\r\n        # For complex tasks: check if overall goal was achieved\r\n        pass\r\n\r\n    def evaluate_context_usage(self, instruction, context):\r\n        """Evaluate if system properly uses contextual information"""\r\n        # Test if system can resolve ambiguities using context\r\n        ambiguous_instruction = "Grasp it"\r\n\r\n        # System should use context to determine what "it" refers to\r\n        result = self.instruction_follower.follow_instruction(ambiguous_instruction, context)\r\n\r\n        # Check if the correct object was grasped based on context\r\n        correct_resolution = self.check_context_resolution(result, context)\r\n        return correct_resolution\r\n\r\n    def test_robustness(self, instruction_follower):\r\n        """Test robustness to various challenges"""\r\n        robustness_tests = {\r\n            \'noise_resilience\': self.test_noise_resilience,\r\n            \'ambiguity_resolution\': self.test_ambiguity_resolution,\r\n            \'error_recovery\': self.test_error_recovery,\r\n            \'temporal_coherence\': self.test_temporal_coherence\r\n        }\r\n\r\n        results = {}\r\n        for test_name, test_func in robustness_tests.items():\r\n            results[test_name] = test_func(instruction_follower)\r\n\r\n        return results\r\n\r\n    def test_ambiguity_resolution(self, instruction_follower):\r\n        """Test ability to resolve ambiguous instructions"""\r\n        ambiguous_cases = [\r\n            ("Bring the cup", {"cups": [{"id": 1, "color": "red"}, {"id": 2, "color": "blue"}]}),\r\n            ("Go there", {"pointed_location": [1.0, 2.0, 0.0]}),\r\n            ("Do it again", {"previous_action": "pick_object"})\r\n        ]\r\n\r\n        resolved_correctly = 0\r\n        for instruction, context in ambiguous_cases:\r\n            try:\r\n                result = instruction_follower.follow_instruction(instruction, context)\r\n                if self.check_ambiguity_resolution(result, instruction, context):\r\n                    resolved_correctly += 1\r\n            except:\r\n                pass  # Count as failure\r\n\r\n        return resolved_correctly / len(ambiguous_cases)\r\n\r\n# Example usage\r\ndef main():\r\n    # Initialize instruction follower\r\n    instruction_follower = HierarchicalInstructionFollower()\r\n\r\n    # Create evaluator\r\n    evaluator = InstructionFollowingEvaluator(instruction_follower)\r\n\r\n    # Run evaluation\r\n    results = evaluator.evaluate_on_dataset(test_dataset)\r\n\r\n    print(f"Success Rate: {results[\'success_rate\']:.2%}")\r\n    print(f"Average Time: {results[\'average_time\']:.2f}s")\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"Instruction following and task planning form the cognitive core of Vision-Language-Action systems, enabling robots to interpret natural language commands and execute them as sequences of physical actions. Through hierarchical planning, semantic parsing, context awareness, and robust error handling, these systems bridge the gap between human communication and robotic execution."}),"\n",(0,o.jsx)(e.p,{children:"The next section will explore embodied language models, which connect abstract language concepts to concrete physical experiences and robotic capabilities."}),"\n",(0,o.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,o.jsx)(e.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function _(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(u,{...n})}):u(n)}}}]);