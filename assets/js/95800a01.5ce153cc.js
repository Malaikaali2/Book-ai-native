"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[2065],{6752(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"capstone/voice-processing","title":"Voice Command Processing Implementation: Autonomous Humanoid Capstone","description":"Overview","source":"@site/docs/capstone/voice-processing.md","sourceDirName":"capstone","slug":"/capstone/voice-processing","permalink":"/Book-ai-native/docs/capstone/voice-processing","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/capstone/voice-processing.md","tags":[],"version":"current","sidebarPosition":16,"frontMatter":{"sidebar_position":16},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Project: The Autonomous Humanoid - Introduction and Requirements","permalink":"/Book-ai-native/docs/capstone/intro"},"next":{"title":"Task Planning and Execution: Autonomous Humanoid Capstone","permalink":"/Book-ai-native/docs/capstone/task-planning"}}');var i=r(4848),s=r(8453);const o={sidebar_position:16},a="Voice Command Processing Implementation: Autonomous Humanoid Capstone",c={},l=[{value:"Overview",id:"overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Voice Processing Pipeline",id:"voice-processing-pipeline",level:3},{value:"Component Integration",id:"component-integration",level:3},{value:"Technical Implementation",id:"technical-implementation",level:2},{value:"1. Speech Recognition Integration",id:"1-speech-recognition-integration",level:3},{value:"Speech-to-Text Setup",id:"speech-to-text-setup",level:4},{value:"Performance Optimization",id:"performance-optimization",level:4},{value:"2. Natural Language Processing",id:"2-natural-language-processing",level:3},{value:"Command Parsing",id:"command-parsing",level:4},{value:"Context Management",id:"context-management",level:4},{value:"3. Intent Classification and Action Mapping",id:"3-intent-classification-and-action-mapping",level:3},{value:"Intent Classification System",id:"intent-classification-system",level:4},{value:"Action Mapping System",id:"action-mapping-system",level:4},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Set up Speech Recognition Environment",id:"step-1-set-up-speech-recognition-environment",level:3},{value:"Step 2: Implement Core Processing Classes",id:"step-2-implement-core-processing-classes",level:3},{value:"Step 3: Performance Optimization",id:"step-3-performance-optimization",level:3},{value:"Step 4: Error Handling and Robustness",id:"step-4-error-handling-and-robustness",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Testing",id:"unit-testing",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Response Time Requirements",id:"response-time-requirements",level:3},{value:"Accuracy Requirements",id:"accuracy-requirements",level:3},{value:"Resource Usage",id:"resource-usage",level:3},{value:"Troubleshooting and Common Issues",id:"troubleshooting-and-common-issues",level:2},{value:"Audio Quality Issues",id:"audio-quality-issues",level:3},{value:"Recognition Problems",id:"recognition-problems",level:3},{value:"Integration Issues",id:"integration-issues",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Security Considerations",id:"security-considerations",level:3},{value:"Accessibility",id:"accessibility",level:3},{value:"Maintainability",id:"maintainability",level:3},{value:"Next Steps and Integration",id:"next-steps-and-integration",level:2},{value:"Integration with Other Capstone Components",id:"integration-with-other-capstone-components",level:3},{value:"Advanced Features",id:"advanced-features",level:3},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-command-processing-implementation-autonomous-humanoid-capstone",children:"Voice Command Processing Implementation: Autonomous Humanoid Capstone"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Voice command processing forms the primary interaction interface for the autonomous humanoid system, enabling natural language communication between users and the robot. This component is responsible for converting spoken commands into structured actions that the robot can understand and execute. The implementation integrates speech recognition, natural language processing, and intent classification to create an intuitive human-robot interaction system."}),"\n",(0,i.jsx)(n.p,{children:'The voice processing pipeline must handle various command types, from simple navigation requests ("Go to the kitchen") to complex manipulation tasks ("Pick up the red cup and place it on the table"). This implementation guide provides detailed instructions for building a robust voice command processing system that meets the performance and accuracy requirements of the capstone project.'}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"voice-processing-pipeline",children:"Voice Processing Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"The voice command processing system follows a multi-stage pipeline architecture:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Audio Input \u2192 Speech Recognition \u2192 Natural Language Processing \u2192 Intent Classification \u2192 Action Planning \u2192 Command Execution\n"})}),"\n",(0,i.jsx)(n.p,{children:"Each stage performs specific processing tasks:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Processing"}),": Capture and preprocess audio input for recognition"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech Recognition"}),": Convert speech to text using ASR (Automatic Speech Recognition)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding"}),": Parse text for intent and parameters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Processing"}),": Apply contextual knowledge to disambiguate commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Mapping"}),": Map understood commands to robot action primitives"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"component-integration",children:"Component Integration"}),"\n",(0,i.jsx)(n.p,{children:"The voice processing system integrates with other capstone components:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Interface"}),": Publishes commands to action servers and navigation system"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception System"}),": Requests object detection and localization for manipulation tasks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Planning"}),": Provides high-level goals to the planning system"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation System"}),": Issues navigation goals for location-based commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation System"}),": Sends manipulation parameters for object interaction"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"1-speech-recognition-integration",children:"1. Speech Recognition Integration"}),"\n",(0,i.jsx)(n.h4,{id:"speech-to-text-setup",children:"Speech-to-Text Setup"}),"\n",(0,i.jsx)(n.p,{children:"The speech recognition component uses either cloud-based services (Google Speech-to-Text, Azure Cognitive Services) or on-premise solutions (Vosk, SpeechRecognition with CMU Sphinx):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\r\nimport rospy\r\nfrom std_msgs.msg import String\r\n\r\nclass SpeechRecognizer:\r\n    def __init__(self):\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        self.command_publisher = rospy.Publisher(\'/voice_commands\', String, queue_size=10)\r\n\r\n        # Adjust for ambient noise\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n\r\n    def listen_for_command(self):\r\n        """Listen for voice command and return recognized text"""\r\n        try:\r\n            with self.microphone as source:\r\n                audio = self.recognizer.listen(source, timeout=5.0)\r\n\r\n            # Use Google Speech Recognition (requires internet)\r\n            command_text = self.recognizer.recognize_google(audio)\r\n            return command_text\r\n        except sr.WaitTimeoutError:\r\n            rospy.loginfo("No speech detected within timeout")\r\n            return None\r\n        except sr.UnknownValueError:\r\n            rospy.loginfo("Could not understand audio")\r\n            return None\r\n        except sr.RequestError as e:\r\n            rospy.logerr(f"Speech recognition error: {e}")\r\n            return None\n'})}),"\n",(0,i.jsx)(n.h4,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.p,{children:"For real-time performance, implement continuous listening with wake word detection:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class WakeWordDetector:\r\n    def __init__(self, wake_word="robot"):\r\n        self.wake_word = wake_word.lower()\r\n        self.is_listening = False\r\n\r\n    def detect_wake_word(self, audio_text):\r\n        """Detect wake word to activate full processing"""\r\n        if self.wake_word in audio_text.lower():\r\n            return True\r\n        return False\r\n\r\nclass ContinuousVoiceProcessor:\r\n    def __init__(self):\r\n        self.speech_recognizer = SpeechRecognizer()\r\n        self.wake_detector = WakeWordDetector()\r\n        self.nlp_processor = NaturalLanguageProcessor()\r\n\r\n    def continuous_processing_loop(self):\r\n        """Main processing loop for voice commands"""\r\n        while not rospy.is_shutdown():\r\n            # Listen for wake word\r\n            if not self.wake_detector.is_listening:\r\n                audio_text = self.speech_recognizer.listen_for_command()\r\n                if audio_text and self.wake_detector.detect_wake_word(audio_text):\r\n                    self.wake_detector.is_listening = True\r\n                    rospy.loginfo("Wake word detected - ready for command")\r\n            else:\r\n                # Process full command\r\n                command_text = self.speech_recognizer.listen_for_command()\r\n                if command_text:\r\n                    self.process_command(command_text)\r\n                    self.wake_detector.is_listening = False\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-natural-language-processing",children:"2. Natural Language Processing"}),"\n",(0,i.jsx)(n.h4,{id:"command-parsing",children:"Command Parsing"}),"\n",(0,i.jsx)(n.p,{children:"The NLP component parses recognized text to extract intent and parameters:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import spacy\r\nimport re\r\nfrom typing import Dict, List, Tuple\r\n\r\nclass NaturalLanguageProcessor:\r\n    def __init__(self):\r\n        # Load spaCy model for English\r\n        try:\r\n            self.nlp = spacy.load(\"en_core_web_sm\")\r\n        except OSError:\r\n            rospy.logerr(\"spaCy English model not found. Install with: python -m spacy download en_core_web_sm\")\r\n            raise\r\n\r\n    def parse_command(self, text: str) -> Dict:\r\n        \"\"\"Parse command text and extract structured information\"\"\"\r\n        doc = self.nlp(text)\r\n\r\n        # Extract intent based on action verbs\r\n        intent = self.extract_intent(doc)\r\n\r\n        # Extract objects and locations\r\n        entities = self.extract_entities(doc)\r\n\r\n        # Extract quantities and modifiers\r\n        modifiers = self.extract_modifiers(doc)\r\n\r\n        return {\r\n            'intent': intent,\r\n            'entities': entities,\r\n            'modifiers': modifiers,\r\n            'original_text': text\r\n        }\r\n\r\n    def extract_intent(self, doc) -> str:\r\n        \"\"\"Extract the primary intent from the command\"\"\"\r\n        # Define common action patterns\r\n        navigation_patterns = ['go', 'move', 'navigate', 'walk', 'drive', 'travel', 'go to', 'move to']\r\n        manipulation_patterns = ['pick', 'grab', 'take', 'lift', 'place', 'put', 'set', 'move', 'bring', 'get']\r\n        query_patterns = ['where', 'what', 'find', 'locate', 'show', 'tell']\r\n\r\n        # Check for action verbs\r\n        for token in doc:\r\n            if token.lemma_ in navigation_patterns:\r\n                return 'NAVIGATE'\r\n            elif token.lemma_ in manipulation_patterns:\r\n                return 'MANIPULATE'\r\n            elif token.lemma_ in query_patterns:\r\n                return 'QUERY'\r\n\r\n        # Default to navigation if no clear intent\r\n        return 'NAVIGATE'\r\n\r\n    def extract_entities(self, doc) -> Dict:\r\n        \"\"\"Extract named entities (objects, locations, people)\"\"\"\r\n        entities = {\r\n            'objects': [],\r\n            'locations': [],\r\n            'people': []\r\n        }\r\n\r\n        # Look for objects based on dependencies and POS tags\r\n        for token in doc:\r\n            if token.pos_ in ['NOUN', 'PROPN'] and token.dep_ in ['dobj', 'pobj', 'attr']:\r\n                # Check if it's a location\r\n                location_keywords = ['kitchen', 'living room', 'bedroom', 'office', 'table', 'couch', 'door', 'window']\r\n                if any(keyword in token.text.lower() for keyword in location_keywords):\r\n                    entities['locations'].append(token.text)\r\n                else:\r\n                    entities['objects'].append(token.text)\r\n\r\n        # Extract colors and other descriptors\r\n        for token in doc:\r\n            if token.pos_ == 'ADJ':\r\n                # Look for color adjectives near objects\r\n                for child in token.children:\r\n                    if child.pos_ in ['NOUN', 'PROPN']:\r\n                        # This adjective likely describes the noun\r\n                        entities['objects'].append(f\"{token.text} {child.text}\")\r\n\r\n        return entities\n"})}),"\n",(0,i.jsx)(n.h4,{id:"context-management",children:"Context Management"}),"\n",(0,i.jsx)(n.p,{children:"Implement context awareness to handle pronouns and references:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ContextManager:\r\n    def __init__(self):\r\n        self.current_context = {}\r\n        self.conversation_history = []\r\n\r\n    def update_context(self, command_result):\r\n        \"\"\"Update context based on command execution results\"\"\"\r\n        self.current_context.update({\r\n            'last_action': command_result.get('action'),\r\n            'last_location': command_result.get('location'),\r\n            'last_object': command_result.get('object'),\r\n            'timestamp': rospy.get_time()\r\n        })\r\n\r\n    def resolve_pronouns(self, text: str) -> str:\r\n        \"\"\"Resolve pronouns like 'it', 'there', 'here' based on context\"\"\"\r\n        resolved_text = text\r\n\r\n        # Replace 'it' with last mentioned object\r\n        if 'it' in text.lower() and self.current_context.get('last_object'):\r\n            resolved_text = resolved_text.replace('it', self.current_context['last_object'])\r\n\r\n        # Replace 'there' with last location\r\n        if 'there' in text.lower() and self.current_context.get('last_location'):\r\n            resolved_text = resolved_text.replace('there', self.current_context['last_location'])\r\n\r\n        return resolved_text\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-intent-classification-and-action-mapping",children:"3. Intent Classification and Action Mapping"}),"\n",(0,i.jsx)(n.h4,{id:"intent-classification-system",children:"Intent Classification System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class IntentClassifier:\r\n    def __init__(self):\r\n        # Define command patterns with regex\r\n        self.patterns = {\r\n            'NAVIGATE': [\r\n                r'go to (the )?(?P<location>\\w+)',\r\n                r'move to (the )?(?P<location>\\w+)',\r\n                r'go to the (?P<location>\\w+)',\r\n                r'go to (?P<location>\\w+)',\r\n                r'navigate to (the )?(?P<location>\\w+)'\r\n            ],\r\n            'MANIPULATE_PICK': [\r\n                r'pick up (the )?(?P<object>\\w+)',\r\n                r'grab (the )?(?P<object>\\w+)',\r\n                r'take (the )?(?P<object>\\w+)',\r\n                r'get (the )?(?P<object>\\w+)'\r\n            ],\r\n            'MANIPULATE_PLACE': [\r\n                r'place (the )?(?P<object>\\w+) (on|at) (the )?(?P<location>\\w+)',\r\n                r'put (the )?(?P<object>\\w+) (on|at) (the )?(?P<location>\\w+)',\r\n                r'set (the )?(?P<object>\\w+) (on|at) (the )?(?P<location>\\w+)'\r\n            ],\r\n            'QUERY': [\r\n                r'where is (the )?(?P<object>\\w+)',\r\n                r'find (the )?(?P<object>\\w+)',\r\n                r'locate (the )?(?P<object>\\w+)'\r\n            ]\r\n        }\r\n\r\n    def classify_intent(self, parsed_command: Dict) -> Dict:\r\n        \"\"\"Classify intent and extract parameters\"\"\"\r\n        original_text = parsed_command['original_text'].lower()\r\n\r\n        for intent, patterns in self.patterns.items():\r\n            for pattern in patterns:\r\n                match = re.search(pattern, original_text)\r\n                if match:\r\n                    return {\r\n                        'intent': intent,\r\n                        'parameters': match.groupdict(),\r\n                        'confidence': 0.9  # High confidence for pattern matches\r\n                    }\r\n\r\n        # Fallback to NLP-based classification\r\n        return self.fallback_classification(parsed_command)\r\n\r\n    def fallback_classification(self, parsed_command: Dict) -> Dict:\r\n        \"\"\"Fallback classification using NLP analysis\"\"\"\r\n        intent = parsed_command['intent']\r\n        entities = parsed_command['entities']\r\n\r\n        # Determine specific manipulation intent\r\n        if intent == 'MANIPULATE':\r\n            # Look for placement verbs vs pickup verbs\r\n            original_text = parsed_command['original_text'].lower()\r\n            if any(verb in original_text for verb in ['place', 'put', 'set', 'on', 'at']):\r\n                intent = 'MANIPULATE_PLACE'\r\n            else:\r\n                intent = 'MANIPULATE_PICK'\r\n\r\n        # Extract parameters from entities\r\n        parameters = {}\r\n        if entities['objects']:\r\n            parameters['object'] = entities['objects'][0]\r\n        if entities['locations']:\r\n            parameters['location'] = entities['locations'][0]\r\n\r\n        return {\r\n            'intent': intent,\r\n            'parameters': parameters,\r\n            'confidence': 0.7  # Lower confidence for fallback\r\n        }\n"})}),"\n",(0,i.jsx)(n.h4,{id:"action-mapping-system",children:"Action Mapping System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ActionMapper:\r\n    def __init__(self):\r\n        self.navigation_publisher = rospy.Publisher('/move_base/goal', MoveBaseActionGoal, queue_size=10)\r\n        self.manipulation_publisher = rospy.Publisher('/manipulation_commands', String, queue_size=10)\r\n        self.query_publisher = rospy.Publisher('/query_commands', String, queue_size=10)\r\n\r\n    def map_to_action(self, classified_intent: Dict):\r\n        \"\"\"Map classified intent to robot action\"\"\"\r\n        intent = classified_intent['intent']\r\n        parameters = classified_intent['parameters']\r\n\r\n        if intent == 'NAVIGATE':\r\n            self.execute_navigation(parameters)\r\n        elif intent in ['MANIPULATE_PICK', 'MANIPULATE_PLACE']:\r\n            self.execute_manipulation(intent, parameters)\r\n        elif intent == 'QUERY':\r\n            self.execute_query(parameters)\r\n\r\n    def execute_navigation(self, parameters: Dict):\r\n        \"\"\"Execute navigation command\"\"\"\r\n        location = parameters.get('location')\r\n        if not location:\r\n            rospy.logerr(\"No location specified for navigation command\")\r\n            return\r\n\r\n        # Convert location name to coordinates (from map)\r\n        location_coords = self.get_coordinates_for_location(location)\r\n\r\n        if location_coords:\r\n            goal = MoveBaseGoal()\r\n            goal.target_pose.header.frame_id = \"map\"\r\n            goal.target_pose.header.stamp = rospy.Time.now()\r\n            goal.target_pose.pose = location_coords\r\n\r\n            # Publish navigation goal\r\n            self.navigation_publisher.publish(goal)\r\n        else:\r\n            rospy.logerr(f\"Unknown location: {location}\")\r\n\r\n    def execute_manipulation(self, intent: str, parameters: Dict):\r\n        \"\"\"Execute manipulation command\"\"\"\r\n        obj = parameters.get('object')\r\n        location = parameters.get('location')\r\n\r\n        if not obj:\r\n            rospy.logerr(\"No object specified for manipulation command\")\r\n            return\r\n\r\n        # Create manipulation command\r\n        command = {\r\n            'action': 'pick' if 'PICK' in intent else 'place',\r\n            'object': obj,\r\n            'target_location': location if location else None\r\n        }\r\n\r\n        # Publish manipulation command\r\n        self.manipulation_publisher.publish(str(command))\r\n\r\n    def get_coordinates_for_location(self, location_name: str) -> Pose:\r\n        \"\"\"Convert location name to map coordinates\"\"\"\r\n        # This would typically come from a saved map of location coordinates\r\n        location_map = {\r\n            'kitchen': Pose(position=Point(2.0, 1.0, 0.0), orientation=Quaternion(0, 0, 0, 1)),\r\n            'living room': Pose(position=Point(-1.0, 2.0, 0.0), orientation=Quaternion(0, 0, 0, 1)),\r\n            'bedroom': Pose(position=Point(3.0, -2.0, 0.0), orientation=Quaternion(0, 0, 0, 1)),\r\n            'office': Pose(position=Point(-2.0, -1.0, 0.0), orientation=Quaternion(0, 0, 0, 1))\r\n        }\r\n\r\n        return location_map.get(location_name.lower())\n"})}),"\n",(0,i.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-set-up-speech-recognition-environment",children:"Step 1: Set up Speech Recognition Environment"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Install required dependencies:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install speechrecognition pyaudio spacy\r\npython -m spacy download en_core_web_sm\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Test microphone access:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\r\nr = sr.Recognizer()\r\nmic = sr.Microphone()\r\n\r\nprint("Available microphones:")\r\nfor device_index in range(mic.get_device_count()):\r\n    device_info = mic.get_device_info(device_index)\r\n    print(f"Device {device_index}: {device_info[\'name\']}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-implement-core-processing-classes",children:"Step 2: Implement Core Processing Classes"}),"\n",(0,i.jsx)(n.p,{children:"Create the main voice processing node that integrates all components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport speech_recognition as sr\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Pose, Point, Quaternion\r\nfrom move_base_msgs.msg import MoveBaseGoal, MoveBaseActionGoal\r\n\r\nclass VoiceCommandProcessor:\r\n    def __init__(self):\r\n        rospy.init_node('voice_command_processor')\r\n\r\n        # Initialize components\r\n        self.speech_recognizer = SpeechRecognizer()\r\n        self.nlp_processor = NaturalLanguageProcessor()\r\n        self.intent_classifier = IntentClassifier()\r\n        self.action_mapper = ActionMapper()\r\n        self.context_manager = ContextManager()\r\n\r\n        # Publishers and subscribers\r\n        self.status_publisher = rospy.Publisher('/voice_status', String, queue_size=10)\r\n        self.command_subscriber = rospy.Subscriber('/voice_commands', String, self.command_callback)\r\n\r\n        rospy.loginfo(\"Voice command processor initialized\")\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Process incoming voice command\"\"\"\r\n        try:\r\n            # Parse the command\r\n            parsed = self.nlp_processor.parse_command(msg.data)\r\n\r\n            # Classify intent\r\n            classified = self.intent_classifier.classify_intent(parsed)\r\n\r\n            # Check confidence threshold\r\n            if classified['confidence'] < 0.5:\r\n                rospy.logwarn(f\"Low confidence command: {msg.data} (confidence: {classified['confidence']})\")\r\n                return\r\n\r\n            # Map to action\r\n            self.action_mapper.map_to_action(classified)\r\n\r\n            # Update context\r\n            self.context_manager.update_context({\r\n                'command': msg.data,\r\n                'intent': classified['intent'],\r\n                'parameters': classified['parameters']\r\n            })\r\n\r\n            # Publish status\r\n            status_msg = String()\r\n            status_msg.data = f\"Executed command: {msg.data}\"\r\n            self.status_publisher.publish(status_msg)\r\n\r\n        except Exception as e:\r\n            rospy.logerr(f\"Error processing command: {e}\")\r\n\r\n    def start_listening(self):\r\n        \"\"\"Start continuous voice command processing\"\"\"\r\n        rospy.loginfo(\"Starting voice command processing...\")\r\n        rospy.spin()\r\n\r\nif __name__ == '__main__':\r\n    processor = VoiceCommandProcessor()\r\n    processor.start_listening()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-performance-optimization",children:"Step 3: Performance Optimization"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement audio preprocessing for noise reduction:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nfrom scipy import signal\r\n\r\nclass AudioPreprocessor:\r\n    def __init__(self):\r\n        # Design a simple low-pass filter to reduce background noise\r\n        self.b, self.a = signal.butter(4, 0.2, \'low\')\r\n\r\n    def preprocess_audio(self, audio_data):\r\n        """Apply noise reduction to audio data"""\r\n        filtered = signal.filtfilt(self.b, self.a, audio_data)\r\n        return filtered\n'})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Add caching for frequently recognized phrases:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from functools import lru_cache\r\n\r\nclass CachedNLPProcessor(NaturalLanguageProcessor):\r\n    @lru_cache(maxsize=100)\r\n    def parse_command_cached(self, text: str):\r\n        """Cached version of command parsing for frequently used commands"""\r\n        return self.parse_command(text)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-error-handling-and-robustness",children:"Step 4: Error Handling and Robustness"}),"\n",(0,i.jsx)(n.p,{children:"Implement comprehensive error handling:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RobustVoiceProcessor:\r\n    def __init__(self):\r\n        self.max_retries = 3\r\n        self.retry_delay = 1.0\r\n        self.timeout = 5.0\r\n\r\n    def robust_recognition(self):\r\n        """Attempt recognition with retries and fallbacks"""\r\n        for attempt in range(self.max_retries):\r\n            try:\r\n                result = self.speech_recognizer.listen_for_command()\r\n                if result:\r\n                    return result\r\n            except Exception as e:\r\n                rospy.logwarn(f"Recognition attempt {attempt + 1} failed: {e}")\r\n                rospy.sleep(self.retry_delay)\r\n\r\n        # If all attempts fail, return None or trigger fallback\r\n        return None\r\n\r\n    def handle_recognition_error(self, error):\r\n        """Handle different types of recognition errors"""\r\n        if "connection" in str(error).lower():\r\n            rospy.logerr("Connection error - falling back to offline recognition")\r\n            # Implement offline recognition\r\n        elif "timeout" in str(error).lower():\r\n            rospy.loginfo("Recognition timeout - continuing to listen")\r\n        else:\r\n            rospy.logerr(f"Recognition error: {error}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,i.jsx)(n.h3,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,i.jsx)(n.p,{children:"Create comprehensive unit tests for each component:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import unittest\r\nfrom unittest.mock import Mock, patch\r\n\r\nclass TestVoiceCommandProcessor(unittest.TestCase):\r\n    def setUp(self):\r\n        self.processor = VoiceCommandProcessor()\r\n\r\n    def test_navigation_intent_parsing(self):\r\n        """Test parsing of navigation commands"""\r\n        test_commands = [\r\n            "Go to the kitchen",\r\n            "Move to the living room",\r\n            "Navigate to the office"\r\n        ]\r\n\r\n        for command in test_commands:\r\n            parsed = self.processor.nlp_processor.parse_command(command)\r\n            classified = self.processor.intent_classifier.classify_intent(parsed)\r\n\r\n            self.assertEqual(classified[\'intent\'], \'NAVIGATE\')\r\n            self.assertGreater(classified[\'confidence\'], 0.5)\r\n\r\n    def test_manipulation_intent_parsing(self):\r\n        """Test parsing of manipulation commands"""\r\n        test_commands = [\r\n            "Pick up the red cup",\r\n            "Grab the blue box",\r\n            "Take the green ball"\r\n        ]\r\n\r\n        for command in test_commands:\r\n            parsed = self.processor.nlp_processor.parse_command(command)\r\n            classified = self.processor.intent_classifier.classify_intent(parsed)\r\n\r\n            self.assertIn(\'MANIPULATE\', classified[\'intent\'])\r\n            self.assertGreater(classified[\'confidence\'], 0.5)\r\n\r\nif __name__ == \'__main__\':\r\n    unittest.main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,i.jsx)(n.p,{children:"Test the complete voice processing pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VoiceProcessingIntegrationTest:\r\n    def __init__(self):\r\n        rospy.init_node(\'voice_processing_test\')\r\n        self.processor = VoiceCommandProcessor()\r\n\r\n    def test_complete_pipeline(self):\r\n        """Test complete voice processing pipeline"""\r\n        test_commands = [\r\n            ("Go to the kitchen", \'NAVIGATE\'),\r\n            ("Pick up the red cup", \'MANIPULATE_PICK\'),\r\n            ("Put the cup on the table", \'MANIPULATE_PLACE\')\r\n        ]\r\n\r\n        for command, expected_intent in test_commands:\r\n            # Simulate voice input\r\n            self.processor.command_callback(String(data=command))\r\n\r\n            # Verify intent classification and action mapping\r\n            # (This would require monitoring published messages)\r\n            print(f"Processed: {command} -> Expected: {expected_intent}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,i.jsx)(n.h3,{id:"response-time-requirements",children:"Response Time Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Processing"}),": < 100ms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech Recognition"}),": < 500ms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NLP Processing"}),": < 200ms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total Response Time"}),": < 3 seconds"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"accuracy-requirements",children:"Accuracy Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech Recognition"}),": >90% accuracy in quiet environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intent Classification"}),": >85% accuracy for common commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Entity Extraction"}),": >80% accuracy for objects and locations"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"resource-usage",children:"Resource Usage"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CPU Usage"}),": < 20% during continuous processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Usage"}),": < 500MB"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Network Usage"}),": < 100KB/s for cloud-based recognition"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting-and-common-issues",children:"Troubleshooting and Common Issues"}),"\n",(0,i.jsx)(n.h3,{id:"audio-quality-issues",children:"Audio Quality Issues"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Background Noise"}),": Use noise suppression techniques or better microphones"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Clipping"}),": Adjust microphone sensitivity and input levels"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recognition Failures"}),": Implement fallback recognition methods"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"recognition-problems",children:"Recognition Problems"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Poor Accuracy"}),": Train custom language models for domain-specific vocabulary"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency Issues"}),": Optimize for local processing when possible"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Wake Word Detection"}),": Fine-tune sensitivity to reduce false positives"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"integration-issues",children:"Integration Issues"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Message Format Mismatches"}),": Ensure consistent message formats between components"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Timing Issues"}),": Implement proper synchronization between processing stages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Loss"}),": Maintain context across system restarts"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Privacy"}),": Do not store voice recordings without consent"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Authentication"}),": Implement voice-based authentication for sensitive commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Protection"}),": Encrypt voice data during transmission"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"accessibility",children:"Accessibility"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multiple Languages"}),": Support for different languages and accents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Alternative Input"}),": Provide text-based command alternatives"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback"}),": Clear audio/visual feedback for command recognition"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"maintainability",children:"Maintainability"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Modular Design"}),": Keep components loosely coupled"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configuration"}),": Use parameter server for configurable settings"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Logging"}),": Comprehensive logging for debugging and monitoring"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps-and-integration",children:"Next Steps and Integration"}),"\n",(0,i.jsx)(n.h3,{id:"integration-with-other-capstone-components",children:"Integration with Other Capstone Components"}),"\n",(0,i.jsx)(n.p,{children:"The voice command processing system integrates with:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Planning"}),": Provides high-level goals to the planning system"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation"}),": Issues navigation goals based on location commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation"}),": Sends object and action parameters to manipulation system"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception"}),": Requests object detection for unknown objects"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,i.jsx)(n.p,{children:"Consider implementing:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-turn Conversations"}),": Support for follow-up questions and clarifications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Emotion Recognition"}),": Detect user emotional state from voice"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adaptive Learning"}),": Learn user preferences and speech patterns over time"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Continue with ",(0,i.jsx)(n.a,{href:"/Book-ai-native/docs/capstone/task-planning",children:"Task Planning and Execution"})," to explore the implementation of the task planning and execution engine that will coordinate the actions initiated by voice commands."]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsx)(n.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>o,x:()=>a});var t=r(6540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);