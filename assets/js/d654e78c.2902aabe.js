"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[1733],{1963(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-3-ai-brain/lab-perception-system","title":"Lab: Isaac Perception System","description":"Learning Objectives","source":"@site/docs/module-3-ai-brain/lab-perception-system.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/lab-perception-system","permalink":"/Book-ai-native/docs/module-3-ai-brain/lab-perception-system","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/module-3-ai-brain/lab-perception-system.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"GPU Optimization Techniques","permalink":"/Book-ai-native/docs/module-3-ai-brain/gpu-optimization"},"next":{"title":"Module 3 Summary: The AI-Robot Brain (NVIDIA Isaac)","permalink":"/Book-ai-native/docs/module-3-ai-brain/summary"}}');var i=r(4848),s=r(8453);const a={sidebar_position:8},o="Lab: Isaac Perception System",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Lab Overview",id:"lab-overview",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Lab Duration",id:"lab-duration",level:3},{value:"Setting Up the Development Environment",id:"setting-up-the-development-environment",level:2},{value:"Option 1: Using Isaac ROS Docker Containers (Recommended)",id:"option-1-using-isaac-ros-docker-containers-recommended",level:3},{value:"Option 2: Native Installation",id:"option-2-native-installation",level:3},{value:"Creating the Perception System Package",id:"creating-the-perception-system-package",level:2},{value:"Creating the Launch File",id:"creating-the-launch-file",level:2},{value:"Creating the Isaac Perception Pipeline",id:"creating-the-isaac-perception-pipeline",level:2},{value:"Creating the Setup Script",id:"creating-the-setup-script",level:2},{value:"Testing the Perception System",id:"testing-the-perception-system",level:2},{value:"Running the Complete Perception System",id:"running-the-complete-perception-system",level:2},{value:"Validation and Testing",id:"validation-and-testing",level:2},{value:"Deployment on Edge Hardware",id:"deployment-on-edge-hardware",level:2},{value:"Summary and Next Steps",id:"summary-and-next-steps",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Further Enhancements",id:"further-enhancements",level:3},{value:"References",id:"references",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lab-isaac-perception-system",children:"Lab: Isaac Perception System"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By completing this lab, you will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement a complete GPU-accelerated perception pipeline using Isaac ROS"}),"\n",(0,i.jsx)(n.li,{children:"Integrate multiple perception modules (detection, segmentation, tracking)"}),"\n",(0,i.jsx)(n.li,{children:"Optimize the perception system for real-time performance"}),"\n",(0,i.jsx)(n.li,{children:"Validate perception outputs against ground truth data"}),"\n",(0,i.jsx)(n.li,{children:"Deploy the perception system on edge hardware"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lab-overview",children:"Lab Overview"}),"\n",(0,i.jsx)(n.p,{children:"In this hands-on lab, you will build a complete perception system using NVIDIA Isaac ROS packages. The system will process camera images to detect objects, segment the scene, and track objects across frames. You'll optimize the system for real-time performance on edge hardware and validate its accuracy."}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before starting this lab, ensure you have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"NVIDIA GPU with CUDA support (RTX series recommended)"}),"\n",(0,i.jsx)(n.li,{children:"JetPack 5.0+ installed on Jetson platform (if deploying on edge)"}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 Humble Hawksbill installed"}),"\n",(0,i.jsx)(n.li,{children:"Isaac ROS packages installed"}),"\n",(0,i.jsx)(n.li,{children:"Docker and NVIDIA Container Toolkit configured"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-duration",children:"Lab Duration"}),"\n",(0,i.jsx)(n.p,{children:"This lab should take approximately 4-6 hours to complete, depending on your familiarity with ROS 2 and Isaac."}),"\n",(0,i.jsx)(n.h2,{id:"setting-up-the-development-environment",children:"Setting Up the Development Environment"}),"\n",(0,i.jsx)(n.h3,{id:"option-1-using-isaac-ros-docker-containers-recommended",children:"Option 1: Using Isaac ROS Docker Containers (Recommended)"}),"\n",(0,i.jsx)(n.p,{children:"First, pull the necessary Isaac ROS containers:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Pull Isaac ROS base container\r\ndocker pull nvcr.io/nvidia/isaac_ros:galactic-ros-dev\r\n\r\n# Pull specific perception packages\r\ndocker pull nvcr.io/nvidia/isaac_ros:isaac_ros_detectnet\r\ndocker pull nvcr.io/nvidia/isaac_ros:isaac_ros_segmentation\r\ndocker pull nvcr.io/nvidia/isaac_ros:isaac_ros_image_pipeline\n"})}),"\n",(0,i.jsx)(n.p,{children:"Create a Docker container with GPU support:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Run Isaac ROS development container\r\ndocker run --gpus all \\\r\n    --rm -it \\\r\n    --network host \\\r\n    --volume /tmp/.X11-unix:/tmp/.X11-unix \\\r\n    --env DISPLAY=$DISPLAY \\\r\n    --env TERM=xterm-256color \\\r\n    --env QT_X11_NO_MITSHM=1 \\\r\n    --privileged \\\r\n    --name isaac_perception_lab \\\r\n    nvcr.io/nvidia/isaac_ros:galactic-ros-dev\r\n\r\n# Inside the container, set up your workspace\r\nmkdir -p ~/isaac_ws/src\r\ncd ~/isaac_ws\r\ncolcon build --symlink-install\r\nsource install/setup.bash\n"})}),"\n",(0,i.jsx)(n.h3,{id:"option-2-native-installation",children:"Option 2: Native Installation"}),"\n",(0,i.jsx)(n.p,{children:"For native installation on Ubuntu:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Add NVIDIA package repositories\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb\r\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\r\nsudo apt-get update\r\n\r\n# Install CUDA toolkit\r\nsudo apt-get install -y cuda-toolkit-11-8\r\n\r\n# Install Isaac ROS packages\r\nsudo apt-get install -y ros-galactic-isaac-ros-common\r\nsudo apt-get install -y ros-galactic-isaac-ros-dnn-inference\r\nsudo apt-get install -y ros-galactic-isaac-ros-image-pipeline\n"})}),"\n",(0,i.jsx)(n.h2,{id:"creating-the-perception-system-package",children:"Creating the Perception System Package"}),"\n",(0,i.jsx)(n.p,{children:"Let's create a new ROS 2 package for our perception system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ws/src\r\nros2 pkg create --build-type ament_python isaac_perception_system\r\ncd isaac_perception_system\n"})}),"\n",(0,i.jsx)(n.p,{children:"Create the main perception node:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: isaac_perception_system/isaac_perception_system/perception_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom geometry_msgs.msg import Point\r\nfrom std_msgs.msg import Header\r\nimport cv2\r\nimport numpy as np\r\nfrom cv_bridge import CvBridge\r\nimport message_filters\r\n\r\nclass IsaacPerceptionSystem(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_perception_system')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Publishers\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray, 'perception/detections', 10\r\n        )\r\n\r\n        self.segmentation_publisher = self.create_publisher(\r\n            Image, 'perception/segmentation', 10\r\n        )\r\n\r\n        # Subscribers using message filters for synchronization\r\n        self.image_sub = message_filters.Subscriber(\r\n            self, Image, 'camera/image_raw'\r\n        )\r\n        self.info_sub = message_filters.Subscriber(\r\n            self, CameraInfo, 'camera/camera_info'\r\n        )\r\n\r\n        # Synchronize image and camera info\r\n        self.sync = message_filters.ApproximateTimeSynchronizer(\r\n            [self.image_sub, self.info_sub], queue_size=10, slop=0.1\r\n        )\r\n        self.sync.registerCallback(self.process_synchronized_data)\r\n\r\n        # Performance tracking\r\n        self.frame_count = 0\r\n        self.start_time = self.get_clock().now()\r\n\r\n        self.get_logger().info('Isaac Perception System initialized')\r\n\r\n    def process_synchronized_data(self, image_msg, info_msg):\r\n        \"\"\"Process synchronized image and camera info data\"\"\"\r\n        try:\r\n            # Convert ROS image to OpenCV format\r\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\r\n\r\n            # Process image through perception pipeline\r\n            detections, segmentation = self.run_perception_pipeline(cv_image)\r\n\r\n            # Publish results\r\n            if detections is not None:\r\n                self.publish_detections(detections, image_msg.header)\r\n\r\n            if segmentation is not None:\r\n                self.publish_segmentation(segmentation, image_msg.header)\r\n\r\n            # Track performance\r\n            self.frame_count += 1\r\n            if self.frame_count % 30 == 0:  # Log every 30 frames\r\n                current_time = self.get_clock().now()\r\n                elapsed = (current_time - self.start_time).nanoseconds / 1e9\r\n                fps = self.frame_count / elapsed\r\n                self.get_logger().info(f'Processing at {fps:.2f} FPS')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing frame: {e}')\r\n\r\n    def run_perception_pipeline(self, image):\r\n        \"\"\"Run the complete perception pipeline\"\"\"\r\n        # This is a simplified version - in practice, you'd use Isaac ROS packages\r\n        # For this lab, we'll simulate the perception pipeline\r\n\r\n        # 1. Object Detection (simulated)\r\n        detections = self.simulate_object_detection(image)\r\n\r\n        # 2. Semantic Segmentation (simulated)\r\n        segmentation = self.simulate_segmentation(image)\r\n\r\n        return detections, segmentation\r\n\r\n    def simulate_object_detection(self, image):\r\n        \"\"\"Simulate object detection using Isaac-compatible format\"\"\"\r\n        # In a real implementation, this would use Isaac ROS detectnet\r\n        # For simulation, we'll create some sample detections\r\n        height, width = image.shape[:2]\r\n\r\n        # Create sample detections (in practice, this comes from neural network)\r\n        sample_detections = [\r\n            {\r\n                'class_name': 'person',\r\n                'confidence': 0.85,\r\n                'bbox': [width//4, height//4, width//2, height//2],  # [x, y, w, h]\r\n                'center': [width//2, height//2]\r\n            },\r\n            {\r\n                'class_name': 'car',\r\n                'confidence': 0.78,\r\n                'bbox': [width//3, height//3, width//3, height//3],\r\n                'center': [width//2, 2*height//3]\r\n            }\r\n        ]\r\n\r\n        return sample_detections\r\n\r\n    def simulate_segmentation(self, image):\r\n        \"\"\"Simulate semantic segmentation\"\"\"\r\n        # In a real implementation, this would use Isaac ROS segmentation\r\n        # For simulation, we'll create a simple segmentation mask\r\n        height, width = image.shape[:2]\r\n\r\n        # Create a simple segmentation mask\r\n        segmentation_mask = np.zeros((height, width), dtype=np.uint8)\r\n\r\n        # Add some regions (simulated segmentation)\r\n        cv2.rectangle(segmentation_mask, (width//4, height//4), (3*width//4, 3*height//4), 1, -1)  # Person region\r\n        cv2.circle(segmentation_mask, (width//2, 3*height//4), width//6, 2, -1)  # Car region\r\n\r\n        return segmentation_mask\r\n\r\n    def publish_detections(self, detections, header):\r\n        \"\"\"Publish detections in vision_msgs format\"\"\"\r\n        detection_array = Detection2DArray()\r\n        detection_array.header = header\r\n\r\n        for det in detections:\r\n            detection = Detection2D()\r\n\r\n            # Set bounding box\r\n            bbox = detection.bbox\r\n            bbox.center.x = det['center'][0]\r\n            bbox.center.y = det['center'][1]\r\n            bbox.size_x = det['bbox'][2]\r\n            bbox.size_y = det['bbox'][3]\r\n\r\n            # Set ID and confidence\r\n            detection.results = []  # This would be populated with actual detection results\r\n\r\n            detection_array.detections.append(detection)\r\n\r\n        self.detection_publisher.publish(detection_array)\r\n\r\n    def publish_segmentation(self, segmentation_mask, header):\r\n        \"\"\"Publish segmentation result\"\"\"\r\n        segmentation_msg = self.bridge.cv2_to_imgmsg(segmentation_mask, encoding='mono8')\r\n        segmentation_msg.header = header\r\n        self.segmentation_publisher.publish(segmentation_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    perception_node = IsaacPerceptionSystem()\r\n\r\n    try:\r\n        rclpy.spin(perception_node)\r\n    except KeyboardInterrupt:\r\n        perception_node.get_logger().info('Shutting down perception system')\r\n    finally:\r\n        perception_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.p,{children:"Now let's create the actual Isaac ROS implementation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: isaac_perception_system/isaac_perception_system/isaac_perception_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom geometry_msgs.msg import Point\r\nfrom std_msgs.msg import Header\r\nimport cv2\r\nimport numpy as np\r\nfrom cv_bridge import CvBridge\r\nimport message_filters\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\n\r\nclass IsaacPerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_perception_node')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Publishers\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray, 'isaac_perception/detections', 10\r\n        )\r\n\r\n        # Set up QoS for camera data\r\n        qos_profile = QoSProfile(\r\n            reliability=ReliabilityPolicy.BEST_EFFORT,\r\n            history=HistoryPolicy.KEEP_LAST,\r\n            depth=1\r\n        )\r\n\r\n        # Subscribers\r\n        self.image_sub = message_filters.Subscriber(\r\n            self, Image, 'camera/image_rect_color', qos_profile=qos_profile\r\n        )\r\n        self.info_sub = message_filters.Subscriber(\r\n            self, CameraInfo, 'camera/camera_info', qos_profile=qos_profile\r\n        )\r\n\r\n        # Synchronize image and camera info\r\n        self.sync = message_filters.ApproximateTimeSynchronizer(\r\n            [self.image_sub, self.info_sub], queue_size=5, slop=0.1\r\n        )\r\n        self.sync.registerCallback(self.process_perception_data)\r\n\r\n        # Performance tracking\r\n        self.frame_count = 0\r\n        self.start_time = self.get_clock().now()\r\n\r\n        # Isaac-specific parameters\r\n        self.declare_parameters(\r\n            namespace='',\r\n            parameters=[\r\n                ('model_path', '/models/detectnet/resnet18_detector.trt'),\r\n                ('input_width', 224),\r\n                ('input_height', 224),\r\n                ('confidence_threshold', 0.5),\r\n                ('max_objects', 10)\r\n            ]\r\n        )\r\n\r\n        self.get_logger().info('Isaac Perception Node initialized')\r\n\r\n    def process_perception_data(self, image_msg, info_msg):\r\n        \"\"\"Process perception data using Isaac ROS packages\"\"\"\r\n        try:\r\n            # In a real Isaac implementation, this would connect to Isaac ROS\r\n            # perception packages like detectnet, segmentation, etc.\r\n            # For this lab, we'll simulate the Isaac pipeline\r\n\r\n            # Track performance\r\n            self.frame_count += 1\r\n            if self.frame_count % 30 == 0:\r\n                current_time = self.get_clock().now()\r\n                elapsed = (current_time - self.start_time).nanoseconds / 1e9\r\n                fps = self.frame_count / elapsed\r\n                self.get_logger().info(f'Isaac perception running at {fps:.2f} FPS')\r\n\r\n            # For simulation purposes, create sample detections\r\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\r\n            detections = self.create_sample_detections(cv_image, image_msg.header)\r\n\r\n            # Publish results\r\n            self.detection_publisher.publish(detections)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in perception pipeline: {e}')\r\n\r\n    def create_sample_detections(self, image, header):\r\n        \"\"\"Create sample detections for demonstration\"\"\"\r\n        height, width = image.shape[:2]\r\n        detection_array = Detection2DArray()\r\n        detection_array.header = header\r\n\r\n        # Create sample detections\r\n        for i in range(3):  # Create 3 sample detections\r\n            detection = Detection2D()\r\n\r\n            # Set random bounding box\r\n            x = np.random.randint(0, width // 2)\r\n            y = np.random.randint(0, height // 2)\r\n            w = np.random.randint(width // 4, width // 2)\r\n            h = np.random.randint(height // 4, height // 2)\r\n\r\n            detection.bbox.center.x = x + w // 2\r\n            detection.bbox.center.y = y + h // 2\r\n            detection.bbox.size_x = w\r\n            detection.bbox.size_y = h\r\n\r\n            # Add detection result\r\n            from vision_msgs.msg import ObjectHypothesisWithPose\r\n            hypothesis = ObjectHypothesisWithPose()\r\n            hypothesis.hypothesis.class_id = f'object_{i}'\r\n            hypothesis.hypothesis.score = 0.8 + np.random.random() * 0.2  # 0.8-1.0\r\n            detection.results.append(hypothesis)\r\n\r\n            detection_array.detections.append(detection)\r\n\r\n        return detection_array\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    perception_node = IsaacPerceptionNode()\r\n\r\n    try:\r\n        rclpy.spin(perception_node)\r\n    except KeyboardInterrupt:\r\n        perception_node.get_logger().info('Shutting down Isaac perception node')\r\n    finally:\r\n        perception_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"creating-the-launch-file",children:"Creating the Launch File"}),"\n",(0,i.jsx)(n.p,{children:"Create a launch file to start the perception system with Isaac ROS packages:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"\x3c!-- File: isaac_perception_system/launch/perception_system.launch.py --\x3e\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import ComposableNodeContainer\r\nfrom launch_ros.descriptions import ComposableNode\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    model_path_arg = DeclareLaunchArgument(\r\n        'model_path',\r\n        default_value='/models/detectnet/resnet18_detector.trt',\r\n        description='Path to the TensorRT model'\r\n    )\r\n\r\n    confidence_threshold_arg = DeclareLaunchArgument(\r\n        'confidence_threshold',\r\n        default_value='0.5',\r\n        description='Confidence threshold for detections'\r\n    )\r\n\r\n    # Get launch configurations\r\n    model_path = LaunchConfiguration('model_path')\r\n    confidence_threshold = LaunchConfiguration('confidence_threshold')\r\n\r\n    # Create composable node container\r\n    perception_container = ComposableNodeContainer(\r\n        name='perception_container',\r\n        namespace='',\r\n        package='rclcpp_components',\r\n        executable='component_container_mt',\r\n        composable_node_descriptions=[\r\n            # Image format conversion for Isaac\r\n            ComposableNode(\r\n                package='isaac_ros_image_proc',\r\n                plugin='isaac_ros::ImageFormatConverterNode',\r\n                name='image_format_converter',\r\n                parameters=[{\r\n                    'encoding_desired': 'rgb8',\r\n                    'image_width': 640,\r\n                    'image_height': 480\r\n                }],\r\n                remappings=[\r\n                    ('image_raw', 'camera/image_raw'),\r\n                    ('image', 'camera/image_rect_color')\r\n                ]\r\n            ),\r\n            # Isaac DetectNet for object detection\r\n            ComposableNode(\r\n                package='isaac_ros_detectnet',\r\n                plugin='nvidia::isaac_ros::dnn_inference::ImageEncoderNode',\r\n                name='detectnet_encoder',\r\n                parameters=[{\r\n                    'model_path': model_path,\r\n                    'input_tensor_names': ['input'],\r\n                    'output_tensor_names': ['output'],\r\n                    'model_input_width': 224,\r\n                    'model_input_height': 224,\r\n                    'model_input_channel': 3,\r\n                    'confidence_threshold': confidence_threshold\r\n                }],\r\n                remappings=[\r\n                    ('encoded_tensor', 'detectnet/encoded_tensor'),\r\n                    ('image', 'camera/image_rect_color')\r\n                ]\r\n            ),\r\n            # Custom perception processing node\r\n            ComposableNode(\r\n                package='isaac_perception_system',\r\n                plugin='isaac_perception_system.IsaacPerceptionNode',\r\n                name='isaac_perception_node',\r\n                parameters=[{\r\n                    'model_path': model_path,\r\n                    'confidence_threshold': confidence_threshold\r\n                }],\r\n                remappings=[\r\n                    ('camera/image_rect_color', 'camera/image_rect_color'),\r\n                    ('camera/camera_info', 'camera/camera_info'),\r\n                    ('isaac_perception/detections', 'perception/detections')\r\n                ]\r\n            )\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    return LaunchDescription([\r\n        model_path_arg,\r\n        confidence_threshold_arg,\r\n        perception_container\r\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"creating-the-isaac-perception-pipeline",children:"Creating the Isaac Perception Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"Now let's create a more complete implementation that integrates Isaac ROS packages:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: isaac_perception_system/isaac_perception_system/complete_perception_pipeline.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\r\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\r\nfrom geometry_msgs.msg import Point\r\nfrom std_msgs.msg import Header, Float32\r\nfrom builtin_interfaces.msg import Duration\r\nimport cv2\r\nimport numpy as np\r\nfrom cv_bridge import CvBridge\r\nimport message_filters\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\nfrom collections import deque\r\nimport time\r\n\r\nclass CompleteIsaacPerceptionPipeline(Node):\r\n    def __init__(self):\r\n        super().__init__('complete_isaac_perception_pipeline')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Publishers\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray, 'isaac_perception/detections', 10\r\n        )\r\n\r\n        self.performance_publisher = self.create_publisher(\r\n            Float32, 'isaac_perception/performance', 10\r\n        )\r\n\r\n        # Set up QoS for camera data\r\n        qos_profile = QoSProfile(\r\n            reliability=ReliabilityPolicy.BEST_EFFORT,\r\n            history=HistoryPolicy.KEEP_LAST,\r\n            depth=1\r\n        )\r\n\r\n        # Subscribers with synchronization\r\n        self.image_sub = message_filters.Subscriber(\r\n            self, Image, 'camera/image_rect_color', qos_profile=qos_profile\r\n        )\r\n        self.info_sub = message_filters.Subscriber(\r\n            self, CameraInfo, 'camera/camera_info', qos_profile=qos_profile\r\n        )\r\n\r\n        # Synchronize image and camera info\r\n        self.sync = message_filters.ApproximateTimeSynchronizer(\r\n            [self.image_sub, self.info_sub], queue_size=5, slop=0.1\r\n        )\r\n        self.sync.registerCallback(self.process_perception_pipeline)\r\n\r\n        # Performance tracking\r\n        self.frame_times = deque(maxlen=30)  # Track last 30 frame times\r\n        self.frame_count = 0\r\n        self.last_performance_report = time.time()\r\n\r\n        # Perception parameters\r\n        self.confidence_threshold = 0.5\r\n        self.max_objects = 20\r\n        self.iou_threshold = 0.3  # Intersection over Union threshold for NMS\r\n\r\n        # Object tracking (simple implementation for this lab)\r\n        self.tracked_objects = {}\r\n        self.next_object_id = 0\r\n\r\n        # Isaac-specific parameters\r\n        self.declare_parameters(\r\n            namespace='',\r\n            parameters=[\r\n                ('model_path', '/models/detectnet/resnet18_detector.trt'),\r\n                ('input_width', 224),\r\n                ('input_height', 224),\r\n                ('confidence_threshold', 0.5),\r\n                ('max_objects', 20),\r\n                ('iou_threshold', 0.3),\r\n                ('enable_tracking', True),\r\n                ('tracking_max_age', 10),\r\n                ('tracking_min_hits', 3)\r\n            ]\r\n        )\r\n\r\n        # Get parameters\r\n        self.confidence_threshold = self.get_parameter('confidence_threshold').value\r\n        self.max_objects = self.get_parameter('max_objects').value\r\n        self.iou_threshold = self.get_parameter('iou_threshold').value\r\n        self.enable_tracking = self.get_parameter('enable_tracking').value\r\n\r\n        self.get_logger().info('Complete Isaac Perception Pipeline initialized')\r\n\r\n    def process_perception_pipeline(self, image_msg, info_msg):\r\n        \"\"\"Process the complete perception pipeline\"\"\"\r\n        start_time = time.time()\r\n\r\n        try:\r\n            # Convert ROS image to OpenCV format\r\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\r\n\r\n            # Run Isaac-based perception (simulated in this lab)\r\n            detections = self.run_isaac_perception(cv_image)\r\n\r\n            # Apply non-maximum suppression to remove duplicate detections\r\n            detections = self.non_maximum_suppression(detections, self.iou_threshold)\r\n\r\n            # Apply object tracking if enabled\r\n            if self.enable_tracking:\r\n                detections = self.update_object_tracking(detections)\r\n\r\n            # Publish results\r\n            self.publish_detections(detections, image_msg.header)\r\n\r\n            # Track performance\r\n            frame_time = time.time() - start_time\r\n            self.frame_times.append(frame_time)\r\n\r\n            # Calculate and publish performance metrics\r\n            self.publish_performance_metrics()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in perception pipeline: {e}')\r\n\r\n    def run_isaac_perception(self, image):\r\n        \"\"\"Simulate Isaac ROS perception pipeline\"\"\"\r\n        # In a real implementation, this would interface with Isaac ROS packages\r\n        # For this lab, we'll simulate the perception with realistic timing\r\n\r\n        height, width = image.shape[:2]\r\n\r\n        # Simulate realistic detection times\r\n        time.sleep(0.02)  # Simulate 20ms processing time\r\n\r\n        # Create realistic detections\r\n        detections = []\r\n\r\n        # Add some random detections based on image content\r\n        num_detections = np.random.poisson(2)  # Average of 2 detections per frame\r\n\r\n        for i in range(min(num_detections, self.max_objects)):\r\n            # Random bounding box\r\n            x = np.random.randint(0, width // 2)\r\n            y = np.random.randint(0, height // 2)\r\n            w = np.random.randint(width // 8, width // 3)\r\n            h = np.random.randint(height // 8, height // 3)\r\n\r\n            # Ensure bounding box is within image bounds\r\n            x = min(x, width - w)\r\n            y = min(y, height - h)\r\n\r\n            # Random confidence (above threshold)\r\n            confidence = self.confidence_threshold + np.random.random() * (1.0 - self.confidence_threshold)\r\n\r\n            detection = {\r\n                'bbox': [x, y, w, h],\r\n                'confidence': confidence,\r\n                'class_id': np.random.choice(['person', 'car', 'bicycle', 'traffic_sign']),\r\n                'center': [x + w//2, y + h//2]\r\n            }\r\n\r\n            detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def non_maximum_suppression(self, detections, iou_threshold):\r\n        \"\"\"Apply non-maximum suppression to remove overlapping detections\"\"\"\r\n        if not detections:\r\n            return detections\r\n\r\n        # Sort by confidence (descending)\r\n        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\r\n\r\n        # Apply NMS\r\n        keep = []\r\n        for detection in detections:\r\n            overlap = False\r\n            for kept in keep:\r\n                # Calculate IoU\r\n                iou = self.calculate_iou(detection['bbox'], kept['bbox'])\r\n                if iou > iou_threshold:\r\n                    overlap = True\r\n                    break\r\n\r\n            if not overlap:\r\n                keep.append(detection)\r\n\r\n        return keep\r\n\r\n    def calculate_iou(self, bbox1, bbox2):\r\n        \"\"\"Calculate Intersection over Union between two bounding boxes\"\"\"\r\n        x1, y1, w1, h1 = bbox1\r\n        x2, y2, w2, h2 = bbox2\r\n\r\n        # Calculate intersection\r\n        xi1 = max(x1, x2)\r\n        yi1 = max(y1, y2)\r\n        xi2 = min(x1 + w1, x2 + w2)\r\n        yi2 = min(y1 + h1, y2 + h2)\r\n\r\n        if xi2 <= xi1 or yi2 <= yi1:\r\n            return 0.0\r\n\r\n        intersection = (xi2 - xi1) * (yi2 - yi1)\r\n        area1 = w1 * h1\r\n        area2 = w2 * h2\r\n        union = area1 + area2 - intersection\r\n\r\n        return intersection / union if union > 0 else 0.0\r\n\r\n    def update_object_tracking(self, detections):\r\n        \"\"\"Simple object tracking to maintain consistent IDs across frames\"\"\"\r\n        # This is a simplified tracking implementation\r\n        # In a real system, you'd use more sophisticated tracking like SORT or Deep SORT\r\n\r\n        updated_detections = []\r\n\r\n        for detection in detections:\r\n            # Find the closest existing tracked object\r\n            best_match = None\r\n            best_distance = float('inf')\r\n\r\n            for obj_id, obj_info in self.tracked_objects.items():\r\n                # Calculate distance to last known position\r\n                dist = np.sqrt(\r\n                    (detection['center'][0] - obj_info['center'][0])**2 +\r\n                    (detection['center'][1] - obj_info['center'][1])**2\r\n                )\r\n\r\n                if dist < best_distance and dist < 50:  # 50 pixel threshold\r\n                    best_distance = dist\r\n                    best_match = obj_id\r\n\r\n            if best_match is not None:\r\n                # Update existing object\r\n                self.tracked_objects[best_match]['center'] = detection['center']\r\n                self.tracked_objects[best_match]['bbox'] = detection['bbox']\r\n                self.tracked_objects[best_match]['confidence'] = detection['confidence']\r\n                self.tracked_objects[best_match]['last_seen'] = time.time()\r\n\r\n                # Add tracking ID to detection\r\n                detection['track_id'] = best_match\r\n            else:\r\n                # Create new tracked object\r\n                new_id = self.next_object_id\r\n                self.tracked_objects[new_id] = {\r\n                    'center': detection['center'],\r\n                    'bbox': detection['bbox'],\r\n                    'confidence': detection['confidence'],\r\n                    'last_seen': time.time(),\r\n                    'class_id': detection['class_id']\r\n                }\r\n                detection['track_id'] = new_id\r\n                self.next_object_id += 1\r\n\r\n            updated_detections.append(detection)\r\n\r\n        # Remove old tracked objects that haven't been seen recently\r\n        current_time = time.time()\r\n        objects_to_remove = []\r\n        for obj_id, obj_info in self.tracked_objects.items():\r\n            if current_time - obj_info['last_seen'] > 1.0:  # 1 second timeout\r\n                objects_to_remove.append(obj_id)\r\n\r\n        for obj_id in objects_to_remove:\r\n            del self.tracked_objects[obj_id]\r\n\r\n        return updated_detections\r\n\r\n    def publish_detections(self, detections, header):\r\n        \"\"\"Publish detections in vision_msgs format\"\"\"\r\n        detection_array = Detection2DArray()\r\n        detection_array.header = header\r\n\r\n        for det in detections:\r\n            detection_2d = Detection2D()\r\n\r\n            # Set bounding box\r\n            detection_2d.bbox.center.x = det['center'][0]\r\n            detection_2d.bbox.center.y = det['center'][1]\r\n            detection_2d.bbox.size_x = det['bbox'][2]\r\n            detection_2d.bbox.size_y = det['bbox'][3]\r\n\r\n            # Add detection result with confidence\r\n            hypothesis = ObjectHypothesisWithPose()\r\n            hypothesis.hypothesis.class_id = det.get('class_id', 'unknown')\r\n            hypothesis.hypothesis.score = det['confidence']\r\n            detection_2d.results.append(hypothesis)\r\n\r\n            # Store tracking ID as a custom field (in a real system, you'd use a custom message)\r\n            detection_2d.bbox.center.z = det.get('track_id', -1)  # Using z as tracking ID storage\r\n\r\n            detection_array.detections.append(detection_2d)\r\n\r\n        self.detection_publisher.publish(detection_array)\r\n\r\n    def publish_performance_metrics(self):\r\n        \"\"\"Publish performance metrics\"\"\"\r\n        if len(self.frame_times) > 0:\r\n            avg_frame_time = np.mean(self.frame_times)\r\n            fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\r\n\r\n            # Publish FPS\r\n            fps_msg = Float32()\r\n            fps_msg.data = float(fps)\r\n            self.performance_publisher.publish(fps_msg)\r\n\r\n            # Log performance periodically\r\n            current_time = time.time()\r\n            if current_time - self.last_performance_report > 5.0:  # Every 5 seconds\r\n                self.get_logger().info(\r\n                    f'Perception performance: {fps:.2f} FPS, '\r\n                    f'avg frame time: {avg_frame_time*1000:.2f} ms, '\r\n                    f'tracked objects: {len(self.tracked_objects)}'\r\n                )\r\n                self.last_performance_report = current_time\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    perception_pipeline = CompleteIsaacPerceptionPipeline()\r\n\r\n    try:\r\n        rclpy.spin(perception_pipeline)\r\n    except KeyboardInterrupt:\r\n        perception_pipeline.get_logger().info('Shutting down perception pipeline')\r\n    finally:\r\n        perception_pipeline.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"creating-the-setup-script",children:"Creating the Setup Script"}),"\n",(0,i.jsx)(n.p,{children:"Let's create a setup script to install dependencies and build the package:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: isaac_perception_system/setup.py\r\nfrom setuptools import setup\r\n\r\npackage_name = 'isaac_perception_system'\r\n\r\nsetup(\r\n    name=package_name,\r\n    version='1.0.0',\r\n    packages=[package_name],\r\n    data_files=[\r\n        ('share/ament_index/resource_index/packages',\r\n            ['resource/' + package_name]),\r\n        ('share/' + package_name, ['package.xml']),\r\n        ('share/' + package_name + '/launch', ['launch/perception_system.launch.py']),\r\n    ],\r\n    install_requires=['setuptools'],\r\n    zip_safe=True,\r\n    maintainer='Your Name',\r\n    maintainer_email='your.email@example.com',\r\n    description='Isaac Perception System for Robotics',\r\n    license='Apache License 2.0',\r\n    tests_require=['pytest'],\r\n    entry_points={\r\n        'console_scripts': [\r\n            'isaac_perception_node = isaac_perception_system.complete_perception_pipeline:main',\r\n            'isaac_perception_sim = isaac_perception_system.perception_node:main',\r\n        ],\r\n    },\r\n)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- File: isaac_perception_system/package.xml --\x3e\r\n<?xml version="1.0"?>\r\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\r\n<package format="3">\r\n  <name>isaac_perception_system</name>\r\n  <version>1.0.0</version>\r\n  <description>Isaac Perception System for Robotics</description>\r\n  <maintainer email="your.email@example.com">Your Name</maintainer>\r\n  <license>Apache License 2.0</license>\r\n\r\n  <depend>rclpy</depend>\r\n  <depend>sensor_msgs</depend>\r\n  <depend>vision_msgs</depend>\r\n  <depend>geometry_msgs</depend>\r\n  <depend>std_msgs</depend>\r\n  <depend>builtin_interfaces</depend>\r\n  <depend>cv_bridge</end>\r\n\r\n  <test_depend>ament_copyright</test_depend>\r\n  <test_depend>ament_flake8</test_depend>\r\n  <test_depend>ament_pep257</test_depend>\r\n  <test_depend>python3-pytest</test_depend>\r\n\r\n  <export>\r\n    <build_type>ament_python</build_type>\r\n  </export>\r\n</package>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"testing-the-perception-system",children:"Testing the Perception System"}),"\n",(0,i.jsx)(n.p,{children:"Create a test script to validate the perception system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# File: isaac_perception_system/test/test_perception_system.py\r\nimport unittest\r\nimport rclpy\r\nfrom rclpy.executors import SingleThreadedExecutor\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport threading\r\nimport time\r\n\r\nclass TestIsaacPerceptionSystem(unittest.TestCase):\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        rclpy.init()\r\n\r\n    @classmethod\r\n    def tearDownClass(cls):\r\n        rclpy.shutdown()\r\n\r\n    def setUp(self):\r\n        self.node = rclpy.create_node(\'test_perception_system\')\r\n        self.executor = SingleThreadedExecutor()\r\n        self.executor.add_node(self.node)\r\n\r\n        # Create publisher for test images\r\n        self.image_publisher = self.node.create_publisher(\r\n            Image, \'camera/image_rect_color\', 10\r\n        )\r\n\r\n        # Create subscriber for detections\r\n        self.detections_received = []\r\n        self.detection_subscription = self.node.create_subscription(\r\n            \'isaac_perception/detections\',\r\n            lambda msg: self.detections_received.append(msg)\r\n        )\r\n\r\n        self.bridge = CvBridge()\r\n\r\n    def test_basic_perception_pipeline(self):\r\n        """Test that the perception pipeline processes images and produces detections"""\r\n        # Create a simple test image\r\n        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n        ros_image = self.bridge.cv2_to_imgmsg(test_image, encoding=\'bgr8\')\r\n\r\n        # Publish test image\r\n        self.image_publisher.publish(ros_image)\r\n\r\n        # Wait for detections\r\n        timeout = time.time() + 5.0  # 5 second timeout\r\n        while time.time() < timeout and len(self.detections_received) == 0:\r\n            rclpy.spin_once(self.node, timeout_sec=0.1)\r\n\r\n        # Check that detections were received\r\n        self.assertGreater(len(self.detections_received), 0,\r\n                          "No detections received from perception system")\r\n\r\n        # Check that detections have expected structure\r\n        detection = self.detections_received[0]\r\n        self.assertIsNotNone(detection.header)\r\n        self.assertGreater(len(detection.detections), 0,\r\n                          "Detections array is empty")\r\n\r\n    def test_performance_metrics(self):\r\n        """Test that performance metrics are published"""\r\n        # Implementation would check performance metrics\r\n        pass\r\n\r\n    def test_tracking_functionality(self):\r\n        """Test that object tracking works across frames"""\r\n        # Implementation would test tracking consistency\r\n        pass\r\n\r\ndef main():\r\n    unittest.main()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"running-the-complete-perception-system",children:"Running the Complete Perception System"}),"\n",(0,i.jsx)(n.p,{children:"Now let's create instructions for running the complete system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\r\n# File: isaac_perception_system/run_perception_system.sh\r\n\r\n# Script to run the Isaac Perception System\r\n\r\necho "Setting up Isaac Perception System..."\r\n\r\n# Source ROS 2 environment\r\nsource /opt/ros/galactic/setup.bash\r\n\r\n# Source workspace\r\ncd ~/isaac_ws\r\nsource install/setup.bash\r\n\r\n# Build the workspace if not already built\r\ncolcon build --packages-select isaac_perception_system\r\n\r\n# Source again after build\r\nsource install/setup.bash\r\n\r\necho "Starting Isaac Perception System..."\r\n\r\n# Run the perception system\r\nros2 launch isaac_perception_system perception_system.launch.py\r\n\r\necho "Isaac Perception System finished."\n'})}),"\n",(0,i.jsx)(n.h2,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,i.jsx)(n.p,{children:"Create a validation script to test the perception system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: isaac_perception_system/scripts/validate_perception_system.py\r\n#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport time\r\nfrom std_msgs.msg import Float32\r\n\r\nclass PerceptionValidator(Node):\r\n    def __init__(self):\r\n        super().__init__('perception_validator')\r\n\r\n        self.bridge = CvBridge()\r\n        self.detection_count = 0\r\n        self.performance_metrics = []\r\n\r\n        # Publishers\r\n        self.image_publisher = self.create_publisher(Image, 'camera/image_rect_color', 10)\r\n\r\n        # Subscribers\r\n        self.detection_subscription = self.create_subscription(\r\n            Detection2DArray,\r\n            'isaac_perception/detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n        self.performance_subscription = self.create_subscription(\r\n            Float32,\r\n            'isaac_perception/performance',\r\n            self.performance_callback,\r\n            10\r\n        )\r\n\r\n        # Timer for sending test images\r\n        self.timer = self.create_timer(0.1, self.publish_test_image)  # 10 Hz\r\n        self.test_start_time = time.time()\r\n        self.test_duration = 30  # Test for 30 seconds\r\n\r\n    def publish_test_image(self):\r\n        \"\"\"Publish a test image for perception system\"\"\"\r\n        current_time = time.time()\r\n        if current_time - self.test_start_time > self.test_duration:\r\n            self.get_logger().info('Test duration completed. Shutting down.')\r\n            rclpy.shutdown()\r\n            return\r\n\r\n        # Create a synthetic test image with known objects\r\n        image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n\r\n        # Add some synthetic objects (circles and rectangles)\r\n        cv2.circle(image, (100, 100), 30, (255, 0, 0), -1)  # Blue circle\r\n        cv2.rectangle(image, (200, 200), (300, 300), (0, 255, 0), -1)  # Green rectangle\r\n\r\n        ros_image = self.bridge.cv2_to_imgmsg(image, encoding='bgr8')\r\n        ros_image.header.stamp = self.get_clock().now().to_msg()\r\n        ros_image.header.frame_id = 'camera_frame'\r\n\r\n        self.image_publisher.publish(ros_image)\r\n\r\n    def detection_callback(self, msg):\r\n        \"\"\"Handle incoming detections\"\"\"\r\n        self.detection_count += len(msg.detections)\r\n        self.get_logger().info(f'Received {len(msg.detections)} detections, total: {self.detection_count}')\r\n\r\n    def performance_callback(self, msg):\r\n        \"\"\"Handle performance metrics\"\"\"\r\n        self.performance_metrics.append(msg.data)\r\n        if len(self.performance_metrics) % 10 == 0:  # Log every 10 metrics\r\n            avg_fps = np.mean(self.performance_metrics[-10:]) if self.performance_metrics else 0\r\n            self.get_logger().info(f'Average FPS (last 10): {avg_fps:.2f}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    validator = PerceptionValidator()\r\n\r\n    try:\r\n        rclpy.spin(validator)\r\n    except KeyboardInterrupt:\r\n        validator.get_logger().info('Validation interrupted by user')\r\n    finally:\r\n        # Print final statistics\r\n        if validator.performance_metrics:\r\n            avg_fps = np.mean(validator.performance_metrics)\r\n            std_fps = np.std(validator.performance_metrics)\r\n            validator.get_logger().info(f'Final Results:')\r\n            validator.get_logger().info(f'  Average FPS: {avg_fps:.2f} \xb1 {std_fps:.2f}')\r\n            validator.get_logger().info(f'  Total detections: {validator.detection_count}')\r\n\r\n        validator.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"deployment-on-edge-hardware",children:"Deployment on Edge Hardware"}),"\n",(0,i.jsx)(n.p,{children:"Create a deployment script for Jetson platforms:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\r\n# File: isaac_perception_system/deploy_to_jetson.sh\r\n\r\n# Deployment script for Isaac Perception System on Jetson\r\n\r\necho "Deploying Isaac Perception System to Jetson..."\r\n\r\n# Set Jetson-specific parameters\r\nexport CUDA_VISIBLE_DEVICES=0\r\nexport NVIDIA_VISIBLE_DEVICES=all\r\n\r\n# Optimize for Jetson\r\nsudo nvpmodel -m 0  # Set to maximum performance mode\r\nsudo jetson_clocks  # Lock clocks to maximum frequency\r\n\r\n# Build for Jetson architecture\r\ncd ~/isaac_ws\r\ncolcon build --packages-select isaac_perception_system --cmake-args -DCMAKE_BUILD_TYPE=Release\r\n\r\n# Source the workspace\r\nsource install/setup.bash\r\n\r\n# Run with Jetson-optimized parameters\r\necho "Starting Isaac Perception System on Jetson..."\r\nros2 launch isaac_perception_system perception_system.launch.py \\\r\n    model_path:=/models/jetson/resnet18_detector.trt \\\r\n    confidence_threshold:=0.6\r\n\r\necho "Deployment completed."\n'})}),"\n",(0,i.jsx)(n.h2,{id:"summary-and-next-steps",children:"Summary and Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"This lab provided hands-on experience with creating a complete Isaac perception system. You learned to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up the Isaac ROS development environment"}),"\n",(0,i.jsx)(n.li,{children:"Create a perception pipeline with detection, segmentation, and tracking"}),"\n",(0,i.jsx)(n.li,{children:"Optimize the system for real-time performance"}),"\n",(0,i.jsx)(n.li,{children:"Validate the system with test data"}),"\n",(0,i.jsx)(n.li,{children:"Deploy the system on edge hardware"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Isaac ROS provides GPU-accelerated perception packages that significantly outperform CPU-based alternatives"}),"\n",(0,i.jsx)(n.li,{children:"Proper synchronization of sensor data is crucial for accurate perception"}),"\n",(0,i.jsx)(n.li,{children:"Performance optimization involves balancing computational load with real-time constraints"}),"\n",(0,i.jsx)(n.li,{children:"Validation and testing are essential for robust perception systems"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"further-enhancements",children:"Further Enhancements"}),"\n",(0,i.jsx)(n.p,{children:"Consider these improvements for your perception system:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced Tracking"}),": Implement more sophisticated tracking algorithms like Deep SORT"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Sensor Fusion"}),": Integrate LiDAR data with camera data for better 3D perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Optimization"}),": Use TensorRT optimization for better inference performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adaptive Processing"}),": Adjust processing parameters based on scene complexity"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsx)(n.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453(e,n,r){r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);