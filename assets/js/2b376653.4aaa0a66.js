"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[1533],{1143(e,r,n){n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"capstone/object-detection","title":"Object Detection and Manipulation: Autonomous Humanoid Capstone","description":"Overview","source":"@site/docs/capstone/object-detection.md","sourceDirName":"capstone","slug":"/capstone/object-detection","permalink":"/Book-ai-native/docs/capstone/object-detection","draft":false,"unlisted":false,"editUrl":"https://github.com/Malaikaali2/Book-ai-native/tree/main/docs/capstone/object-detection.md","tags":[],"version":"current","sidebarPosition":19,"frontMatter":{"sidebar_position":19},"sidebar":"tutorialSidebar","previous":{"title":"Navigation and Obstacle Avoidance: Autonomous Humanoid Capstone","permalink":"/Book-ai-native/docs/capstone/navigation"},"next":{"title":"Failure Handling and Status Reporting: Autonomous Humanoid Capstone","permalink":"/Book-ai-native/docs/capstone/failure-handling"}}');var s=n(4848),o=n(8453);const i={sidebar_position:19},a="Object Detection and Manipulation: Autonomous Humanoid Capstone",c={},l=[{value:"Overview",id:"overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Perception and Manipulation Pipeline",id:"perception-and-manipulation-pipeline",level:3},{value:"Integration with Other Systems",id:"integration-with-other-systems",level:3},{value:"Technical Implementation",id:"technical-implementation",level:2},{value:"1. Object Detection and Recognition",id:"1-object-detection-and-recognition",level:3},{value:"Deep Learning-Based Object Detection",id:"deep-learning-based-object-detection",level:4},{value:"3D Pose Estimation",id:"3d-pose-estimation",level:4},{value:"2. Grasp Planning and Manipulation",id:"2-grasp-planning-and-manipulation",level:3},{value:"Grasp Planning System",id:"grasp-planning-system",level:4},{value:"Manipulation Execution System",id:"manipulation-execution-system",level:4},{value:"3. Integration with Perception and Navigation",id:"3-integration-with-perception-and-navigation",level:3},{value:"Perception-Action Coordination",id:"perception-action-coordination",level:4},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Set up Object Detection Infrastructure",id:"step-1-set-up-object-detection-infrastructure",level:3},{value:"Step 2: Configure Manipulation Parameters",id:"step-2-configure-manipulation-parameters",level:3},{value:"Step 3: Implement Advanced Features",id:"step-3-implement-advanced-features",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Testing",id:"unit-testing",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Detection Performance",id:"detection-performance",level:3},{value:"Accuracy Requirements",id:"accuracy-requirements",level:3},{value:"Troubleshooting and Common Issues",id:"troubleshooting-and-common-issues",level:2},{value:"Detection Problems",id:"detection-problems",level:3},{value:"Manipulation Problems",id:"manipulation-problems",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Maintainability",id:"maintainability",level:3},{value:"Next Steps and Integration",id:"next-steps-and-integration",level:2},{value:"Integration with Other Capstone Components",id:"integration-with-other-capstone-components",level:3},{value:"Advanced Features",id:"advanced-features",level:3},{value:"References",id:"references",level:2}];function p(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"object-detection-and-manipulation-autonomous-humanoid-capstone",children:"Object Detection and Manipulation: Autonomous Humanoid Capstone"})}),"\n",(0,s.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(r.p,{children:"Object detection and manipulation form the physical interaction layer of the autonomous humanoid system, enabling the robot to perceive, identify, and manipulate objects in its environment. This component encompasses computer vision for object recognition, 3D pose estimation for spatial understanding, grasp planning for manipulation, and precise control for executing manipulation tasks. The system must handle various object types, sizes, and materials while maintaining real-time performance and safety guarantees."}),"\n",(0,s.jsx)(r.p,{children:"The manipulation system integrates with navigation for precise positioning, with perception for object information, with task planning for coordinated manipulation tasks, and with voice processing for object-related commands. This implementation guide provides detailed instructions for building a robust object detection and manipulation system that can operate effectively in real-world environments with varying lighting and object conditions."}),"\n",(0,s.jsx)(r.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsx)(r.h3,{id:"perception-and-manipulation-pipeline",children:"Perception and Manipulation Pipeline"}),"\n",(0,s.jsx)(r.p,{children:"The object detection and manipulation system implements a multi-stage pipeline architecture:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{children:"RGB-D Input \u2192 Object Detection \u2192 Pose Estimation \u2192 Grasp Planning \u2192 Manipulation Execution \u2192 Task Completion\n"})}),"\n",(0,s.jsx)(r.p,{children:"The architecture consists of:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Perception Module"}),": Detects and classifies objects in the environment"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Pose Estimation"}),": Determines 6-DOF poses of detected objects"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Grasp Planning"}),": Plans feasible grasps for manipulation"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Manipulation Control"}),": Executes precise manipulation actions"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Feedback Integration"}),": Monitors execution and handles failures"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Learning Component"}),": Improves performance through experience"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"integration-with-other-systems",children:"Integration with Other Systems"}),"\n",(0,s.jsx)(r.p,{children:"The manipulation system interfaces with:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Navigation System"}),": Receives positioning commands for object access"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Task Planning"}),": Coordinates manipulation tasks with overall plan"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Perception System"}),": Gets environmental information and object data"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Voice Processing"}),": Handles object-related commands and queries"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Localization"}),": Maintains accurate spatial relationships"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,s.jsx)(r.h3,{id:"1-object-detection-and-recognition",children:"1. Object Detection and Recognition"}),"\n",(0,s.jsx)(r.h4,{id:"deep-learning-based-object-detection",children:"Deep Learning-Based Object Detection"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"import torch\r\nimport torchvision\r\nfrom torchvision import transforms\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport cv2\r\nimport rospy\r\nfrom sensor_msgs.msg import Image as ImageMsg\r\nfrom cv_bridge import CvBridge\r\nfrom geometry_msgs.msg import Point\r\nfrom std_msgs.msg import String\r\n\r\nclass ObjectDetector:\r\n    \"\"\"Deep learning-based object detection system\"\"\"\r\n\r\n    def __init__(self, model_path=None):\r\n        self.bridge = CvBridge()\r\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n\r\n        # Load pre-trained object detection model (e.g., YOLOv5, Detectron2, etc.)\r\n        if model_path:\r\n            self.model = torch.load(model_path)\r\n        else:\r\n            # Use a standard model like Faster R-CNN pre-trained on COCO\r\n            self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\r\n\r\n        self.model.to(self.device)\r\n        self.model.eval()\r\n\r\n        # COCO dataset class names\r\n        self.coco_names = [\r\n            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\r\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\r\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\r\n            'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\r\n            'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\r\n            'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\r\n            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\r\n            'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\r\n            'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\r\n            'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\r\n            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\r\n            'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\r\n        ]\r\n\r\n        # Confidence threshold for detections\r\n        self.confidence_threshold = 0.5\r\n\r\n        # Transform for input images\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor(),\r\n        ])\r\n\r\n    def detect_objects(self, image):\r\n        \"\"\"Detect objects in an input image\"\"\"\r\n        # Convert ROS image to OpenCV format if needed\r\n        if isinstance(image, ImageMsg):\r\n            cv_image = self.bridge.imgmsg_to_cv2(image, \"bgr8\")\r\n        else:\r\n            cv_image = image\r\n\r\n        # Convert BGR to RGB\r\n        rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\r\n\r\n        # Apply transforms\r\n        input_tensor = self.transform(rgb_image).unsqueeze(0).to(self.device)\r\n\r\n        # Perform inference\r\n        with torch.no_grad():\r\n            predictions = self.model(input_tensor)\r\n\r\n        # Process predictions\r\n        detections = []\r\n        for i, (boxes, scores, labels) in enumerate(zip(predictions[0]['boxes'],\r\n                                                       predictions[0]['scores'],\r\n                                                       predictions[0]['labels'])):\r\n            for box, score, label in zip(boxes, scores, labels):\r\n                if score >= self.confidence_threshold:\r\n                    x1, y1, x2, y2 = box.tolist()\r\n                    class_name = self.coco_names[label.item()]\r\n\r\n                    detection = {\r\n                        'bbox': [x1, y1, x2, y2],\r\n                        'score': score.item(),\r\n                        'class_name': class_name,\r\n                        'class_id': label.item()\r\n                    }\r\n                    detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def filter_detections_by_class(self, detections, target_classes):\r\n        \"\"\"Filter detections to include only specific object classes\"\"\"\r\n        filtered_detections = []\r\n        for detection in detections:\r\n            if detection['class_name'] in target_classes:\r\n                filtered_detections.append(detection)\r\n        return filtered_detections\r\n\r\n    def get_object_center(self, detection):\r\n        \"\"\"Get the center point of a detection bounding box\"\"\"\r\n        x1, y1, x2, y2 = detection['bbox']\r\n        center_x = (x1 + x2) / 2\r\n        center_y = (y1 + y2) / 2\r\n        return center_x, center_y\r\n\r\nclass SemanticObjectDetector:\r\n    \"\"\"Semantic object detection with custom training capabilities\"\"\"\r\n\r\n    def __init__(self, custom_model_path=None):\r\n        self.base_detector = ObjectDetector()\r\n        self.custom_objects = {}  # Custom object models\r\n        self.semantic_knowledge = self._load_semantic_knowledge()\r\n\r\n        if custom_model_path:\r\n            self.load_custom_model(custom_model_path)\r\n\r\n    def _load_semantic_knowledge(self):\r\n        \"\"\"Load semantic knowledge about objects\"\"\"\r\n        return {\r\n            'cup': {\r\n                'graspable': True,\r\n                'typical_size': [0.05, 0.1, 0.1],  # width, height, depth in meters\r\n                'grasp_points': ['handle', 'body'],\r\n                'manipulation_constraints': {\r\n                    'max_weight': 0.5,\r\n                    'orientation_sensitive': True\r\n                }\r\n            },\r\n            'book': {\r\n                'graspable': True,\r\n                'typical_size': [0.2, 0.3, 0.02],\r\n                'grasp_points': ['spine', 'cover'],\r\n                'manipulation_constraints': {\r\n                    'max_weight': 1.0,\r\n                    'orientation_sensitive': False\r\n                }\r\n            },\r\n            'bottle': {\r\n                'graspable': True,\r\n                'typical_size': [0.07, 0.25, 0.07],\r\n                'grasp_points': ['neck', 'body'],\r\n                'manipulation_constraints': {\r\n                    'max_weight': 1.0,\r\n                    'orientation_sensitive': True\r\n                }\r\n            }\r\n        }\r\n\r\n    def load_custom_model(self, model_path):\r\n        \"\"\"Load a custom object detection model\"\"\"\r\n        custom_model = torch.load(model_path)\r\n        self.custom_objects = custom_model\r\n\r\n    def detect_custom_objects(self, image, custom_classes=None):\r\n        \"\"\"Detect custom objects using specialized models\"\"\"\r\n        if not custom_classes:\r\n            custom_classes = list(self.custom_objects.keys())\r\n\r\n        # For each custom class, apply specialized detection\r\n        all_detections = self.base_detector.detect_objects(image)\r\n        custom_detections = []\r\n\r\n        for obj_class in custom_classes:\r\n            if obj_class in self.custom_objects:\r\n                # Apply custom detection logic\r\n                class_detections = self._detect_specific_class(image, obj_class)\r\n                custom_detections.extend(class_detections)\r\n\r\n        # Combine with base detections\r\n        all_detections.extend(custom_detections)\r\n        return all_detections\r\n\r\n    def _detect_specific_class(self, image, class_name):\r\n        \"\"\"Detect a specific object class using specialized approach\"\"\"\r\n        # This would implement class-specific detection logic\r\n        # For example, using template matching, geometric features, etc.\r\n        return []\r\n\r\n    def get_object_properties(self, object_name):\r\n        \"\"\"Get semantic properties of an object\"\"\"\r\n        return self.semantic_knowledge.get(object_name, {})\n"})}),"\n",(0,s.jsx)(r.h4,{id:"3d-pose-estimation",children:"3D Pose Estimation"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'import open3d as o3d\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass PoseEstimator:\r\n    """Estimates 6-DOF poses of detected objects"""\r\n\r\n    def __init__(self):\r\n        self.camera_intrinsics = None\r\n        self.point_cloud_resolution = 0.01  # 1cm resolution\r\n\r\n    def set_camera_intrinsics(self, fx, fy, cx, cy):\r\n        """Set camera intrinsic parameters"""\r\n        self.camera_intrinsics = {\r\n            \'fx\': fx, \'fy\': fy,\r\n            \'cx\': cx, \'cy\': cy\r\n        }\r\n\r\n    def estimate_pose_3d(self, rgb_image, depth_image, detection):\r\n        """Estimate 3D pose of a detected object"""\r\n        if self.camera_intrinsics is None:\r\n            raise ValueError("Camera intrinsics not set")\r\n\r\n        # Get bounding box coordinates\r\n        x1, y1, x2, y2 = detection[\'bbox\']\r\n        center_x, center_y = self.base_detector.get_object_center(detection)\r\n\r\n        # Extract region of interest from depth image\r\n        roi_depth = depth_image[int(y1):int(y2), int(x1):int(x2)]\r\n\r\n        # Convert to point cloud\r\n        points_3d = self._depth_to_point_cloud(\r\n            roi_depth,\r\n            int(center_x), int(center_y)\r\n        )\r\n\r\n        if len(points_3d) < 10:  # Not enough points for reliable pose\r\n            return None\r\n\r\n        # Estimate object center in 3D\r\n        object_center_3d = np.mean(points_3d, axis=0)\r\n\r\n        # Estimate object orientation (simplified - in practice, use more sophisticated methods)\r\n        # For now, assume upright orientation\r\n        orientation = R.from_euler(\'xyz\', [0, 0, 0]).as_quat()\r\n\r\n        pose_3d = {\r\n            \'position\': object_center_3d,\r\n            \'orientation\': orientation,\r\n            \'confidence\': detection[\'score\']\r\n        }\r\n\r\n        return pose_3d\r\n\r\n    def _depth_to_point_cloud(self, depth_roi, center_x, center_y):\r\n        """Convert depth ROI to 3D point cloud"""\r\n        if self.camera_intrinsics is None:\r\n            return np.array([])\r\n\r\n        h, w = depth_roi.shape\r\n        points = []\r\n\r\n        for y in range(h):\r\n            for x in range(w):\r\n                depth_val = depth_roi[y, x]\r\n\r\n                if depth_val > 0 and not np.isnan(depth_val):  # Valid depth\r\n                    # Convert to 3D coordinates\r\n                    z = depth_val\r\n                    x_3d = (x - self.camera_intrinsics[\'cx\']) * z / self.camera_intrinsics[\'fx\']\r\n                    y_3d = (y - self.camera_intrinsics[\'cy\']) * z / self.camera_intrinsics[\'fy\']\r\n\r\n                    points.append([x_3d, y_3d, z])\r\n\r\n        return np.array(points)\r\n\r\n    def estimate_object_dimensions(self, point_cloud):\r\n        """Estimate object dimensions from point cloud"""\r\n        if len(point_cloud) == 0:\r\n            return None\r\n\r\n        # Calculate bounding box\r\n        min_coords = np.min(point_cloud, axis=0)\r\n        max_coords = np.max(point_cloud, axis=0)\r\n\r\n        dimensions = max_coords - min_coords\r\n        return dimensions\r\n\r\nclass MultiViewPoseEstimator:\r\n    """Uses multiple camera views for improved pose estimation"""\r\n\r\n    def __init__(self):\r\n        self.cameras = {}  # Multiple camera configurations\r\n        self.fusion_threshold = 0.05  # 5cm threshold for fusion\r\n\r\n    def add_camera(self, camera_id, intrinsics, extrinsics):\r\n        """Add a camera to the multi-view system"""\r\n        self.cameras[camera_id] = {\r\n            \'intrinsics\': intrinsics,\r\n            \'extrinsics\': extrinsics  # Transform from camera to robot base\r\n        }\r\n\r\n    def estimate_pose_multiview(self, images, detections):\r\n        """Estimate pose using multiple camera views"""\r\n        # Get pose estimates from each camera\r\n        pose_estimates = []\r\n\r\n        for cam_id, detection in zip(self.cameras.keys(), detections):\r\n            if cam_id in self.cameras:\r\n                pose = self._estimate_single_view_pose(\r\n                    images[cam_id], detection, cam_id\r\n                )\r\n                if pose:\r\n                    # Transform to robot base frame\r\n                    base_pose = self._transform_to_base_frame(pose, cam_id)\r\n                    pose_estimates.append(base_pose)\r\n\r\n        # Fuse estimates if multiple views available\r\n        if len(pose_estimates) > 1:\r\n            return self._fuse_poses(pose_estimates)\r\n        elif len(pose_estimates) == 1:\r\n            return pose_estimates[0]\r\n        else:\r\n            return None\r\n\r\n    def _transform_to_base_frame(self, pose, camera_id):\r\n        """Transform pose from camera frame to robot base frame"""\r\n        extrinsics = self.cameras[camera_id][\'extrinsics\']\r\n        # Apply transformation (simplified)\r\n        # In practice, this would use proper transformation matrices\r\n        return pose\n'})}),"\n",(0,s.jsx)(r.h3,{id:"2-grasp-planning-and-manipulation",children:"2. Grasp Planning and Manipulation"}),"\n",(0,s.jsx)(r.h4,{id:"grasp-planning-system",children:"Grasp Planning System"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Tuple, Optional\r\n\r\n@dataclass\r\nclass GraspPose:\r\n    """Represents a potential grasp pose"""\r\n    position: np.ndarray  # 3D position [x, y, z]\r\n    orientation: np.ndarray  # Quaternion [x, y, z, w]\r\n    approach_direction: np.ndarray  # Approach vector [x, y, z]\r\n    grasp_type: str  # \'pinch\', \'power\', \'hook\', etc.\r\n    score: float  # Quality score\r\n    width: float  # Required gripper width\r\n\r\nclass GraspPlanner:\r\n    """Plans feasible grasps for objects"""\r\n\r\n    def __init__(self):\r\n        self.gripper_width_range = (0.01, 0.1)  # 1-10cm gripper range\r\n        self.approach_distance = 0.1  # 10cm approach distance\r\n        self.lift_distance = 0.05  # 5cm lift after grasp\r\n\r\n    def plan_grasps(self, object_pose, object_dimensions, object_class=None):\r\n        """Plan potential grasps for an object"""\r\n        grasps = []\r\n\r\n        # Generate multiple grasp candidates based on object dimensions\r\n        center = object_pose[\'position\']\r\n        dims = object_dimensions\r\n\r\n        # Side grasp (for objects with handle or narrow profile)\r\n        if dims[0] < dims[1] and dims[0] < dims[2]:  # Narrowest dimension is width\r\n            side_grasp = self._generate_side_grasp(center, dims, \'pinch\')\r\n            if side_grasp:\r\n                grasps.append(side_grasp)\r\n\r\n        # Top grasp (for objects suitable for overhead grasping)\r\n        if dims[2] > 0.1:  # Object has sufficient height\r\n            top_grasp = self._generate_top_grasp(center, dims, \'pinch\')\r\n            if top_grasp:\r\n                grasps.append(top_grasp)\r\n\r\n        # Power grasp (for larger objects)\r\n        if dims[0] > 0.05 or dims[1] > 0.05:  # Object is large enough\r\n            power_grasp = self._generate_power_grasp(center, dims, \'power\')\r\n            if power_grasp:\r\n                grasps.append(power_grasp)\r\n\r\n        # Sort grasps by score\r\n        grasps.sort(key=lambda g: g.score, reverse=True)\r\n\r\n        return grasps\r\n\r\n    def _generate_side_grasp(self, center, dimensions, grasp_type):\r\n        """Generate a side grasp for the object"""\r\n        # Approach from the side (along the narrowest dimension)\r\n        approach_dir = np.array([1, 0, 0])  # Default approach from positive X\r\n        position = center + approach_dir * (dimensions[0] / 2 + 0.05)  # 5cm offset\r\n\r\n        # Orientation: gripper aligned with object\r\n        orientation = R.from_euler(\'xyz\', [0, 0, 0]).as_quat()\r\n\r\n        # Calculate required gripper width (based on object thickness)\r\n        gripper_width = min(dimensions[1], dimensions[2]) * 0.8  # 80% of smaller dimension\r\n\r\n        # Check if gripper width is within range\r\n        if not (self.gripper_width_range[0] <= gripper_width <= self.gripper_width_range[1]):\r\n            return None\r\n\r\n        grasp = GraspPose(\r\n            position=position,\r\n            orientation=orientation,\r\n            approach_direction=approach_dir,\r\n            grasp_type=grasp_type,\r\n            score=0.8,  # High score for side grasp\r\n            width=gripper_width\r\n        )\r\n\r\n        return grasp\r\n\r\n    def _generate_top_grasp(self, center, dimensions, grasp_type):\r\n        """Generate a top grasp for the object"""\r\n        # Approach from above\r\n        approach_dir = np.array([0, 0, -1])  # Approach from above\r\n        position = center + approach_dir * (dimensions[2] / 2 + 0.05)  # 5cm above object\r\n\r\n        # Orientation: gripper aligned with object\r\n        orientation = R.from_euler(\'xyz\', [0, 0, 0]).as_quat()\r\n\r\n        # Calculate required gripper width\r\n        gripper_width = min(dimensions[0], dimensions[1]) * 0.8\r\n\r\n        # Check if gripper width is within range\r\n        if not (self.gripper_width_range[0] <= gripper_width <= self.gripper_width_range[1]):\r\n            return None\r\n\r\n        grasp = GraspPose(\r\n            position=position,\r\n            orientation=orientation,\r\n            approach_direction=approach_dir,\r\n            grasp_type=grasp_type,\r\n            score=0.7,  # Good score for top grasp\r\n            width=gripper_width\r\n        )\r\n\r\n        return grasp\r\n\r\n    def _generate_power_grasp(self, center, dimensions, grasp_type):\r\n        """Generate a power grasp for the object"""\r\n        # Approach from the side for power grasp\r\n        approach_dir = np.array([0, 1, 0])  # Approach from positive Y\r\n        position = center + approach_dir * (dimensions[1] / 2 + 0.05)\r\n\r\n        # Orientation: gripper perpendicular to approach\r\n        orientation = R.from_euler(\'xyz\', [0, 0, np.pi/2]).as_quat()\r\n\r\n        # Calculate required gripper width\r\n        gripper_width = max(dimensions[0], dimensions[2]) * 0.6  # Use larger dimensions for power grasp\r\n\r\n        # Check if gripper width is within range\r\n        if not (self.gripper_width_range[0] <= gripper_width <= self.gripper_width_range[1]):\r\n            return None\r\n\r\n        grasp = GraspPose(\r\n            position=position,\r\n            orientation=orientation,\r\n            approach_direction=approach_dir,\r\n            grasp_type=grasp_type,\r\n            score=0.6,  # Good score for power grasp\r\n            width=gripper_width\r\n        )\r\n\r\n        return grasp\r\n\r\n    def validate_grasp(self, grasp_pose, object_pose, environment_data):\r\n        """Validate if a grasp is feasible in the current environment"""\r\n        # Check for collisions with environment\r\n        if self._check_collision(grasp_pose, environment_data):\r\n            return False, "Collision detected"\r\n\r\n        # Check if approach direction is clear\r\n        if not self._check_approach_clear(grasp_pose, environment_data):\r\n            return False, "Approach path blocked"\r\n\r\n        # Check if object is accessible\r\n        if not self._check_accessibility(grasp_pose):\r\n            return False, "Object not accessible"\r\n\r\n        return True, "Valid grasp"\r\n\r\n    def _check_collision(self, grasp_pose, environment_data):\r\n        """Check for collisions at grasp pose"""\r\n        # This would interface with collision checking system\r\n        # For now, return False (no collision)\r\n        return False\r\n\r\n    def _check_approach_clear(self, grasp_pose, environment_data):\r\n        """Check if approach path is clear"""\r\n        # This would check along the approach direction\r\n        # For now, return True (path clear)\r\n        return True\r\n\r\n    def _check_accessibility(self, grasp_pose):\r\n        """Check if the grasp pose is physically reachable"""\r\n        # This would check against robot kinematic constraints\r\n        # For now, return True (accessible)\r\n        return True\r\n\r\nclass GraspOptimizer:\r\n    """Optimizes grasp selection based on multiple criteria"""\r\n\r\n    def __init__(self):\r\n        self.weights = {\r\n            \'stability\': 0.4,\r\n            \'accessibility\': 0.3,\r\n            \'efficiency\': 0.2,\r\n            \'safety\': 0.1\r\n        }\r\n\r\n    def select_best_grasp(self, grasp_candidates, object_info, robot_state):\r\n        """Select the best grasp based on multiple criteria"""\r\n        if not grasp_candidates:\r\n            return None\r\n\r\n        best_grasp = None\r\n        best_score = -float(\'inf\')\r\n\r\n        for grasp in grasp_candidates:\r\n            score = self._evaluate_grasp(grasp, object_info, robot_state)\r\n\r\n            if score > best_score:\r\n                best_score = score\r\n                best_grasp = grasp\r\n\r\n        return best_grasp\r\n\r\n    def _evaluate_grasp(self, grasp, object_info, robot_state):\r\n        """Evaluate a grasp based on multiple criteria"""\r\n        stability_score = self._evaluate_stability(grasp, object_info)\r\n        accessibility_score = self._evaluate_accessibility(grasp, robot_state)\r\n        efficiency_score = self._evaluate_efficiency(grasp, robot_state)\r\n        safety_score = self._evaluate_safety(grasp)\r\n\r\n        # Weighted sum of all criteria\r\n        total_score = (\r\n            self.weights[\'stability\'] * stability_score +\r\n            self.weights[\'accessibility\'] * accessibility_score +\r\n            self.weights[\'efficiency\'] * efficiency_score +\r\n            self.weights[\'safety\'] * safety_score\r\n        )\r\n\r\n        return total_score\r\n\r\n    def _evaluate_stability(self, grasp, object_info):\r\n        """Evaluate grasp stability"""\r\n        # Stability depends on grasp type and object properties\r\n        if grasp.grasp_type == \'pinch\':\r\n            return 0.8 if grasp.score > 0.7 else 0.6\r\n        elif grasp.grasp_type == \'power\':\r\n            return 0.9 if grasp.score > 0.6 else 0.7\r\n        else:\r\n            return grasp.score\r\n\r\n    def _evaluate_accessibility(self, grasp, robot_state):\r\n        """Evaluate how accessible the grasp is"""\r\n        # This would check robot kinematic constraints\r\n        # For now, return a simple score based on grasp position\r\n        distance_from_base = np.linalg.norm(grasp.position)\r\n        if distance_from_base < 1.0:  # Within reach\r\n            return 1.0\r\n        elif distance_from_base < 1.5:  # Reachable but difficult\r\n            return 0.7\r\n        else:  # Out of reach\r\n            return 0.2\r\n\r\n    def _evaluate_efficiency(self, grasp, robot_state):\r\n        """Evaluate manipulation efficiency"""\r\n        # Efficiency depends on how well the grasp aligns with the task\r\n        return 0.8  # Default high efficiency\r\n\r\n    def _evaluate_safety(self, grasp):\r\n        """Evaluate safety of the grasp"""\r\n        # Safety depends on object properties and grasp type\r\n        return 1.0  # Assume safe for now\n'})}),"\n",(0,s.jsx)(r.h4,{id:"manipulation-execution-system",children:"Manipulation Execution System"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'import actionlib\r\nfrom control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal\r\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\r\nfrom geometry_msgs.msg import PoseStamped, Point, Quaternion\r\n\r\nclass ManipulationController:\r\n    """Controls the robot arm for manipulation tasks"""\r\n\r\n    def __init__(self, arm_controller_name=\'/arm_controller/follow_joint_trajectory\'):\r\n        self.arm_client = actionlib.SimpleActionClient(arm_controller_name, FollowJointTrajectoryAction)\r\n        self.gripper_client = actionlib.SimpleActionClient(\'/gripper_controller/gripper_cmd\', GripperCommandAction)\r\n\r\n        # Wait for action servers to be available\r\n        rospy.loginfo("Waiting for action servers...")\r\n        self.arm_client.wait_for_server()\r\n        self.gripper_client.wait_for_server()\r\n\r\n        # Robot-specific parameters\r\n        self.joint_names = [\'joint1\', \'joint2\', \'joint3\', \'joint4\', \'joint5\', \'joint6\']\r\n        self.gripper_joint = \'gripper_joint\'\r\n\r\n        # Safety parameters\r\n        self.max_velocity = 0.5  # rad/s\r\n        self.max_acceleration = 1.0  # rad/s^2\r\n        self.gripper_force_limit = 50.0  # N\r\n\r\n    def execute_grasp(self, grasp_pose, object_info):\r\n        """Execute a grasp at the specified pose"""\r\n        try:\r\n            # Move to pre-grasp position\r\n            pre_grasp_pose = self._calculate_pre_grasp_pose(grasp_pose)\r\n            success = self.move_to_pose(pre_grasp_pose)\r\n\r\n            if not success:\r\n                rospy.logerr("Failed to move to pre-grasp pose")\r\n                return False\r\n\r\n            # Approach the object\r\n            approach_success = self._approach_object(grasp_pose)\r\n            if not approach_success:\r\n                rospy.logerr("Failed to approach object")\r\n                return False\r\n\r\n            # Execute grasp\r\n            grasp_success = self._execute_grasp_action(grasp_pose, object_info)\r\n            if not grasp_success:\r\n                rospy.logerr("Failed to execute grasp")\r\n                # Try to recover\r\n                self._recover_from_grasp_failure(grasp_pose)\r\n                return False\r\n\r\n            # Lift the object\r\n            lift_success = self._lift_object(grasp_pose)\r\n            if not lift_success:\r\n                rospy.logerr("Failed to lift object")\r\n                return False\r\n\r\n            rospy.loginfo("Successfully grasped object")\r\n            return True\r\n\r\n        except Exception as e:\r\n            rospy.logerr(f"Error executing grasp: {e}")\r\n            return False\r\n\r\n    def _calculate_pre_grasp_pose(self, grasp_pose):\r\n        """Calculate pre-grasp pose by moving back along approach direction"""\r\n        pre_grasp_pos = grasp_pose.position - grasp_pose.approach_direction * 0.1  # 10cm back\r\n        return {\r\n            \'position\': pre_grasp_pos,\r\n            \'orientation\': grasp_pose.orientation\r\n        }\r\n\r\n    def _approach_object(self, grasp_pose):\r\n        """Move from pre-grasp to grasp position"""\r\n        # Create trajectory with smooth approach\r\n        trajectory = JointTrajectory()\r\n        trajectory.joint_names = self.joint_names\r\n\r\n        # Add intermediate points for smooth approach\r\n        current_pos = self.get_current_pose()\r\n        approach_steps = 10\r\n\r\n        for i in range(approach_steps + 1):\r\n            fraction = i / approach_steps\r\n            step_pos = (1 - fraction) * current_pos[\'position\'] + fraction * grasp_pose.position\r\n\r\n            point = JointTrajectoryPoint()\r\n            point.positions = self._inverse_kinematics(step_pos, grasp_pose.orientation)\r\n            point.velocities = [0.0] * len(self.joint_names)  # Start and end with zero velocity\r\n            point.accelerations = [0.0] * len(self.joint_names)\r\n            point.time_from_start = rospy.Duration(2.0 * fraction)  # 2 seconds total\r\n\r\n            trajectory.points.append(point)\r\n\r\n        # Send trajectory goal\r\n        goal = FollowJointTrajectoryGoal()\r\n        goal.trajectory = trajectory\r\n        goal.goal_time_tolerance = rospy.Duration(1.0)\r\n\r\n        self.arm_client.send_goal(goal)\r\n        self.arm_client.wait_for_result(rospy.Duration(10.0))  # 10 second timeout\r\n\r\n        return self.arm_client.get_result() is not None\r\n\r\n    def _execute_grasp_action(self, grasp_pose, object_info):\r\n        """Execute the actual grasp action"""\r\n        # Close gripper to appropriate width\r\n        gripper_cmd = GripperCommand()\r\n        gripper_cmd.position = grasp_pose.width * 0.8  # Close to 80% of required width\r\n        gripper_cmd.max_effort = self.gripper_force_limit\r\n\r\n        goal = GripperCommandGoal()\r\n        goal.command = gripper_cmd\r\n\r\n        self.gripper_client.send_goal(goal)\r\n        self.gripper_client.wait_for_result(rospy.Duration(5.0))  # 5 second timeout\r\n\r\n        result = self.gripper_client.get_result()\r\n        return result and result.reached_goal\r\n\r\n    def _lift_object(self, grasp_pose):\r\n        """Lift the object after successful grasp"""\r\n        # Move up by lift distance\r\n        lift_pos = grasp_pose.position + np.array([0, 0, 0.05])  # Lift 5cm\r\n\r\n        lift_pose = {\r\n            \'position\': lift_pos,\r\n            \'orientation\': grasp_pose.orientation\r\n        }\r\n\r\n        return self.move_to_pose(lift_pose)\r\n\r\n    def _recover_from_grasp_failure(self, grasp_pose):\r\n        """Attempt to recover from grasp failure"""\r\n        rospy.loginfo("Attempting grasp recovery...")\r\n\r\n        # Move back to safe position\r\n        safe_pos = grasp_pose.position + grasp_pose.approach_direction * 0.15  # Move back 15cm\r\n        safe_pose = {\r\n            \'position\': safe_pos,\r\n            \'orientation\': grasp_pose.orientation\r\n        }\r\n\r\n        self.move_to_pose(safe_pose)\r\n\r\n    def move_to_pose(self, pose):\r\n        """Move the end effector to a specified pose"""\r\n        try:\r\n            # Calculate joint angles using inverse kinematics\r\n            joint_angles = self._inverse_kinematics(pose[\'position\'], pose[\'orientation\'])\r\n\r\n            if joint_angles is None:\r\n                rospy.logerr("Could not find IK solution")\r\n                return False\r\n\r\n            # Create trajectory\r\n            trajectory = JointTrajectory()\r\n            trajectory.joint_names = self.joint_names\r\n\r\n            point = JointTrajectoryPoint()\r\n            point.positions = joint_angles\r\n            point.velocities = [0.0] * len(self.joint_names)\r\n            point.accelerations = [0.0] * len(self.joint_names)\r\n            point.time_from_start = rospy.Duration(3.0)  # 3 seconds\r\n\r\n            trajectory.points.append(point)\r\n\r\n            # Send goal\r\n            goal = FollowJointTrajectoryGoal()\r\n            goal.trajectory = trajectory\r\n            goal.goal_time_tolerance = rospy.Duration(1.0)\r\n\r\n            self.arm_client.send_goal(goal)\r\n            self.arm_client.wait_for_result(rospy.Duration(10.0))\r\n\r\n            return self.arm_client.get_result() is not None\r\n\r\n        except Exception as e:\r\n            rospy.logerr(f"Error moving to pose: {e}")\r\n            return False\r\n\r\n    def _inverse_kinematics(self, position, orientation):\r\n        """Calculate inverse kinematics for desired pose"""\r\n        # This would interface with the robot\'s IK solver\r\n        # For now, return a placeholder solution\r\n        # In practice, this would use KDL, MoveIt!, or robot-specific IK\r\n        return [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # Placeholder joint angles\r\n\r\n    def get_current_pose(self):\r\n        """Get current end-effector pose"""\r\n        # This would interface with forward kinematics or TF\r\n        # For now, return a placeholder\r\n        return {\r\n            \'position\': np.array([0.0, 0.0, 0.0]),\r\n            \'orientation\': np.array([0.0, 0.0, 0.0, 1.0])\r\n        }\r\n\r\nclass PlaceController:\r\n    """Controls object placement operations"""\r\n\r\n    def __init__(self, manipulation_controller):\r\n        self.manip_ctrl = manipulation_controller\r\n        self.approach_distance = 0.1  # 10cm approach for placement\r\n\r\n    def place_object(self, target_pose, placement_type=\'careful\'):\r\n        """Place the currently grasped object at target pose"""\r\n        try:\r\n            # Move to pre-place position\r\n            pre_place_pose = self._calculate_pre_place_pose(target_pose)\r\n            success = self.manip_ctrl.move_to_pose(pre_place_pose)\r\n\r\n            if not success:\r\n                rospy.logerr("Failed to move to pre-place pose")\r\n                return False\r\n\r\n            # Approach placement location\r\n            approach_success = self._approach_placement(target_pose, placement_type)\r\n            if not approach_success:\r\n                rospy.logerr("Failed to approach placement location")\r\n                return False\r\n\r\n            # Release object\r\n            release_success = self._release_object(placement_type)\r\n            if not release_success:\r\n                rospy.logerr("Failed to release object")\r\n                return False\r\n\r\n            # Retract gripper\r\n            retract_success = self._retract_gripper(target_pose)\r\n            if not retract_success:\r\n                rospy.logerr("Failed to retract gripper")\r\n                return False\r\n\r\n            rospy.loginfo("Successfully placed object")\r\n            return True\r\n\r\n        except Exception as e:\r\n            rospy.logerr(f"Error placing object: {e}")\r\n            return False\r\n\r\n    def _calculate_pre_place_pose(self, target_pose):\r\n        """Calculate pre-place pose by moving up from target"""\r\n        pre_place_pos = target_pose[\'position\'] + np.array([0, 0, 0.1])  # 10cm above target\r\n        return {\r\n            \'position\': pre_place_pos,\r\n            \'orientation\': target_pose[\'orientation\']\r\n        }\r\n\r\n    def _approach_placement(self, target_pose, placement_type):\r\n        """Approach the placement location"""\r\n        # Move down to target position\r\n        trajectory = JointTrajectory()\r\n        trajectory.joint_names = self.manip_ctrl.joint_names\r\n\r\n        # Create smooth descent trajectory\r\n        current_pos = self.manip_ctrl.get_current_pose()\r\n        descent_steps = 10\r\n\r\n        for i in range(descent_steps + 1):\r\n            fraction = i / descent_steps\r\n            step_pos = (1 - fraction) * current_pos[\'position\'] + fraction * target_pose[\'position\']\r\n\r\n            point = JointTrajectoryPoint()\r\n            point.positions = self.manip_ctrl._inverse_kinematics(step_pos, target_pose[\'orientation\'])\r\n            point.velocities = [0.0] * len(self.manip_ctrl.joint_names)\r\n            point.accelerations = [0.0] * len(self.manip_ctrl.joint_names)\r\n            point.time_from_start = rospy.Duration(2.0 * fraction)\r\n\r\n            trajectory.points.append(point)\r\n\r\n        # Send trajectory\r\n        goal = FollowJointTrajectoryGoal()\r\n        goal.trajectory = trajectory\r\n        goal.goal_time_tolerance = rospy.Duration(1.0)\r\n\r\n        self.manip_ctrl.arm_client.send_goal(goal)\r\n        self.manip_ctrl.arm_client.wait_for_result(rospy.Duration(10.0))\r\n\r\n        return self.manip_ctrl.arm_client.get_result() is not None\r\n\r\n    def _release_object(self, placement_type):\r\n        """Release the grasped object"""\r\n        # Open gripper fully\r\n        gripper_cmd = GripperCommand()\r\n        gripper_cmd.position = 0.1  # Fully open\r\n        gripper_cmd.max_effort = 5.0  # Low effort to avoid dropping too quickly\r\n\r\n        goal = GripperCommandGoal()\r\n        goal.command = gripper_cmd\r\n\r\n        self.manip_ctrl.gripper_client.send_goal(goal)\r\n        self.manip_ctrl.gripper_client.wait_for_result(rospy.Duration(5.0))\r\n\r\n        result = self.manip_ctrl.gripper_client.get_result()\r\n        return result is not None\r\n\r\n    def _retract_gripper(self, target_pose):\r\n        """Retract gripper after placement"""\r\n        # Move gripper up and away from placed object\r\n        retract_pos = target_pose[\'position\'] + np.array([0, 0, 0.15])  # 15cm above\r\n        retract_pose = {\r\n            \'position\': retract_pos,\r\n            \'orientation\': target_pose[\'orientation\']\r\n        }\r\n\r\n        return self.manip_ctrl.move_to_pose(retract_pose)\n'})}),"\n",(0,s.jsx)(r.h3,{id:"3-integration-with-perception-and-navigation",children:"3. Integration with Perception and Navigation"}),"\n",(0,s.jsx)(r.h4,{id:"perception-action-coordination",children:"Perception-Action Coordination"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'class PerceptionActionCoordinator:\r\n    """Coordinates perception and action systems"""\r\n\r\n    def __init__(self):\r\n        self.object_detector = SemanticObjectDetector()\r\n        self.pose_estimator = MultiViewPoseEstimator()\r\n        self.grasp_planner = GraspPlanner()\r\n        self.grasp_optimizer = GraspOptimizer()\r\n        self.manip_controller = ManipulationController()\r\n        self.place_controller = PlaceController(self.manip_controller)\r\n\r\n        # ROS interfaces\r\n        self.rgb_sub = rospy.Subscriber(\'/camera/rgb/image_raw\', ImageMsg, self.rgb_callback)\r\n        self.depth_sub = rospy.Subscriber(\'/camera/depth/image_raw\', ImageMsg, self.depth_callback)\r\n        self.pointcloud_sub = rospy.Subscriber(\'/camera/depth/points\', PointCloud2, self.pointcloud_callback)\r\n\r\n        self.current_rgb = None\r\n        self.current_depth = None\r\n        self.current_pointcloud = None\r\n\r\n    def rgb_callback(self, msg):\r\n        """Handle RGB image callback"""\r\n        self.current_rgb = msg\r\n\r\n    def depth_callback(self, msg):\r\n        """Handle depth image callback"""\r\n        # Convert to appropriate format\r\n        self.current_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\r\n\r\n    def pointcloud_callback(self, msg):\r\n        """Handle point cloud callback"""\r\n        self.current_pointcloud = msg\r\n\r\n    def find_and_grasp_object(self, target_object, target_location=None):\r\n        """Find and grasp a specific object"""\r\n        # First, navigate to the area if location specified\r\n        if target_location:\r\n            rospy.loginfo(f"Navigating to {target_location} to find {target_object}")\r\n            # This would interface with navigation system\r\n            # navigation_client.send_goal(target_location)\r\n            # navigation_client.wait_for_result()\r\n\r\n        # Detect objects in current view\r\n        if self.current_rgb is None:\r\n            rospy.logerr("No RGB image available")\r\n            return False\r\n\r\n        detections = self.object_detector.detect_objects(self.current_rgb)\r\n\r\n        # Filter for target object\r\n        target_detections = self.object_detector.filter_detections_by_class(\r\n            detections, [target_object]\r\n        )\r\n\r\n        if not target_detections:\r\n            rospy.logwarn(f"No {target_object} detected in current view")\r\n            return False\r\n\r\n        # Get the highest confidence detection\r\n        best_detection = max(target_detections, key=lambda d: d[\'score\'])\r\n\r\n        # Estimate 3D pose\r\n        if self.current_depth is not None:\r\n            object_pose = self.pose_estimator.estimate_pose_3d(\r\n                self.current_rgb, self.current_depth, best_detection\r\n            )\r\n        else:\r\n            rospy.logwarn("No depth image available for 3D pose estimation")\r\n            return False\r\n\r\n        if object_pose is None:\r\n            rospy.logerr("Could not estimate 3D pose")\r\n            return False\r\n\r\n        # Get object dimensions if possible\r\n        object_dims = self._estimate_object_dimensions(best_detection)\r\n\r\n        # Plan grasps\r\n        grasp_candidates = self.grasp_planner.plan_grasps(\r\n            object_pose, object_dims, target_object\r\n        )\r\n\r\n        if not grasp_candidates:\r\n            rospy.logerr("No valid grasps found")\r\n            return False\r\n\r\n        # Select best grasp\r\n        current_robot_state = self._get_robot_state()\r\n        best_grasp = self.grasp_optimizer.select_best_grasp(\r\n            grasp_candidates,\r\n            {\'object_class\': target_object, \'dimensions\': object_dims},\r\n            current_robot_state\r\n        )\r\n\r\n        if best_grasp is None:\r\n            rospy.logerr("Could not select a valid grasp")\r\n            return False\r\n\r\n        # Execute grasp\r\n        rospy.loginfo(f"Attempting to grasp {target_object} with score {best_grasp.score}")\r\n        success = self.manip_controller.execute_grasp(best_grasp, {\'class\': target_object})\r\n\r\n        return success\r\n\r\n    def place_object_at_location(self, target_pose, placement_type=\'careful\'):\r\n        """Place currently grasped object at target location"""\r\n        return self.place_controller.place_object(target_pose, placement_type)\r\n\r\n    def _estimate_object_dimensions(self, detection):\r\n        """Estimate object dimensions from detection (simplified)"""\r\n        # This would use more sophisticated methods in practice\r\n        # For now, return typical dimensions based on class\r\n        x1, y1, x2, y2 = detection[\'bbox\']\r\n        width_pixels = x2 - x1\r\n        height_pixels = y2 - y1\r\n\r\n        # Convert to approximate meters (this is very simplified)\r\n        # In practice, you\'d use calibrated camera parameters\r\n        approximate_width = width_pixels * 0.001  # Rough conversion\r\n        approximate_height = height_pixels * 0.001\r\n        approximate_depth = 0.1  # Assume 10cm depth\r\n\r\n        return np.array([approximate_width, approximate_height, approximate_depth])\r\n\r\n    def _get_robot_state(self):\r\n        """Get current robot state for grasp optimization"""\r\n        # This would interface with robot state publisher\r\n        return {\r\n            \'position\': np.array([0.0, 0.0, 0.0]),\r\n            \'orientation\': np.array([0.0, 0.0, 0.0, 1.0]),\r\n            \'joint_angles\': [0.0] * 6\r\n        }\r\n\r\nclass ManipulationTaskExecutor:\r\n    """Executes complex manipulation tasks"""\r\n\r\n    def __init__(self):\r\n        self.coordinator = PerceptionActionCoordinator()\r\n        self.task_queue = []\r\n        self.current_task = None\r\n\r\n    def pick_and_place_task(self, object_to_pick, pick_location, place_location):\r\n        """Execute a pick and place task"""\r\n        try:\r\n            # Navigate to pick location\r\n            rospy.loginfo(f"Navigating to pick up {object_to_pick}")\r\n            # navigation logic would go here\r\n\r\n            # Find and grasp the object\r\n            rospy.loginfo(f"Looking for {object_to_pick}")\r\n            grasp_success = self.coordinator.find_and_grasp_object(object_to_pick)\r\n\r\n            if not grasp_success:\r\n                rospy.logerr(f"Failed to grasp {object_to_pick}")\r\n                return False\r\n\r\n            rospy.loginfo(f"Successfully grasped {object_to_pick}")\r\n\r\n            # Navigate to place location\r\n            rospy.loginfo(f"Navigating to place location")\r\n            # navigation logic would go here\r\n\r\n            # Place the object\r\n            place_pose = {\r\n                \'position\': np.array([place_location[\'x\'], place_location[\'y\'], place_location[\'z\']]),\r\n                \'orientation\': np.array([0, 0, 0, 1])  # Default orientation\r\n            }\r\n\r\n            place_success = self.coordinator.place_object_at_location(place_pose)\r\n\r\n            if place_success:\r\n                rospy.loginfo(f"Successfully placed {object_to_pick}")\r\n                return True\r\n            else:\r\n                rospy.logerr(f"Failed to place {object_to_pick}")\r\n                return False\r\n\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in pick and place task: {e}")\r\n            return False\r\n\r\n    def stack_objects_task(self, object_type, stack_location, num_objects):\r\n        """Stack multiple objects of the same type"""\r\n        for i in range(num_objects):\r\n            rospy.loginfo(f"Stacking object {i+1} of {num_objects}")\r\n\r\n            # Find and pick up object\r\n            pick_success = self.coordinator.find_and_grasp_object(object_type)\r\n            if not pick_success:\r\n                rospy.logwarn(f"Could not find object {i+1}, stopping stack task")\r\n                break\r\n\r\n            # Calculate placement pose (higher for each subsequent object)\r\n            stack_height = 0.1 * (i + 1)  # 10cm per object\r\n            place_pose = {\r\n                \'position\': np.array([stack_location[\'x\'], stack_location[\'y\'],\r\n                                     stack_location[\'z\'] + stack_height]),\r\n                \'orientation\': np.array([0, 0, 0, 1])\r\n            }\r\n\r\n            # Place object\r\n            place_success = self.coordinator.place_object_at_location(place_pose)\r\n            if not place_success:\r\n                rospy.logerr(f"Failed to place object {i+1} in stack")\r\n                break\r\n\r\n        return True  # Consider task successful even if not all objects placed\n'})}),"\n",(0,s.jsx)(r.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsx)(r.h3,{id:"step-1-set-up-object-detection-infrastructure",children:"Step 1: Set up Object Detection Infrastructure"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Create the main manipulation node:"}),"\n"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nfrom sensor_msgs.msg import Image, PointCloud2\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom std_msgs.msg import String\r\nfrom manipulation_msgs.msg import ManipulationGoal, ManipulationResult\r\n\r\nclass ManipulationServer:\r\n    def __init__(self):\r\n        rospy.init_node(\'manipulation_server\')\r\n\r\n        # Initialize components\r\n        self.coordinator = PerceptionActionCoordinator()\r\n        self.task_executor = ManipulationTaskExecutor()\r\n\r\n        # State variables\r\n        self.is_busy = False\r\n        self.current_object = None\r\n\r\n        # Publishers and subscribers\r\n        self.manipulation_sub = rospy.Subscriber(\'/manipulation_goal\', String, self.manipulation_callback)\r\n        self.object_detect_sub = rospy.Subscriber(\'/object_detection_goal\', String, self.detection_callback)\r\n        self.result_pub = rospy.Publisher(\'/manipulation_result\', String, queue_size=10)\r\n        self.status_pub = rospy.Publisher(\'/manipulation_status\', String, queue_size=10)\r\n\r\n        rospy.loginfo("Manipulation server initialized")\r\n\r\n    def manipulation_callback(self, msg):\r\n        """Handle manipulation commands"""\r\n        if self.is_busy:\r\n            rospy.logwarn("Manipulation server busy, rejecting new command")\r\n            return\r\n\r\n        try:\r\n            # Parse manipulation command\r\n            command = msg.data.strip().split()\r\n\r\n            if len(command) >= 2:\r\n                action = command[0].lower()\r\n                target = command[1].lower()\r\n\r\n                if action == \'pick\':\r\n                    self.is_busy = True\r\n                    self._publish_status("EXECUTING_PICK")\r\n\r\n                    success = self.coordinator.find_and_grasp_object(target)\r\n\r\n                    if success:\r\n                        self.current_object = target\r\n                        self._publish_result(f"SUCCESS: Picked {target}")\r\n                        self._publish_status("PICK_COMPLETED")\r\n                    else:\r\n                        self._publish_result(f"FAILURE: Could not pick {target}")\r\n                        self._publish_status("PICK_FAILED")\r\n\r\n                    self.is_busy = False\r\n\r\n                elif action == \'place\':\r\n                    if self.current_object:\r\n                        self.is_busy = True\r\n                        self._publish_status("EXECUTING_PLACE")\r\n\r\n                        # For now, place at default location\r\n                        place_pose = {\r\n                            \'position\': np.array([0.5, 0.0, 0.1]),\r\n                            \'orientation\': np.array([0, 0, 0, 1])\r\n                        }\r\n\r\n                        success = self.coordinator.place_object_at_location(place_pose)\r\n\r\n                        if success:\r\n                            self._publish_result(f"SUCCESS: Placed {self.current_object}")\r\n                            self._publish_status("PLACE_COMPLETED")\r\n                            self.current_object = None\r\n                        else:\r\n                            self._publish_result(f"FAILURE: Could not place {self.current_object}")\r\n                            self._publish_status("PLACE_FAILED")\r\n\r\n                        self.is_busy = False\r\n                    else:\r\n                        self._publish_result("FAILURE: No object currently grasped")\r\n                        self._publish_status("NO_OBJECT_TO_PLACE")\r\n\r\n                elif action == \'detect\':\r\n                    self.is_busy = True\r\n                    self._publish_status("EXECUTING_DETECTION")\r\n\r\n                    # Detect objects in current view\r\n                    if self.coordinator.current_rgb is not None:\r\n                        detections = self.coordinator.object_detector.detect_objects(\r\n                            self.coordinator.current_rgb\r\n                        )\r\n\r\n                        # Publish detected objects\r\n                        detected_objects = [det[\'class_name\'] for det in detections\r\n                                          if det[\'score\'] > 0.5]\r\n                        result_msg = f"DETECTED: {\', \'.join(detected_objects)}"\r\n                        self._publish_result(result_msg)\r\n                        self._publish_status("DETECTION_COMPLETED")\r\n                    else:\r\n                        self._publish_result("FAILURE: No image data available")\r\n                        self._publish_status("NO_IMAGE_DATA")\r\n\r\n                    self.is_busy = False\r\n            else:\r\n                self._publish_result("FAILURE: Invalid command format")\r\n                self._publish_status("INVALID_COMMAND")\r\n\r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing manipulation command: {e}")\r\n            self._publish_result(f"FAILURE: {str(e)}")\r\n            self._publish_status("ERROR")\r\n            self.is_busy = False\r\n\r\n    def detection_callback(self, msg):\r\n        """Handle object detection requests"""\r\n        try:\r\n            target_class = msg.data.strip()\r\n\r\n            if self.coordinator.current_rgb is not None:\r\n                detections = self.coordinator.object_detector.detect_objects(\r\n                    self.coordinator.current_rgb\r\n                )\r\n\r\n                target_detections = self.coordinator.object_detector.filter_detections_by_class(\r\n                    detections, [target_class]\r\n                )\r\n\r\n                if target_detections:\r\n                    best_detection = max(target_detections, key=lambda d: d[\'score\'])\r\n                    result_msg = f"FOUND: {target_class} with confidence {best_detection[\'score\']:.2f}"\r\n                    self._publish_result(result_msg)\r\n                else:\r\n                    self._publish_result(f"NOT_FOUND: {target_class}")\r\n            else:\r\n                self._publish_result("FAILURE: No image data available")\r\n\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in detection callback: {e}")\r\n            self._publish_result(f"FAILURE: {str(e)}")\r\n\r\n    def _publish_result(self, result):\r\n        """Publish manipulation result"""\r\n        result_msg = String()\r\n        result_msg.data = result\r\n        self.result_pub.publish(result_msg)\r\n\r\n    def _publish_status(self, status):\r\n        """Publish manipulation status"""\r\n        status_msg = String()\r\n        status_msg.data = status\r\n        self.status_pub.publish(status_msg)\r\n\r\n    def run(self):\r\n        """Main execution loop"""\r\n        rate = rospy.Rate(10)  # 10 Hz\r\n\r\n        while not rospy.is_shutdown():\r\n            # Perform any periodic tasks\r\n            rate.sleep()\r\n\r\nif __name__ == \'__main__\':\r\n    manip_server = ManipulationServer()\r\n    try:\r\n        manip_server.run()\r\n    except rospy.ROSInterruptException:\r\n        pass\n'})}),"\n",(0,s.jsx)(r.h3,{id:"step-2-configure-manipulation-parameters",children:"Step 2: Configure Manipulation Parameters"}),"\n",(0,s.jsx)(r.p,{children:"Create a configuration file for manipulation parameters:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-yaml",children:'# manipulation_params.yaml\r\nmanipulation_server:\r\n  ros__parameters:\r\n    gripper_joint: "gripper_joint"\r\n    gripper_min_position: 0.0\r\n    gripper_max_position: 0.1\r\n    gripper_force_limit: 50.0\r\n    approach_distance: 0.1\r\n    lift_distance: 0.05\r\n    max_velocity: 0.5\r\n    max_acceleration: 1.0\r\n    confidence_threshold: 0.7\r\n    detection_timeout: 10.0\r\n\r\nobject_detector:\r\n  ros__parameters:\r\n    model_path: "/path/to/detection/model"\r\n    confidence_threshold: 0.5\r\n    nms_threshold: 0.4\r\n    image_topic: "/camera/rgb/image_raw"\r\n    depth_topic: "/camera/depth/image_raw"\r\n\r\ngrasp_planner:\r\n  ros__parameters:\r\n    gripper_width_min: 0.01\r\n    gripper_width_max: 0.1\r\n    approach_distance: 0.1\r\n    lift_distance: 0.05\r\n    max_grasps_to_generate: 10\n'})}),"\n",(0,s.jsx)(r.h3,{id:"step-3-implement-advanced-features",children:"Step 3: Implement Advanced Features"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'class LearningBasedManipulation:\r\n    """Implement learning-based improvements to manipulation"""\r\n\r\n    def __init__(self):\r\n        self.success_history = {}  # Track success rates for different objects\r\n        self.grasp_success_count = {}  # Count successful grasps per object type\r\n        self.grasp_attempt_count = {}  # Count all grasp attempts per object type\r\n\r\n    def record_grasp_attempt(self, object_class, grasp_pose, success):\r\n        """Record the outcome of a grasp attempt"""\r\n        if object_class not in self.grasp_attempt_count:\r\n            self.grasp_attempt_count[object_class] = 0\r\n            self.grasp_success_count[object_class] = 0\r\n\r\n        self.grasp_attempt_count[object_class] += 1\r\n        if success:\r\n            self.grasp_success_count[object_class] += 1\r\n\r\n    def get_object_success_rate(self, object_class):\r\n        """Get success rate for grasping a particular object class"""\r\n        if object_class not in self.grasp_attempt_count:\r\n            return 0.0  # No data yet\r\n\r\n        attempts = self.grasp_attempt_count[object_class]\r\n        successes = self.grasp_success_count[object_class]\r\n\r\n        if attempts == 0:\r\n            return 0.0\r\n\r\n        return successes / attempts\r\n\r\n    def adapt_grasp_selection(self, grasp_candidates, object_class):\r\n        """Adapt grasp selection based on historical success data"""\r\n        success_rate = self.get_object_success_rate(object_class)\r\n\r\n        if success_rate < 0.5:  # Low success rate, be more conservative\r\n            # Favor higher-scoring grasps more heavily\r\n            for grasp in grasp_candidates:\r\n                grasp.score *= 1.2  # Boost score slightly for conservative approach\r\n        elif success_rate > 0.8:  # High success rate, can be more adventurous\r\n            # Consider more grasp options\r\n            pass  # Keep original scores\r\n\r\n        return grasp_candidates\r\n\r\nclass AdaptiveManipulationController(ManipulationController):\r\n    """Manipulation controller with adaptive behavior"""\r\n\r\n    def __init__(self, learning_module):\r\n        super().__init__()\r\n        self.learning_module = learning_module\r\n\r\n    def execute_grasp_with_learning(self, grasp_pose, object_info):\r\n        """Execute grasp with learning-based adaptations"""\r\n        object_class = object_info.get(\'class\', \'unknown\')\r\n\r\n        # Adapt grasp selection based on learning\r\n        adapted_grasp = self._adapt_grasp_for_object(grasp_pose, object_class)\r\n\r\n        # Execute the grasp\r\n        success = self.execute_grasp(adapted_grasp, object_info)\r\n\r\n        # Record the outcome\r\n        self.learning_module.record_grasp_attempt(object_class, adapted_grasp, success)\r\n\r\n        return success\r\n\r\n    def _adapt_grasp_for_object(self, grasp_pose, object_class):\r\n        """Adapt grasp based on object-specific knowledge"""\r\n        success_rate = self.learning_module.get_object_success_rate(object_class)\r\n\r\n        # If success rate is low, make conservative adjustments\r\n        if success_rate < 0.5:\r\n            # Increase approach distance for fragile objects\r\n            adapted_pose = self._increase_approach_distance(grasp_pose)\r\n        else:\r\n            adapted_pose = grasp_pose\r\n\r\n        return adapted_pose\r\n\r\n    def _increase_approach_distance(self, grasp_pose):\r\n        """Increase approach distance for safer grasping"""\r\n        # Move approach position further from object\r\n        approach_offset = grasp_pose.approach_direction * 0.02  # Additional 2cm\r\n        adapted_pos = grasp_pose.position + approach_offset\r\n\r\n        adapted_grasp = GraspPose(\r\n            position=adapted_pos,\r\n            orientation=grasp_pose.orientation,\r\n            approach_direction=grasp_pose.approach_direction,\r\n            grasp_type=grasp_pose.grasp_type,\r\n            score=grasp_pose.score,\r\n            width=grasp_pose.width\r\n        )\r\n\r\n        return adapted_grasp\n'})}),"\n",(0,s.jsx)(r.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,s.jsx)(r.h3,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"import unittest\r\nfrom unittest.mock import Mock, patch\r\n\r\nclass TestObjectDetector(unittest.TestCase):\r\n    def setUp(self):\r\n        self.detector = ObjectDetector()\r\n\r\n    def test_object_detection(self):\r\n        \"\"\"Test basic object detection functionality\"\"\"\r\n        # Create a simple test image (in practice, use a real image)\r\n        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\r\n\r\n        # This would test the detection pipeline\r\n        # For now, we'll just ensure the method doesn't crash\r\n        try:\r\n            detections = self.detector.detect_objects(test_image)\r\n            self.assertIsInstance(detections, list)\r\n        except Exception as e:\r\n            self.fail(f\"Detection failed with error: {e}\")\r\n\r\n    def test_class_filtering(self):\r\n        \"\"\"Test filtering detections by class\"\"\"\r\n        mock_detections = [\r\n            {'bbox': [10, 10, 50, 50], 'score': 0.8, 'class_name': 'cup'},\r\n            {'bbox': [60, 60, 100, 100], 'score': 0.7, 'class_name': 'book'},\r\n            {'bbox': [110, 110, 150, 150], 'score': 0.9, 'class_name': 'bottle'}\r\n        ]\r\n\r\n        filtered = self.detector.filter_detections_by_class(mock_detections, ['cup', 'bottle'])\r\n\r\n        self.assertEqual(len(filtered), 2)\r\n        self.assertIn('cup', [d['class_name'] for d in filtered])\r\n        self.assertIn('bottle', [d['class_name'] for d in filtered])\r\n\r\nclass TestGraspPlanner(unittest.TestCase):\r\n    def setUp(self):\r\n        self.planner = GraspPlanner()\r\n\r\n    def test_grasp_generation(self):\r\n        \"\"\"Test grasp generation for a simple object\"\"\"\r\n        object_pose = {\r\n            'position': np.array([0.5, 0.0, 0.1]),\r\n            'orientation': np.array([0, 0, 0, 1])\r\n        }\r\n        object_dims = np.array([0.05, 0.05, 0.1])  # 5x5x10cm object\r\n\r\n        grasps = self.planner.plan_grasps(object_pose, object_dims, 'cup')\r\n\r\n        self.assertGreater(len(grasps), 0)\r\n        for grasp in grasps:\r\n            self.assertIsInstance(grasp, GraspPose)\r\n            self.assertGreaterEqual(grasp.score, 0.0)\r\n            self.assertLessEqual(grasp.score, 1.0)\r\n\r\n    def test_grasp_validation(self):\r\n        \"\"\"Test grasp validation\"\"\"\r\n        grasp_pose = GraspPose(\r\n            position=np.array([0.5, 0.0, 0.1]),\r\n            orientation=np.array([0, 0, 0, 1]),\r\n            approach_direction=np.array([1, 0, 0]),\r\n            grasp_type='pinch',\r\n            score=0.8,\r\n            width=0.03\r\n        )\r\n\r\n        # Mock environment data\r\n        env_data = {}\r\n\r\n        is_valid, reason = self.planner.validate_grasp(grasp_pose, object_pose, env_data)\r\n\r\n        self.assertTrue(is_valid)\r\n\r\nclass TestGraspOptimizer(unittest.TestCase):\r\n    def setUp(self):\r\n        self.optimizer = GraspOptimizer()\r\n\r\n    def test_grasp_selection(self):\r\n        \"\"\"Test best grasp selection\"\"\"\r\n        grasps = [\r\n            GraspPose(\r\n                position=np.array([0.5, 0.0, 0.1]),\r\n                orientation=np.array([0, 0, 0, 1]),\r\n                approach_direction=np.array([1, 0, 0]),\r\n                grasp_type='pinch',\r\n                score=0.8,\r\n                width=0.03\r\n            ),\r\n            GraspPose(\r\n                position=np.array([0.6, 0.0, 0.1]),\r\n                orientation=np.array([0, 0, 0, 1]),\r\n                approach_direction=np.array([0, 1, 0]),\r\n                grasp_type='power',\r\n                score=0.7,\r\n                width=0.05\r\n            )\r\n        ]\r\n\r\n        object_info = {'class': 'cup', 'dimensions': np.array([0.05, 0.05, 0.1])}\r\n        robot_state = {'position': np.array([0, 0, 0]), 'joint_angles': [0]*6}\r\n\r\n        best_grasp = self.optimizer.select_best_grasp(grasps, object_info, robot_state)\r\n\r\n        self.assertIsNotNone(best_grasp)\r\n        self.assertIn(best_grasp, grasps)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\n"})}),"\n",(0,s.jsx)(r.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'class ManipulationIntegrationTest:\r\n    def __init__(self):\r\n        rospy.init_node(\'manipulation_integration_test\')\r\n        self.coordinator = PerceptionActionCoordinator()\r\n\r\n    def test_detection_to_grasp_pipeline(self):\r\n        """Test complete pipeline from detection to grasp"""\r\n        # This would require a real robot or simulation\r\n        # For now, we\'ll test the individual components\r\n\r\n        print("Testing detection to grasp pipeline...")\r\n\r\n        # Simulate having an image\r\n        if self.coordinator.current_rgb is not None:\r\n            # Test detection\r\n            detections = self.coordinator.object_detector.detect_objects(\r\n                self.coordinator.current_rgb\r\n            )\r\n            print(f"Detected {len(detections)} objects")\r\n\r\n            if detections:\r\n                # Test pose estimation (if depth available)\r\n                if self.coordinator.current_depth is not None:\r\n                    best_detection = detections[0]  # Use first detection\r\n                    object_pose = self.coordinator.pose_estimator.estimate_pose_3d(\r\n                        self.coordinator.current_rgb,\r\n                        self.coordinator.current_depth,\r\n                        best_detection\r\n                    )\r\n                    print(f"Estimated pose: {object_pose}")\r\n\r\n                    if object_pose:\r\n                        # Test grasp planning\r\n                        object_dims = self.coordinator._estimate_object_dimensions(best_detection)\r\n                        grasp_candidates = self.coordinator.grasp_planner.plan_grasps(\r\n                            object_pose, object_dims, best_detection[\'class_name\']\r\n                        )\r\n                        print(f"Generated {len(grasp_candidates)} grasp candidates")\r\n\r\n                        if grasp_candidates:\r\n                            # Test grasp optimization\r\n                            best_grasp = self.coordinator.grasp_optimizer.select_best_grasp(\r\n                                grasp_candidates,\r\n                                {\'object_class\': best_detection[\'class_name\'], \'dimensions\': object_dims},\r\n                                self.coordinator._get_robot_state()\r\n                            )\r\n                            print(f"Selected best grasp with score: {best_grasp.score if best_grasp else \'None\'}")\r\n\r\n        print("Pipeline test completed")\r\n\r\n    def test_pick_and_place_task(self):\r\n        """Test pick and place task execution"""\r\n        # This would test the ManipulationTaskExecutor\r\n        task_executor = ManipulationTaskExecutor()\r\n\r\n        # Example task (would need real object and locations in practice)\r\n        # result = task_executor.pick_and_place_task(\'cup\',\r\n        #                                          {\'x\': 0.5, \'y\': 0.0, \'z\': 0.0},\r\n        #                                          {\'x\': 0.8, \'y\': 0.0, \'z\': 0.0})\r\n        # print(f"Pick and place result: {result}")\r\n\r\n        print("Pick and place task test completed (simulation)")\n'})}),"\n",(0,s.jsx)(r.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,s.jsx)(r.h3,{id:"detection-performance",children:"Detection Performance"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Object Detection"}),": < 100ms per frame for 640x480 images"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Pose Estimation"}),": < 200ms per object with depth information"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Grasp Planning"}),": < 50ms per grasp candidate"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Manipulation Execution"}),": < 10s for complete pick/place operation"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Memory Usage"}),": < 2GB for full perception pipeline"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"accuracy-requirements",children:"Accuracy Requirements"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Object Detection"}),": > 85% accuracy for known objects"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Pose Estimation"}),": < 2cm position error, < 10\xb0 orientation error"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Grasp Success Rate"}),": > 80% for common objects"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Placement Accuracy"}),": < 3cm error from target location"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"troubleshooting-and-common-issues",children:"Troubleshooting and Common Issues"}),"\n",(0,s.jsx)(r.h3,{id:"detection-problems",children:"Detection Problems"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"False Positives"}),": Adjust confidence thresholds and use NMS"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Missed Objects"}),": Improve lighting conditions and camera calibration"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Pose Inaccuracy"}),": Use multiple views and improve depth quality"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Real-time Performance"}),": Optimize model size or use edge computing"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"manipulation-problems",children:"Manipulation Problems"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Grasp Failures"}),": Improve grasp planning and object property knowledge"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Collision Detection"}),": Enhance environment modeling and path planning"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Object Slippage"}),": Adjust gripper force and improve grasp type selection"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Kinematic Singularities"}),": Implement redundancy resolution"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(r.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Force Limiting"}),": Always limit gripper and arm forces"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Collision Avoidance"}),": Check for collisions before movement"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Emergency Stop"}),": Implement immediate stop capabilities"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Workspace Limits"}),": Respect physical workspace boundaries"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Model Efficiency"}),": Use optimized neural network models"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Multi-threading"}),": Separate perception and action threads"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Caching"}),": Cache frequently computed values"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Adaptive Resolution"}),": Adjust processing based on requirements"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"maintainability",children:"Maintainability"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Modular Design"}),": Keep perception, planning, and control separate"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Parameter Configuration"}),": Use ROS parameters for easy tuning"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Comprehensive Logging"}),": Log all manipulation decisions and outcomes"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Testing Framework"}),": Maintain extensive test coverage"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"next-steps-and-integration",children:"Next Steps and Integration"}),"\n",(0,s.jsx)(r.h3,{id:"integration-with-other-capstone-components",children:"Integration with Other Capstone Components"}),"\n",(0,s.jsx)(r.p,{children:"The manipulation system integrates with:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Navigation"}),": Coordinates for precise positioning"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Task Planning"}),": Receives manipulation goals and reports status"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Perception"}),": Gets object information and environmental data"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Voice Processing"}),": Handles object-related commands"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Localization"}),": Maintains spatial relationships"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,s.jsx)(r.p,{children:"Consider implementing:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Tactile Feedback"}),": Use tactile sensors for better grasp control"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Learning from Demonstration"}),": Learn new manipulation skills from human examples"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Multi-object Manipulation"}),": Handle multiple objects simultaneously"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Deformable Object Manipulation"}),": Handle cloth, rope, and other deformable objects"]}),"\n"]}),"\n",(0,s.jsxs)(r.p,{children:["Continue with ",(0,s.jsx)(r.a,{href:"/Book-ai-native/docs/capstone/failure-handling",children:"Failure Handling and Status Reporting"})," to explore the implementation of robust error handling and system monitoring capabilities that ensure the autonomous humanoid system operates safely and reliably."]}),"\n",(0,s.jsx)(r.h2,{id:"references",children:"References"}),"\n",(0,s.jsx)(r.p,{children:"[All sources will be cited in the References section at the end of the book, following APA format]"})]})}function d(e={}){const{wrapper:r}={...(0,o.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453(e,r,n){n.d(r,{R:()=>i,x:()=>a});var t=n(6540);const s={},o=t.createContext(s);function i(e){const r=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(o.Provider,{value:r},e.children)}}}]);